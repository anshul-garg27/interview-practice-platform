{
  "problem_title": "News Feed Aggregator System - Part 3: Content Recommendation & ML Integration",
  "part_number": 3,
  "builds_on": "Part 2",
  "difficulty": "hard",
  "problem_understanding": {
    "what_changes": "Part 3 transforms the feed from rule-based ranking to ML-powered recommendations. Instead of just matching publishers/categories, we learn from user behavior (clicks, read time, shares) to predict what content they'll engage with. We add collaborative filtering (what similar users like) and content-based filtering (category affinity from engagement).",
    "new_requirements": [
      "Track engagement signals (click, read, share, save) with duration",
      "Build user profiles from implicit feedback (engagement patterns)",
      "Find similar users for collaborative filtering",
      "Generate ML-ranked recommendations that balance relevance, freshness, and diversity",
      "Support model training (batch recomputation of features)"
    ],
    "new_constraints": [
      "getRecommendations must be fast (<50ms) - can't iterate all users",
      "Must handle cold start (new users with no engagement)",
      "Balance exploitation (known preferences) vs exploration (new content)",
      "Enforce diversity in recommendations (no duplicate topics in top-N)"
    ],
    "key_insight": "The AHA moment: Represent users as weighted category vectors built from engagement signals. Similarity between users becomes cosine similarity of these vectors. This gives us O(K) scoring per article where K=categories, not O(U) where U=users. Pre-compute similar users lazily and cache for fast collaborative lookups."
  },
  "requirements_coverage": {
    "checklist": [
      {
        "requirement": "Record engagement with type and duration",
        "how_met": "record_engagement stores Engagement objects, immediately updates user_category_weights for online learning",
        "gotchas": [
          "Don't forget to invalidate caches when engagement is recorded"
        ]
      },
      {
        "requirement": "ML-based recommendations",
        "how_met": "get_recommendations combines content score (category match), collaborative score (similar user articles), freshness, and popularity",
        "gotchas": [
          "Filter out already-engaged articles to avoid recommending seen content"
        ]
      },
      {
        "requirement": "Find similar users",
        "how_met": "get_similar_users computes cosine similarity of user category weight vectors, caches results",
        "gotchas": [
          "Handle cold start - new users have empty vectors"
        ]
      },
      {
        "requirement": "Diversity in results",
        "how_met": "_diversify_results ensures top results don't all have same primary category",
        "gotchas": [
          "Still need to fill count slots even after diversity filtering"
        ]
      },
      {
        "requirement": "Batch training support",
        "how_met": "train_model recomputes all category weights from stored engagements, clears caches",
        "gotchas": [
          "Production would use actual ML; this simulates feature recomputation"
        ]
      }
    ],
    "complexity_targets": [
      {
        "operation": "record_engagement",
        "target": "O(C)",
        "achieved": "O(C)",
        "why": "C = categories in article, update weight per category"
      },
      {
        "operation": "get_recommendations",
        "target": "O(A * (C + S))",
        "achieved": "O(A * (C + S))",
        "why": "A = articles, C = categories per article, S = similar users to check"
      },
      {
        "operation": "get_similar_users",
        "target": "O(U * K) first call, O(1) cached",
        "achieved": "O(U * K) / O(1)",
        "why": "U = users, K = categories. Computed once, cached for subsequent calls"
      }
    ],
    "non_goals": [
      "Actual neural network training",
      "Real-time model serving infrastructure",
      "A/B testing framework",
      "Feature store implementation"
    ]
  },
  "assumptions": [
    "Engagement data fits in memory (production: external feature store)",
    "User similarity can be computed lazily on first request (production: pre-computed in batch)",
    "Single engagement per (user, article, type) - no deduplication needed",
    "Cold start users get trending/popular content until they have 3+ engagements",
    "Categories are discrete (no hierarchical similarity like 'Politics' ~ 'Government')"
  ],
  "tradeoffs": [
    {
      "decision": "Category vectors vs neural embeddings",
      "chosen": "Category vectors",
      "why": "Simple, interpretable, no training infrastructure needed. Good enough for interview.",
      "alternative": "Two-tower neural network",
      "when_to_switch": "When you have millions of users and need to capture subtle patterns"
    },
    {
      "decision": "Online vs batch learning",
      "chosen": "Hybrid - online for weights, batch for similarity cache",
      "why": "Immediate feedback incorporation while keeping similarity computation bounded",
      "alternative": "Pure batch",
      "when_to_switch": "If real-time updates aren't required"
    },
    {
      "decision": "Lazy vs eager similar user computation",
      "chosen": "Lazy with caching",
      "why": "Only compute when needed, amortize cost over requests",
      "alternative": "Precompute all pairs",
      "when_to_switch": "If you need guaranteed low latency on first request"
    }
  ],
  "extensibility_notes": {
    "what_to_keep_stable": [
      "Existing Part 1 & 2 methods unchanged",
      "record_engagement signature",
      "get_recommendations signature",
      "Engagement data stored for future retraining"
    ],
    "what_to_change": [
      "user_category_weights updates on each engagement",
      "similar_users_cache invalidated when user profile changes"
    ],
    "interfaces_and_boundaries": "Engagement storage is separate from scoring logic. Scoring function can be swapped (e.g., plug in actual ML model) without changing data collection.",
    "invariants": [
      "user_category_weights[user][cat] >= 0 for all entries",
      "Engagement list only grows (append-only log)",
      "Similar users cache invalidated when user profile changes",
      "Recommendations never include articles user already engaged with"
    ]
  },
  "visual_explanation": {
    "before_after": "```\nBEFORE (Part 2):                    AFTER (Part 3):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 User Profile        \u2502            \u2502 User Profile        \u2502\n\u2502 \u251c\u2500\u2500 followed_pubs   \u2502            \u2502 \u251c\u2500\u2500 followed_pubs   \u2502\n\u2502 \u2514\u2500\u2500 interests       \u2502            \u2502 \u251c\u2500\u2500 interests       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502 \u2514\u2500\u2500 category_weights\u2502 \u2190 NEW!\n         \u2502                         \u2502     (learned from   \u2502\n         \u25bc                         \u2502      engagement)    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502 Rule-based scoring  \u2502                     \u2502\n\u2502 (explicit prefs)    \u2502                     \u25bc\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                   \u2502 ML-based scoring    \u2502\n                                   \u2502 \u251c\u2500\u2500 Content match   \u2502\n                                   \u2502 \u251c\u2500\u2500 Collab filter   \u2502\n                                   \u2502 \u251c\u2500\u2500 Freshness       \u2502\n                                   \u2502 \u2514\u2500\u2500 Diversity       \u2502\n                                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
    "algorithm_flow": "```\nrecord_engagement(user, article, READ, 180s):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1. Store engagement in user_engagements[user]        \u2502\n\u2502    engagement = Engagement(article, READ, 180, now)   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2. Calculate weight: READ + 180s \u2192 3.0 points        \u2502\n\u2502    (1.0 base \u00d7 180/60 minutes)                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 3. Update category weights for article's categories   \u2502\n\u2502    article.categories = [\"Tech\", \"AI\"]                \u2502\n\u2502    user_category_weights[user][\"Tech\"] += 3.0        \u2502\n\u2502    user_category_weights[user][\"AI\"] += 3.0          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 4. Invalidate caches                                  \u2502\n\u2502    similar_users_cache[user] = None                   \u2502\n\u2502    user_feed_cache[user] = None                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nget_recommendations(user, 5):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1. Get user's category weights (learned profile)      \u2502\n\u2502    {\"Tech\": 5.0, \"AI\": 3.0, \"Sports\": 0.5}           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 2. For each unseen article, compute ML score:         \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n\u2502    \u2502 content_score = \u03a3(cat_weight \u00d7 match) = 2.5    \u2502\u2502\n\u2502    \u2502 collab_score  = similar_users_engaged \u00d7 1.0    \u2502\u2502\n\u2502    \u2502 freshness     = max(0, 2 - age_hours/12)       \u2502\u2502\n\u2502    \u2502 popularity    = article.popularity \u00d7 0.3       \u2502\u2502\n\u2502    \u2502 TOTAL = 2.5 + 1.0 + 1.5 + 0.3 = 5.3           \u2502\u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 3. Sort by score, apply diversity filter              \u2502\n\u2502    Top 5 with varied primary categories               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```"
  },
  "approaches": [
    {
      "name": "Naive Extension",
      "description": "Simply use engagement count as another scoring factor in Part 2's _score_article. For each article, count how many times similar articles (same category) were engaged with.",
      "time_complexity": "O(A \u00d7 E) where A=articles, E=engagements",
      "space_complexity": "O(E) for storing engagements",
      "why_not_optimal": "Scanning all engagements for each article is slow. No user similarity computation. No proper weighting for engagement types. Doesn't support diversity enforcement."
    },
    {
      "name": "Optimal Approach",
      "description": "Build user profiles as weighted category vectors updated on each engagement (online learning). Cache similar users lazily. Score articles using content match (O(C) per article) + collaborative boost (O(S) lookups) + freshness + diversity re-ranking.",
      "time_complexity": "O(A \u00d7 (C + S)) for recommendations where C=categories, S=similar users",
      "space_complexity": "O(U \u00d7 K + U \u00d7 E_avg) for user vectors and engagement history",
      "key_insight": "Category vectors are sparse and fixed-dimension. This transforms O(E) engagement scanning into O(C) dot product. Similar user cache amortizes the O(U \u00d7 K) similarity computation."
    }
  ],
  "optimal_solution": {
    "explanation_md": "The solution uses **feature engineering** to convert raw engagement signals into actionable user profiles:\n\n1. **Engagement \u2192 Category Weights**: Each engagement updates the user's affinity for the article's categories. Weight depends on engagement type (SHARE > SAVE > READ > CLICK) and duration (longer reads = stronger signal).\n\n2. **User Profile = Category Vector**: After several engagements, a user has a profile like `{\"Tech\": 8.5, \"AI\": 4.2, \"Sports\": 0.5}`. This is their **learned embedding**.\n\n3. **Collaborative Filtering**: Find similar users by **cosine similarity** of their category vectors. Cache results since vectors change slowly.\n\n4. **Scoring Formula**: For each candidate article:\n   - **Content score**: Sum of user's weights for article's categories\n   - **Collaborative score**: +1.0 if any similar user engaged with it\n   - **Freshness boost**: Decays over 24 hours\n   - **Popularity signal**: Global engagement indicator\n\n5. **Diversity Re-ranking**: Don't just return top-5 by score. Ensure varied primary categories to avoid \"all politics\" feeds.",
    "data_structures": [
      {
        "structure": "Dict[str, List[Engagement]]",
        "purpose": "Store raw engagement events for retraining and analysis"
      },
      {
        "structure": "Dict[str, Dict[str, float]]",
        "purpose": "User category weight vectors (learned profiles)"
      },
      {
        "structure": "Dict[str, List[str]]",
        "purpose": "Cached similar users per user for fast collaborative lookups"
      }
    ],
    "algorithm_steps": [
      "record_engagement: Store event, update user's category weights with engagement-type-specific weight, invalidate caches",
      "get_recommendations: (1) Get user profile, (2) Score all unseen articles with ML formula, (3) Sort by score, (4) Apply diversity filter, (5) Return top-N",
      "get_similar_users: (1) Check cache, (2) If miss: compute cosine similarity with all other users, (3) Sort by similarity, (4) Cache and return top-K",
      "train_model: (1) Clear all weights, (2) Replay all engagements to recompute weights, (3) Clear similarity cache"
    ]
  },
  "solution_python_lines": [
    "\"\"\"",
    "News Feed Aggregator - Part 3: ML-Powered Recommendations",
    "Key insight: User profiles as category weight vectors enable fast scoring.",
    "\"\"\"",
    "from typing import List, Dict, Set",
    "from dataclasses import dataclass, field",
    "from collections import defaultdict",
    "from enum import Enum",
    "import time",
    "import math",
    "",
    "class EngagementType(Enum):",
    "    CLICK = \"CLICK\"",
    "    READ = \"READ\"",
    "    SHARE = \"SHARE\"",
    "    SAVE = \"SAVE\"",
    "",
    "@dataclass",
    "class Article:",
    "    id: str",
    "    publisher_id: str",
    "    title: str",
    "    url: str",
    "    categories: List[str]",
    "    published_at: float",
    "    popularity: float = 0.0",
    "    score: float = 0.0",
    "",
    "@dataclass",
    "class Publisher:",
    "    id: str",
    "    name: str",
    "    rss_url: str",
    "    categories: List[str]",
    "",
    "@dataclass",
    "class User:",
    "    id: str",
    "    followed_publishers: Set[str] = field(default_factory=set)",
    "    interests: Set[str] = field(default_factory=set)",
    "    publisher_affinity: Dict[str, int] = field(default_factory=dict)",
    "",
    "@dataclass",
    "class NotificationPrefs:",
    "    max_daily: int = 5",
    "    categories: List[str] = field(default_factory=list)",
    "    quiet_hours_start: int = 22",
    "    quiet_hours_end: int = 8",
    "    enabled: bool = True",
    "",
    "@dataclass",
    "class Notification:",
    "    id: str",
    "    user_id: str",
    "    article_id: str",
    "    title: str",
    "    priority: int",
    "    created_at: float",
    "    read: bool = False",
    "",
    "# Part 3: New data class",
    "@dataclass",
    "class Engagement:",
    "    article_id: str",
    "    engagement_type: EngagementType",
    "    duration_seconds: int",
    "    timestamp: float",
    "",
    "class NewsAggregator:",
    "    \"\"\"News aggregator with multi-tier caching, notifications, and ML recommendations.\"\"\"",
    "    ",
    "    def __init__(self):",
    "        # Part 1 fields",
    "        self.publishers: Dict[str, Publisher] = {}",
    "        self.users: Dict[str, User] = {}",
    "        self.articles: Dict[str, Article] = {}",
    "        self.category_cache: Dict[str, List[Article]] = defaultdict(list)",
    "        self.publisher_cache: Dict[str, List[Article]] = defaultdict(list)",
    "        self.user_feed_cache: Dict[str, List[Article]] = {}",
    "        self.CACHE_SIZE = 100",
    "        ",
    "        # Part 2 fields",
    "        self.notification_prefs: Dict[str, NotificationPrefs] = {}",
    "        self.pending_notifications: Dict[str, List[Notification]] = defaultdict(list)",
    "        self.daily_notification_count: Dict[str, int] = defaultdict(int)",
    "        self.last_count_reset: Dict[str, float] = {}",
    "        self.category_subscribers: Dict[str, Set[str]] = defaultdict(set)",
    "        self.publisher_followers: Dict[str, Set[str]] = defaultdict(set)",
    "        self.BREAKING_NEWS_THRESHOLD = 8",
    "        self._notification_counter = 0",
    "        ",
    "        # Part 3: ML recommendation infrastructure",
    "        self.user_engagements: Dict[str, List[Engagement]] = defaultdict(list)",
    "        self.user_category_weights: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))",
    "        self.similar_users_cache: Dict[str, List[str]] = {}",
    "    ",
    "    # ========== Part 1 Methods (unchanged) ==========",
    "    ",
    "    def register_publisher(self, publisher_id: str, name: str,",
    "                           rss_url: str, categories: List[str]) -> None:",
    "        self.publishers[publisher_id] = Publisher(publisher_id, name, rss_url, categories)",
    "    ",
    "    def follow_publisher(self, user_id: str, publisher_id: str) -> None:",
    "        user = self._get_or_create_user(user_id)",
    "        user.followed_publishers.add(publisher_id)",
    "        self.user_feed_cache.pop(user_id, None)",
    "        self.publisher_followers[publisher_id].add(user_id)",
    "    ",
    "    def set_user_interests(self, user_id: str, categories: List[str]) -> None:",
    "        user = self._get_or_create_user(user_id)",
    "        for old_cat in user.interests:",
    "            self.category_subscribers[old_cat].discard(user_id)",
    "        user.interests = set(categories)",
    "        for cat in categories:",
    "            self.category_subscribers[cat].add(user_id)",
    "        self.user_feed_cache.pop(user_id, None)",
    "    ",
    "    def like_article(self, user_id: str, article_id: str) -> None:",
    "        if article_id not in self.articles:",
    "            return",
    "        user = self._get_or_create_user(user_id)",
    "        pub_id = self.articles[article_id].publisher_id",
    "        user.publisher_affinity[pub_id] = user.publisher_affinity.get(pub_id, 0) + 1",
    "    ",
    "    def get_user_feed(self, user_id: str, page: int, page_size: int) -> List[Article]:",
    "        if user_id in self.user_feed_cache:",
    "            return self._paginate(self.user_feed_cache[user_id], page, page_size)",
    "        user = self._get_or_create_user(user_id)",
    "        candidates: Dict[str, Article] = {}",
    "        for interest in user.interests:",
    "            for article in self.category_cache.get(interest, []):",
    "                candidates[article.id] = article",
    "        for pub_id in user.followed_publishers:",
    "            for article in self.publisher_cache.get(pub_id, []):",
    "                candidates[article.id] = article",
    "        if not candidates:",
    "            candidates = {a.id: a for a in self._get_trending_articles()}",
    "        scored = []",
    "        for article in candidates.values():",
    "            article.score = self._score_article(article, user)",
    "            scored.append(article)",
    "        scored.sort(key=lambda a: a.score, reverse=True)",
    "        self.user_feed_cache[user_id] = scored",
    "        return self._paginate(scored, page, page_size)",
    "    ",
    "    def _score_article(self, article: Article, user: User) -> float:",
    "        score = 0.0",
    "        if article.publisher_id in user.followed_publishers:",
    "            score += 2.0",
    "        if set(article.categories) & user.interests:",
    "            score += 1.5",
    "        age_hours = (time.time() - article.published_at) / 3600",
    "        score += max(0, 1.0 - age_hours / 24)",
    "        affinity = user.publisher_affinity.get(article.publisher_id, 0)",
    "        score += min(affinity * 0.3, 1.5)",
    "        score += article.popularity * 0.5",
    "        return score",
    "    ",
    "    def ingest_article(self, article: Article) -> None:",
    "        self.articles[article.id] = article",
    "        pub_articles = self.publisher_cache[article.publisher_id]",
    "        pub_articles.insert(0, article)",
    "        self.publisher_cache[article.publisher_id] = pub_articles[:50]",
    "        for category in article.categories:",
    "            cat_articles = self.category_cache[category]",
    "            cat_articles.append(article)",
    "            cat_articles.sort(key=lambda a: a.published_at, reverse=True)",
    "            self.category_cache[category] = cat_articles[:self.CACHE_SIZE]",
    "    ",
    "    def _get_or_create_user(self, user_id: str) -> User:",
    "        if user_id not in self.users:",
    "            self.users[user_id] = User(id=user_id)",
    "        return self.users[user_id]",
    "    ",
    "    def _get_trending_articles(self) -> List[Article]:",
    "        all_articles = list(self.articles.values())",
    "        all_articles.sort(key=lambda a: (a.popularity, a.published_at), reverse=True)",
    "        return all_articles[:50]",
    "    ",
    "    def _paginate(self, items: List[Article], page: int, page_size: int) -> List[Article]:",
    "        start = page * page_size",
    "        return items[start:start + page_size]",
    "    ",
    "    # ========== Part 2 Methods (unchanged) ==========",
    "    ",
    "    def publish_breaking_news(self, publisher_id: str, article: Article, priority: int) -> None:",
    "        self.ingest_article(article)",
    "        if priority < self.BREAKING_NEWS_THRESHOLD:",
    "            return",
    "        audience: Set[str] = set()",
    "        audience.update(self.publisher_followers.get(publisher_id, set()))",
    "        for category in article.categories:",
    "            audience.update(self.category_subscribers.get(category, set()))",
    "        now = time.time()",
    "        for user_id in audience:",
    "            if self._should_notify(user_id, article.categories, now):",
    "                self._create_notification(user_id, article, priority, now)",
    "    ",
    "    def set_notification_preferences(self, user_id: str, prefs: NotificationPrefs) -> None:",
    "        self._get_or_create_user(user_id)",
    "        self.notification_prefs[user_id] = prefs",
    "    ",
    "    def get_pending_notifications(self, user_id: str) -> List[Notification]:",
    "        notifications = self.pending_notifications.get(user_id, [])",
    "        unread = [n for n in notifications if not n.read]",
    "        unread.sort(key=lambda n: (n.priority, n.created_at), reverse=True)",
    "        return unread",
    "    ",
    "    def mark_notification_read(self, user_id: str, notification_id: str) -> None:",
    "        for notification in self.pending_notifications.get(user_id, []):",
    "            if notification.id == notification_id:",
    "                notification.read = True",
    "                break",
    "    ",
    "    def _should_notify(self, user_id: str, categories: List[str], now: float) -> bool:",
    "        prefs = self.notification_prefs.get(user_id, NotificationPrefs())",
    "        if not prefs.enabled:",
    "            return False",
    "        if prefs.categories and not set(categories) & set(prefs.categories):",
    "            return False",
    "        current_hour = int(time.localtime(now).tm_hour)",
    "        if prefs.quiet_hours_start > prefs.quiet_hours_end:",
    "            if current_hour >= prefs.quiet_hours_start or current_hour < prefs.quiet_hours_end:",
    "                return False",
    "        else:",
    "            if prefs.quiet_hours_start <= current_hour < prefs.quiet_hours_end:",
    "                return False",
    "        self._maybe_reset_daily_count(user_id, now)",
    "        if self.daily_notification_count[user_id] >= prefs.max_daily:",
    "            return False",
    "        return True",
    "    ",
    "    def _create_notification(self, user_id: str, article: Article, priority: int, now: float) -> None:",
    "        self._notification_counter += 1",
    "        notification = Notification(",
    "            id=f\"notif_{self._notification_counter}\", user_id=user_id,",
    "            article_id=article.id, title=article.title, priority=priority, created_at=now)",
    "        self.pending_notifications[user_id].append(notification)",
    "        self.daily_notification_count[user_id] += 1",
    "    ",
    "    def _maybe_reset_daily_count(self, user_id: str, now: float) -> None:",
    "        last_reset = self.last_count_reset.get(user_id, 0)",
    "        if now - last_reset > 86400:",
    "            self.daily_notification_count[user_id] = 0",
    "            self.last_count_reset[user_id] = now",
    "    ",
    "    # ========== Part 3: ML Recommendation Methods ==========",
    "    ",
    "    def record_engagement(self, user_id: str, article_id: str,",
    "                          engagement_type: EngagementType, duration_seconds: int) -> None:",
    "        \"\"\"Record engagement for ML training. Updates user profile immediately (online learning).\"\"\"",
    "        if article_id not in self.articles:",
    "            return",
    "        self._get_or_create_user(user_id)",
    "        ",
    "        # Store raw engagement",
    "        engagement = Engagement(article_id, engagement_type, duration_seconds, time.time())",
    "        self.user_engagements[user_id].append(engagement)",
    "        ",
    "        # Online learning: update category weights immediately",
    "        article = self.articles[article_id]",
    "        weight = self._get_engagement_weight(engagement_type, duration_seconds)",
    "        for cat in article.categories:",
    "            self.user_category_weights[user_id][cat] += weight",
    "        ",
    "        # Invalidate caches",
    "        self.similar_users_cache.pop(user_id, None)",
    "        self.user_feed_cache.pop(user_id, None)",
    "    ",
    "    def _get_engagement_weight(self, engagement_type: EngagementType, duration_seconds: int) -> float:",
    "        \"\"\"Weight based on engagement strength. SHARE > SAVE > READ (duration-based) > CLICK.\"\"\"",
    "        if engagement_type == EngagementType.SHARE:",
    "            return 5.0",
    "        elif engagement_type == EngagementType.SAVE:",
    "            return 3.0",
    "        elif engagement_type == EngagementType.READ:",
    "            return max(1.0, duration_seconds / 60)  # 1 point per minute, min 1",
    "        else:  # CLICK",
    "            return 0.5",
    "    ",
    "    def get_recommendations(self, user_id: str, count: int) -> List[Article]:",
    "        \"\"\"ML-ranked recommendations using content + collaborative filtering.\"\"\"",
    "        self._get_or_create_user(user_id)",
    "        user_weights = dict(self.user_category_weights.get(user_id, {}))",
    "        ",
    "        # Filter out already-engaged articles",
    "        seen = {e.article_id for e in self.user_engagements.get(user_id, [])}",
    "        ",
    "        # Score all candidate articles",
    "        candidates = []",
    "        for article in self.articles.values():",
    "            if article.id in seen:",
    "                continue",
    "            score = self._ml_score_article(article, user_id, user_weights)",
    "            article.score = score",
    "            candidates.append(article)",
    "        ",
    "        candidates.sort(key=lambda a: a.score, reverse=True)",
    "        return self._diversify_results(candidates, count)",
    "    ",
    "    def _ml_score_article(self, article: Article, user_id: str,",
    "                          user_weights: Dict[str, float]) -> float:",
    "        \"\"\"Combined scoring: content match + collaborative + freshness + popularity.\"\"\"",
    "        score = 0.0",
    "        ",
    "        # Content-based: match categories with user preferences",
    "        for cat in article.categories:",
    "            score += user_weights.get(cat, 0) * 0.3",
    "        ",
    "        # Collaborative: boost if similar users engaged",
    "        for sim_user in self.get_similar_users(user_id, 5):",
    "            for eng in self.user_engagements.get(sim_user, []):",
    "                if eng.article_id == article.id:",
    "                    score += 1.5",
    "                    break",
    "        ",
    "        # Freshness boost (decays over 24 hours)",
    "        age_hours = (time.time() - article.published_at) / 3600",
    "        score += max(0, 2.0 - age_hours / 12)",
    "        ",
    "        # Popularity signal",
    "        score += article.popularity * 0.3",
    "        ",
    "        # Cold start boost: encourage exploration for new users",
    "        if not user_weights:",
    "            score += 1.0",
    "        ",
    "        return score",
    "    ",
    "    def get_similar_users(self, user_id: str, count: int) -> List[str]:",
    "        \"\"\"Find users with similar category preferences (cosine similarity).\"\"\"",
    "        if user_id in self.similar_users_cache:",
    "            return self.similar_users_cache[user_id][:count]",
    "        ",
    "        user_weights = self.user_category_weights.get(user_id, {})",
    "        if not user_weights:",
    "            return []",
    "        ",
    "        similarities = []",
    "        for other_id, other_weights in self.user_category_weights.items():",
    "            if other_id == user_id:",
    "                continue",
    "            sim = self._cosine_similarity(user_weights, other_weights)",
    "            if sim > 0:",
    "                similarities.append((other_id, sim))",
    "        ",
    "        similarities.sort(key=lambda x: x[1], reverse=True)",
    "        result = [uid for uid, _ in similarities[:count * 2]]  # Cache more for reuse",
    "        self.similar_users_cache[user_id] = result",
    "        return result[:count]",
    "    ",
    "    def _cosine_similarity(self, w1: Dict[str, float], w2: Dict[str, float]) -> float:",
    "        \"\"\"Cosine similarity between two category weight vectors.\"\"\"",
    "        common = set(w1.keys()) & set(w2.keys())",
    "        if not common:",
    "            return 0.0",
    "        dot = sum(w1[k] * w2[k] for k in common)",
    "        norm1 = math.sqrt(sum(v * v for v in w1.values()))",
    "        norm2 = math.sqrt(sum(v * v for v in w2.values()))",
    "        return dot / (norm1 * norm2) if norm1 and norm2 else 0.0",
    "    ",
    "    def _diversify_results(self, candidates: List[Article], count: int) -> List[Article]:",
    "        \"\"\"Ensure variety: avoid too many articles from same category.\"\"\"",
    "        result, seen_cats = [], set()",
    "        for article in candidates:",
    "            primary_cat = article.categories[0] if article.categories else None",
    "            # Allow if new category or we haven't filled half the slots yet",
    "            if primary_cat not in seen_cats or len(result) < count // 2:",
    "                result.append(article)",
    "                if primary_cat:",
    "                    seen_cats.add(primary_cat)",
    "            if len(result) >= count:",
    "                break",
    "        # Fill remaining if needed",
    "        for article in candidates:",
    "            if article not in result:",
    "                result.append(article)",
    "            if len(result) >= count:",
    "                break",
    "        return result",
    "    ",
    "    def train_model(self) -> None:",
    "        \"\"\"Batch job: recompute user profiles from engagement history.\"\"\"",
    "        self.user_category_weights.clear()",
    "        for user_id, engagements in self.user_engagements.items():",
    "            for eng in engagements:",
    "                if eng.article_id in self.articles:",
    "                    article = self.articles[eng.article_id]",
    "                    weight = self._get_engagement_weight(eng.engagement_type, eng.duration_seconds)",
    "                    for cat in article.categories:",
    "                        self.user_category_weights[user_id][cat] += weight",
    "        self.similar_users_cache.clear()",
    "",
    "",
    "if __name__ == '__main__':",
    "    print(\"=\" * 60)",
    "    print(\"PART 3: ML-POWERED RECOMMENDATIONS DEMO\")",
    "    print(\"=\" * 60)",
    "    ",
    "    agg = NewsAggregator()",
    "    now = time.time()",
    "    ",
    "    # Setup publishers and articles",
    "    agg.register_publisher(\"pub_tc\", \"TechCrunch\", \"rss\", [\"Technology\"])",
    "    agg.register_publisher(\"pub_espn\", \"ESPN\", \"rss\", [\"Sports\"])",
    "    ",
    "    tech_articles = [",
    "        Article(f\"tech_{i}\", \"pub_tc\", f\"Tech Article {i}\", \"url\",",
    "                [\"Technology\", \"AI\"], now - i * 3600, 0.8)",
    "        for i in range(1, 8)",
    "    ]",
    "    sports_articles = [",
    "        Article(f\"sports_{i}\", \"pub_espn\", f\"Sports Article {i}\", \"url\",",
    "                [\"Sports\"], now - i * 3600, 0.7)",
    "        for i in range(1, 5)",
    "    ]",
    "    for art in tech_articles + sports_articles:",
    "        agg.ingest_article(art)",
    "    print(f\"\\n[1] Ingested {len(tech_articles)} tech + {len(sports_articles)} sports articles\")",
    "    ",
    "    # Simulate user engagement - user_1 prefers tech",
    "    print(\"\\n[2] Recording engagements for user_1 (tech preference):\")",
    "    agg.record_engagement(\"user_1\", \"tech_1\", EngagementType.READ, 180)",
    "    print(\"    READ tech_1 for 180s (3 min)\")",
    "    agg.record_engagement(\"user_1\", \"tech_2\", EngagementType.READ, 240)",
    "    print(\"    READ tech_2 for 240s (4 min)\")",
    "    agg.record_engagement(\"user_1\", \"tech_3\", EngagementType.SHARE, 0)",
    "    print(\"    SHARED tech_3\")",
    "    agg.record_engagement(\"user_1\", \"sports_1\", EngagementType.CLICK, 10)",
    "    print(\"    CLICKED sports_1 (10s bounce)\")",
    "    ",
    "    # Check learned profile",
    "    weights = agg.user_category_weights[\"user_1\"]",
    "    print(f\"\\n[3] Learned user profile: {dict(weights)}\")",
    "    ",
    "    # Get recommendations",
    "    recs = agg.get_recommendations(\"user_1\", 5)",
    "    print(f\"\\n[4] Top 5 recommendations for user_1:\")",
    "    for i, art in enumerate(recs, 1):",
    "        print(f\"    {i}. {art.title} (categories: {art.categories}, score: {art.score:.2f})\")",
    "    ",
    "    # Similar users test",
    "    print(\"\\n[5] Setting up user_2 with similar tech preference:\")",
    "    agg.record_engagement(\"user_2\", \"tech_4\", EngagementType.READ, 200)",
    "    agg.record_engagement(\"user_2\", \"tech_5\", EngagementType.SAVE, 0)",
    "    ",
    "    similar = agg.get_similar_users(\"user_1\", 3)",
    "    print(f\"    Similar to user_1: {similar}\")",
    "    ",
    "    # Collaborative boost: user_2's favorites boost user_1's recs",
    "    recs_after = agg.get_recommendations(\"user_1\", 5)",
    "    print(f\"\\n[6] Recommendations after collaborative signal:\")",
    "    for i, art in enumerate(recs_after, 1):",
    "        print(f\"    {i}. {art.title} (score: {art.score:.2f})\")",
    "    ",
    "    # Cold start user",
    "    print(\"\\n[7] Cold start user (no engagements):\")",
    "    cold_recs = agg.get_recommendations(\"new_user\", 3)",
    "    print(f\"    Recommendations: {[a.title for a in cold_recs]}\")",
    "    ",
    "    # Batch training",
    "    print(\"\\n[8] Running train_model() batch job...\")",
    "    agg.train_model()",
    "    print(f\"    Profiles recomputed, cache cleared\")",
    "    ",
    "    print(\"\\n\" + \"=\" * 60)",
    "    print(\"DEMO COMPLETE - ML recommendations with collaborative filtering\")",
    "    print(\"=\" * 60)"
  ],
  "solution_java_lines": [
    "import java.util.*;",
    "import java.util.stream.Collectors;",
    "",
    "public class NewsAggregator {",
    "    ",
    "    enum EngagementType { CLICK, READ, SHARE, SAVE }",
    "    ",
    "    static class Article {",
    "        String id, publisherId, title, url;",
    "        List<String> categories;",
    "        long publishedAt;",
    "        double popularity, score;",
    "        Article(String id, String publisherId, String title, String url,",
    "                List<String> categories, long publishedAt, double popularity) {",
    "            this.id = id; this.publisherId = publisherId; this.title = title;",
    "            this.url = url; this.categories = categories;",
    "            this.publishedAt = publishedAt; this.popularity = popularity;",
    "        }",
    "    }",
    "    ",
    "    static class Publisher {",
    "        String id, name, rssUrl;",
    "        List<String> categories;",
    "        Publisher(String id, String name, String rssUrl, List<String> categories) {",
    "            this.id = id; this.name = name; this.rssUrl = rssUrl; this.categories = categories;",
    "        }",
    "    }",
    "    ",
    "    static class User {",
    "        String id;",
    "        Set<String> followedPublishers = new HashSet<>();",
    "        Set<String> interests = new HashSet<>();",
    "        Map<String, Integer> publisherAffinity = new HashMap<>();",
    "        User(String id) { this.id = id; }",
    "    }",
    "    ",
    "    static class NotificationPrefs {",
    "        int maxDaily = 5;",
    "        List<String> categories = new ArrayList<>();",
    "        int quietHoursStart = 22, quietHoursEnd = 8;",
    "        boolean enabled = true;",
    "    }",
    "    ",
    "    static class Notification {",
    "        String id, userId, articleId, title;",
    "        int priority;",
    "        long createdAt;",
    "        boolean read = false;",
    "        Notification(String id, String userId, String articleId, String title, int priority, long createdAt) {",
    "            this.id = id; this.userId = userId; this.articleId = articleId;",
    "            this.title = title; this.priority = priority; this.createdAt = createdAt;",
    "        }",
    "    }",
    "    ",
    "    // Part 3: Engagement record",
    "    static class Engagement {",
    "        String articleId;",
    "        EngagementType type;",
    "        int durationSeconds;",
    "        long timestamp;",
    "        Engagement(String articleId, EngagementType type, int durationSeconds, long timestamp) {",
    "            this.articleId = articleId; this.type = type;",
    "            this.durationSeconds = durationSeconds; this.timestamp = timestamp;",
    "        }",
    "    }",
    "    ",
    "    // Part 1 & 2 fields",
    "    private Map<String, Publisher> publishers = new HashMap<>();",
    "    private Map<String, User> users = new HashMap<>();",
    "    private Map<String, Article> articles = new HashMap<>();",
    "    private Map<String, List<Article>> categoryCache = new HashMap<>();",
    "    private Map<String, List<Article>> publisherCache = new HashMap<>();",
    "    private Map<String, List<Article>> userFeedCache = new HashMap<>();",
    "    private Map<String, NotificationPrefs> notificationPrefs = new HashMap<>();",
    "    private Map<String, List<Notification>> pendingNotifications = new HashMap<>();",
    "    private Map<String, Integer> dailyNotificationCount = new HashMap<>();",
    "    private Map<String, Long> lastCountReset = new HashMap<>();",
    "    private Map<String, Set<String>> categorySubscribers = new HashMap<>();",
    "    private Map<String, Set<String>> publisherFollowers = new HashMap<>();",
    "    private static final int CACHE_SIZE = 100;",
    "    private static final int BREAKING_NEWS_THRESHOLD = 8;",
    "    private int notificationCounter = 0;",
    "    ",
    "    // Part 3: ML recommendation fields",
    "    private Map<String, List<Engagement>> userEngagements = new HashMap<>();",
    "    private Map<String, Map<String, Double>> userCategoryWeights = new HashMap<>();",
    "    private Map<String, List<String>> similarUsersCache = new HashMap<>();",
    "    ",
    "    // ========== Part 1 & 2 Methods (unchanged - abbreviated) ==========",
    "    ",
    "    public void registerPublisher(String id, String name, String rssUrl, List<String> categories) {",
    "        publishers.put(id, new Publisher(id, name, rssUrl, categories));",
    "    }",
    "    ",
    "    public void followPublisher(String userId, String publisherId) {",
    "        User user = getOrCreateUser(userId);",
    "        user.followedPublishers.add(publisherId);",
    "        userFeedCache.remove(userId);",
    "        publisherFollowers.computeIfAbsent(publisherId, k -> new HashSet<>()).add(userId);",
    "    }",
    "    ",
    "    public void setUserInterests(String userId, List<String> categories) {",
    "        User user = getOrCreateUser(userId);",
    "        for (String old : user.interests)",
    "            if (categorySubscribers.containsKey(old)) categorySubscribers.get(old).remove(userId);",
    "        user.interests = new HashSet<>(categories);",
    "        for (String cat : categories)",
    "            categorySubscribers.computeIfAbsent(cat, k -> new HashSet<>()).add(userId);",
    "        userFeedCache.remove(userId);",
    "    }",
    "    ",
    "    public void ingestArticle(Article article) {",
    "        articles.put(article.id, article);",
    "        publisherCache.computeIfAbsent(article.publisherId, k -> new ArrayList<>()).add(0, article);",
    "        for (String cat : article.categories) {",
    "            categoryCache.computeIfAbsent(cat, k -> new ArrayList<>()).add(article);",
    "            categoryCache.get(cat).sort((a, b) -> Long.compare(b.publishedAt, a.publishedAt));",
    "            if (categoryCache.get(cat).size() > CACHE_SIZE)",
    "                categoryCache.put(cat, new ArrayList<>(categoryCache.get(cat).subList(0, CACHE_SIZE)));",
    "        }",
    "    }",
    "    ",
    "    private User getOrCreateUser(String userId) {",
    "        return users.computeIfAbsent(userId, User::new);",
    "    }",
    "    ",
    "    // ========== Part 3: ML Recommendation Methods ==========",
    "    ",
    "    public void recordEngagement(String userId, String articleId,",
    "                                  EngagementType type, int durationSeconds) {",
    "        if (!articles.containsKey(articleId)) return;",
    "        getOrCreateUser(userId);",
    "        ",
    "        Engagement eng = new Engagement(articleId, type, durationSeconds, System.currentTimeMillis());",
    "        userEngagements.computeIfAbsent(userId, k -> new ArrayList<>()).add(eng);",
    "        ",
    "        // Online learning: update category weights",
    "        Article article = articles.get(articleId);",
    "        double weight = getEngagementWeight(type, durationSeconds);",
    "        Map<String, Double> weights = userCategoryWeights.computeIfAbsent(userId, k -> new HashMap<>());",
    "        for (String cat : article.categories)",
    "            weights.merge(cat, weight, Double::sum);",
    "        ",
    "        // Invalidate caches",
    "        similarUsersCache.remove(userId);",
    "        userFeedCache.remove(userId);",
    "    }",
    "    ",
    "    private double getEngagementWeight(EngagementType type, int durationSeconds) {",
    "        switch (type) {",
    "            case SHARE: return 5.0;",
    "            case SAVE: return 3.0;",
    "            case READ: return Math.max(1.0, durationSeconds / 60.0);",
    "            default: return 0.5;",
    "        }",
    "    }",
    "    ",
    "    public List<Article> getRecommendations(String userId, int count) {",
    "        getOrCreateUser(userId);",
    "        Map<String, Double> userWeights = userCategoryWeights.getOrDefault(userId, Map.of());",
    "        Set<String> seen = userEngagements.getOrDefault(userId, List.of()).stream()",
    "            .map(e -> e.articleId).collect(Collectors.toSet());",
    "        ",
    "        List<Article> candidates = new ArrayList<>();",
    "        for (Article article : articles.values()) {",
    "            if (seen.contains(article.id)) continue;",
    "            article.score = mlScoreArticle(article, userId, userWeights);",
    "            candidates.add(article);",
    "        }",
    "        candidates.sort((a, b) -> Double.compare(b.score, a.score));",
    "        return diversifyResults(candidates, count);",
    "    }",
    "    ",
    "    private double mlScoreArticle(Article article, String userId, Map<String, Double> userWeights) {",
    "        double score = 0.0;",
    "        ",
    "        // Content-based scoring",
    "        for (String cat : article.categories)",
    "            score += userWeights.getOrDefault(cat, 0.0) * 0.3;",
    "        ",
    "        // Collaborative filtering",
    "        for (String simUser : getSimilarUsers(userId, 5)) {",
    "            for (Engagement eng : userEngagements.getOrDefault(simUser, List.of())) {",
    "                if (eng.articleId.equals(article.id)) { score += 1.5; break; }",
    "            }",
    "        }",
    "        ",
    "        // Freshness boost",
    "        double ageHours = (System.currentTimeMillis() - article.publishedAt) / 3600000.0;",
    "        score += Math.max(0, 2.0 - ageHours / 12);",
    "        ",
    "        // Popularity + cold start",
    "        score += article.popularity * 0.3;",
    "        if (userWeights.isEmpty()) score += 1.0;",
    "        ",
    "        return score;",
    "    }",
    "    ",
    "    public List<String> getSimilarUsers(String userId, int count) {",
    "        if (similarUsersCache.containsKey(userId))",
    "            return similarUsersCache.get(userId).subList(0, Math.min(count, similarUsersCache.get(userId).size()));",
    "        ",
    "        Map<String, Double> userWeights = userCategoryWeights.get(userId);",
    "        if (userWeights == null || userWeights.isEmpty()) return List.of();",
    "        ",
    "        List<Map.Entry<String, Double>> sims = new ArrayList<>();",
    "        for (var entry : userCategoryWeights.entrySet()) {",
    "            if (entry.getKey().equals(userId)) continue;",
    "            double sim = cosineSimilarity(userWeights, entry.getValue());",
    "            if (sim > 0) sims.add(Map.entry(entry.getKey(), sim));",
    "        }",
    "        sims.sort((a, b) -> Double.compare(b.getValue(), a.getValue()));",
    "        List<String> result = sims.stream().map(Map.Entry::getKey).limit(count * 2).collect(Collectors.toList());",
    "        similarUsersCache.put(userId, result);",
    "        return result.subList(0, Math.min(count, result.size()));",
    "    }",
    "    ",
    "    private double cosineSimilarity(Map<String, Double> w1, Map<String, Double> w2) {",
    "        Set<String> common = new HashSet<>(w1.keySet());",
    "        common.retainAll(w2.keySet());",
    "        if (common.isEmpty()) return 0.0;",
    "        double dot = common.stream().mapToDouble(k -> w1.get(k) * w2.get(k)).sum();",
    "        double norm1 = Math.sqrt(w1.values().stream().mapToDouble(v -> v * v).sum());",
    "        double norm2 = Math.sqrt(w2.values().stream().mapToDouble(v -> v * v).sum());",
    "        return (norm1 > 0 && norm2 > 0) ? dot / (norm1 * norm2) : 0.0;",
    "    }",
    "    ",
    "    private List<Article> diversifyResults(List<Article> candidates, int count) {",
    "        List<Article> result = new ArrayList<>();",
    "        Set<String> seenCats = new HashSet<>();",
    "        for (Article art : candidates) {",
    "            String primaryCat = art.categories.isEmpty() ? null : art.categories.get(0);",
    "            if (!seenCats.contains(primaryCat) || result.size() < count / 2) {",
    "                result.add(art);",
    "                if (primaryCat != null) seenCats.add(primaryCat);",
    "            }",
    "            if (result.size() >= count) break;",
    "        }",
    "        for (Article art : candidates) {",
    "            if (!result.contains(art)) result.add(art);",
    "            if (result.size() >= count) break;",
    "        }",
    "        return result;",
    "    }",
    "    ",
    "    public void trainModel() {",
    "        userCategoryWeights.clear();",
    "        for (var entry : userEngagements.entrySet()) {",
    "            String userId = entry.getKey();",
    "            for (Engagement eng : entry.getValue()) {",
    "                if (articles.containsKey(eng.articleId)) {",
    "                    Article art = articles.get(eng.articleId);",
    "                    double weight = getEngagementWeight(eng.type, eng.durationSeconds);",
    "                    Map<String, Double> weights = userCategoryWeights.computeIfAbsent(userId, k -> new HashMap<>());",
    "                    for (String cat : art.categories) weights.merge(cat, weight, Double::sum);",
    "                }",
    "            }",
    "        }",
    "        similarUsersCache.clear();",
    "    }",
    "    ",
    "    public static void main(String[] args) {",
    "        System.out.println(\"=\".repeat(60));",
    "        System.out.println(\"PART 3: ML-POWERED RECOMMENDATIONS DEMO\");",
    "        System.out.println(\"=\".repeat(60));",
    "        ",
    "        NewsAggregator agg = new NewsAggregator();",
    "        long now = System.currentTimeMillis();",
    "        ",
    "        // Setup articles",
    "        for (int i = 1; i <= 6; i++)",
    "            agg.ingestArticle(new Article(\"tech_\" + i, \"pub_tc\", \"Tech Article \" + i,",
    "                \"url\", List.of(\"Technology\", \"AI\"), now - i * 3600000, 0.8));",
    "        for (int i = 1; i <= 3; i++)",
    "            agg.ingestArticle(new Article(\"sports_\" + i, \"pub_espn\", \"Sports Article \" + i,",
    "                \"url\", List.of(\"Sports\"), now - i * 3600000, 0.7));",
    "        System.out.println(\"\\n[1] Ingested tech and sports articles\");",
    "        ",
    "        // Record engagements - user prefers tech",
    "        agg.recordEngagement(\"user_1\", \"tech_1\", EngagementType.READ, 180);",
    "        agg.recordEngagement(\"user_1\", \"tech_2\", EngagementType.SHARE, 0);",
    "        agg.recordEngagement(\"user_1\", \"sports_1\", EngagementType.CLICK, 5);",
    "        System.out.println(\"[2] Recorded engagements for user_1 (tech preference)\");",
    "        ",
    "        // Get recommendations",
    "        List<Article> recs = agg.getRecommendations(\"user_1\", 5);",
    "        System.out.println(\"\\n[3] Top 5 recommendations:\");",
    "        for (int i = 0; i < recs.size(); i++)",
    "            System.out.printf(\"    %d. %s (score: %.2f)%n\", i + 1, recs.get(i).title, recs.get(i).score);",
    "        ",
    "        // Similar user",
    "        agg.recordEngagement(\"user_2\", \"tech_3\", EngagementType.READ, 200);",
    "        List<String> similar = agg.getSimilarUsers(\"user_1\", 3);",
    "        System.out.println(\"\\n[4] Similar users to user_1: \" + similar);",
    "        ",
    "        System.out.println(\"\\n\" + \"=\".repeat(60));",
    "        System.out.println(\"DEMO COMPLETE\");",
    "    }",
    "}"
  ],
  "code_walkthrough": [
    {
      "lines": "1-60",
      "explanation": "Data classes including new EngagementType enum and Engagement dataclass for tracking user interactions"
    },
    {
      "lines": "80-95",
      "explanation": "Part 3 fields: user_engagements stores raw events, user_category_weights is the learned profile, similar_users_cache for fast collaborative lookups"
    },
    {
      "lines": "200-220",
      "explanation": "record_engagement: Stores event, calculates weight based on type/duration, updates category weights immediately (online learning), invalidates relevant caches"
    },
    {
      "lines": "222-235",
      "explanation": "_get_engagement_weight: SHARE (5.0) > SAVE (3.0) > READ (duration-based) > CLICK (0.5). Long reads signal strong interest."
    },
    {
      "lines": "237-255",
      "explanation": "get_recommendations: Filters seen articles, scores remaining with ML formula, sorts, and applies diversity filter"
    },
    {
      "lines": "257-280",
      "explanation": "_ml_score_article: Combines content match (category weights), collaborative signal (similar users), freshness, popularity, and cold start boost"
    },
    {
      "lines": "282-305",
      "explanation": "get_similar_users: Computes cosine similarity of category weight vectors, caches result for efficiency"
    },
    {
      "lines": "315-330",
      "explanation": "_diversify_results: Ensures top results have varied primary categories - avoids all-politics or all-sports feeds"
    }
  ],
  "complexity_analysis": {
    "time": {
      "new_methods": {
        "record_engagement": {
          "complexity": "O(C)",
          "explanation": "C = categories in article. Update weight for each category, constant-time hash ops."
        },
        "get_recommendations": {
          "complexity": "O(A \u00d7 (C + S \u00d7 E_sim))",
          "explanation": "A = articles, C = categories per article, S = similar users checked, E_sim = engagements per similar user. In practice, S and E_sim are small constants."
        },
        "get_similar_users": {
          "complexity": "O(U \u00d7 K) first call, O(1) cached",
          "explanation": "U = users, K = categories. Cosine similarity requires iterating all users once, then cached."
        },
        "train_model": {
          "complexity": "O(E_total \u00d7 C_avg)",
          "explanation": "Replay all engagements, update weights for each category."
        }
      },
      "overall_change": "Part 1 & 2 operations unchanged. get_recommendations is the new expensive operation but practical for 10K articles with caching."
    },
    "space": {
      "additional_space": "O(U \u00d7 E_avg + U \u00d7 K)",
      "explanation": "E_avg = avg engagements per user (stored as log), K = categories (for weight vectors). Similar users cache adds O(U \u00d7 S) where S = cached similar users per user."
    }
  },
  "dry_run": {
    "example_input": "User reads tech articles, clicks away from sports, then requests recommendations",
    "steps": [
      {
        "step": 1,
        "action": "record_engagement('user_1', 'tech_1', READ, 180)",
        "state": "user_category_weights['user_1'] = {'Technology': 3.0, 'AI': 3.0}",
        "explanation": "READ 180s = 3.0 weight. Article has categories [Technology, AI], each gets +3.0"
      },
      {
        "step": 2,
        "action": "record_engagement('user_1', 'tech_2', SHARE, 0)",
        "state": "weights = {'Technology': 8.0, 'AI': 8.0}",
        "explanation": "SHARE = 5.0 weight. Both categories get +5.0"
      },
      {
        "step": 3,
        "action": "record_engagement('user_1', 'sports_1', CLICK, 10)",
        "state": "weights = {'Technology': 8.0, 'AI': 8.0, 'Sports': 0.5}",
        "explanation": "CLICK = 0.5 weight. Sports gets only +0.5 (weak signal)"
      },
      {
        "step": 4,
        "action": "get_recommendations('user_1', 5)",
        "state": "Scoring articles...",
        "explanation": "Filter out tech_1, tech_2, sports_1 (already seen). Score remaining articles."
      },
      {
        "step": 5,
        "action": "Score tech_3",
        "state": "score = 2.4 (content) + 0 (collab) + 1.75 (fresh) + 0.24 (pop) = 4.39",
        "explanation": "Content: (8.0 + 8.0) \u00d7 0.3 \u00d7 0.5 = 2.4 for matching categories"
      },
      {
        "step": 6,
        "action": "Score sports_2",
        "state": "score = 0.15 (content) + 0 (collab) + 1.58 (fresh) + 0.21 (pop) = 1.94",
        "explanation": "Content: 0.5 \u00d7 0.3 = 0.15 for Sports. Much lower than tech articles."
      },
      {
        "step": 7,
        "action": "Diversity filter",
        "state": "Return [tech_3, tech_4, sports_2, tech_5, tech_6]",
        "explanation": "Tech articles dominate scores, but diversity ensures at least one sports in top 5"
      }
    ],
    "final_output": "Tech articles ranked first due to learned preference, with diversity ensuring variety"
  },
  "debugging_playbook": {
    "fast_sanity_checks": [
      "Single engagement should create category weights",
      "Cold start user should get trending articles"
    ],
    "likely_bugs": [
      "Not invalidating caches after engagement",
      "Division by zero in cosine similarity",
      "Including seen articles in recommendations"
    ],
    "recommended_logs_or_asserts": [
      "assert article_id in self.articles before weight update",
      "log user_category_weights after engagement",
      "assert len(recommendations) <= count"
    ],
    "how_to_localize": "1. Check if engagement recorded (print user_engagements). 2. Check if weights updated (print user_category_weights). 3. Check scoring (print individual scores). 4. Check filtering (print seen set)."
  },
  "edge_cases": [
    {
      "case": "Cold start user (no engagements)",
      "handling": "Returns trending articles with cold start boost (+1.0)",
      "gotcha": "Don't return empty list; fall back to popular content"
    },
    {
      "case": "Article with no categories",
      "handling": "Skip content scoring, rely on collaborative + freshness",
      "gotcha": "Avoid index out of bounds on empty categories list"
    },
    {
      "case": "No similar users found",
      "handling": "Collaborative score = 0, rely on content-based only",
      "gotcha": "get_similar_users returns empty list, not None"
    },
    {
      "case": "User engaged with all articles",
      "handling": "get_recommendations returns empty list",
      "gotcha": "Valid case - user has seen everything"
    },
    {
      "case": "Very high engagement volume",
      "handling": "Weights grow large but relative ordering preserved",
      "gotcha": "Consider normalization in production"
    }
  ],
  "test_cases": [
    {
      "name": "Engagement weight calculation",
      "input": "record_engagement('u1', 'art1', READ, 120)",
      "expected": "weight = 2.0 (120/60)",
      "explanation": "READ engagement weighted by duration in minutes"
    },
    {
      "name": "ML scoring prioritizes preferences",
      "input": "User with high tech weight, low sports weight",
      "expected": "Tech articles score higher than sports",
      "explanation": "Content score = \u03a3(category_weight \u00d7 0.3) favors learned preferences"
    },
    {
      "name": "Collaborative boost",
      "input": "Similar user engaged with article X",
      "expected": "Article X gets +1.5 score for original user",
      "explanation": "If similar user read it, it's likely relevant"
    },
    {
      "name": "Diversity enforcement",
      "input": "Top 10 are all Technology articles",
      "expected": "Top 5 includes at least one non-Technology if available",
      "explanation": "Diversity filter ensures category variety"
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Not filtering already-engaged articles from recommendations",
      "why_wrong": "User doesn't want to see content they've already read/clicked",
      "correct_approach": "Build seen set from user_engagements, filter before scoring",
      "code_example_wrong": "for article in self.articles.values(): candidates.append(article)",
      "code_example_correct": "seen = {e.article_id for e in self.user_engagements[user_id]}; for article in self.articles.values(): if article.id not in seen: candidates.append(article)"
    },
    {
      "mistake": "Computing similar users on every request",
      "why_wrong": "O(U \u00d7 K) similarity computation is expensive",
      "correct_approach": "Cache similar users, invalidate only when user's profile changes",
      "code_example_wrong": "def get_similar_users(self, ...): return self._compute_all_similarities(user_id)",
      "code_example_correct": "def get_similar_users(self, ...): if user_id in self.similar_users_cache: return self.similar_users_cache[user_id]; ..."
    },
    {
      "mistake": "Not handling cold start users",
      "why_wrong": "New users have no engagement history, no category weights",
      "correct_approach": "Add cold start boost to ML score; fall back to trending/popular content",
      "code_example_wrong": "score = user_weights[cat] * 0.3  # KeyError for new user",
      "code_example_correct": "score += user_weights.get(cat, 0) * 0.3; if not user_weights: score += 1.0"
    }
  ],
  "interview_tips": {
    "how_to_present": "Start by explaining that Part 3 transforms rule-based ranking to ML-based. Key insight: represent users as learned category vectors from engagement. This enables fast content scoring and efficient similar user computation.",
    "what_to_mention": [
      "Online vs batch learning tradeoff (we do hybrid)",
      "Exploitation vs exploration balance (diversity filter + cold start boost)",
      "Why cosine similarity for users (sparse vectors, scale-invariant)",
      "Cache invalidation strategy"
    ],
    "time_allocation": "2 min understand requirements, 3 min explain approach, 8 min implement, 2 min test/discuss",
    "if_stuck": [
      "Think: what signals do we have? (engagement events)",
      "How to convert signals to scores? (weighted categories)",
      "How to find similar users? (compare category vectors)"
    ]
  },
  "connection_to_next_part": "Part 4 might add: A/B testing framework for comparing recommendation strategies, real-time model serving with feature store, or multi-objective optimization (engagement vs revenue vs diversity).",
  "communication_script": {
    "transition_from_previous": "Great, Part 2 handles notifications. For Part 3, I need to add ML-powered recommendations that learn from user behavior. The key change is tracking engagement signals and building user profiles from them.",
    "explaining_changes": "I'll add three main things: (1) record_engagement to capture user interactions, (2) user_category_weights as learned profiles updated online, (3) get_recommendations that combines content matching with collaborative filtering.",
    "while_extending_code": [
      "Adding Engagement dataclass to store user interactions...",
      "user_category_weights will be our learned user embedding - sparse vector of category affinities...",
      "The ML scoring function combines content score, collaborative signal, freshness, and diversity..."
    ],
    "after_completing": "This now handles Part 3. record_engagement is O(C) for online learning, get_recommendations is O(A \u00d7 (C + S)) with caching. The collaborative filtering finds similar users via cosine similarity of category vectors. Ready for questions?"
  },
  "time_milestones": {
    "time_budget": "12-15 minutes for this part",
    "by_2_min": "Understand: need to track engagement, build user profiles, recommend using content + collaborative filtering",
    "by_5_min": "Explain approach: category weight vectors for users, cosine similarity for similar users, combined scoring formula",
    "by_10_min": "Implement record_engagement and get_recommendations core logic",
    "warning_signs": "If still debating data structures at 5 min, simplify: just use category weights, skip neural embeddings"
  },
  "recovery_strategies": {
    "if_part_builds_wrong": "If Part 2 caches are interfering, ensure you're invalidating user_feed_cache when recording engagement",
    "if_new_requirement_unclear": "Ask: 'Should recommendations include articles the user already engaged with, or only unseen content?'",
    "if_running_behind": "Implement record_engagement + simple content-based scoring first. Add collaborative filtering as enhancement. Mention diversity filter but implement simply."
  },
  "signal_points": {
    "wow_factors_for_followup": [
      "Mentioning that category vectors are a simplified embedding - production would use learned neural embeddings",
      "Discussing cold start problem and how exploration helps",
      "Explaining cache invalidation strategy (why only invalidate on engagement, not on article ingestion)",
      "Proposing multi-armed bandit for exploration/exploitation tradeoff"
    ]
  },
  "pattern_recognition": {
    "pattern": "Feature-Based Recommendation with Collaborative Filtering",
    "indicators": [
      "Learn from user behavior",
      "Find similar users",
      "Personalized recommendations",
      "Balance relevance and diversity"
    ],
    "similar_problems": [
      "LC 1603 - Design Parking System (user state tracking)",
      "Movie recommendation systems",
      "E-commerce product recommendations"
    ],
    "template": "1. Encode users/items as feature vectors. 2. Update vectors on interactions. 3. Score items by similarity to user vector. 4. Add collaborative signal from similar users. 5. Re-rank for diversity."
  },
  "thinking_process": [
    {
      "step": 1,
      "thought": "When I see 'learn from engagement', I think implicit feedback \u2192 need to convert actions to preferences",
      "why": "Different engagement types (click vs share) indicate different preference strengths"
    },
    {
      "step": 2,
      "thought": "When I see 'similar users', I think collaborative filtering \u2192 need user-user similarity",
      "why": "Classic recommendation pattern: users who like similar things will like similar new things"
    },
    {
      "step": 3,
      "thought": "The constraint is <50ms latency \u2192 can't compute everything at request time",
      "why": "This forces pre-computation and caching of similarity, online updates for weights"
    }
  ],
  "interviewer_perspective": {
    "what_they_evaluate": [
      "Can you design a recommendation system without overthinking?",
      "Do you understand content-based vs collaborative filtering?",
      "Can you handle cold start and diversity?",
      "Is your code extensible for real ML models?"
    ],
    "bonus_points": [
      "Mentioning exploration/exploitation tradeoff",
      "Discussing how to upgrade to neural embeddings",
      "Proposing A/B testing for recommendation strategies",
      "Noting that weights should be normalized in production"
    ],
    "red_flags": [
      "Trying to implement actual neural networks",
      "Ignoring cold start users",
      "Not caching similar users computation",
      "Recommending already-seen content"
    ]
  },
  "ai_copilot_tips": {
    "what_to_do": [
      "Use AI for cosine similarity formula",
      "Let it help with dataclass boilerplate",
      "Use it for the diversity filter logic"
    ],
    "what_not_to_do": [
      "Don't let AI add complex ML libraries",
      "Verify the scoring formula makes sense",
      "Make sure cache invalidation is correct"
    ]
  },
  "red_flags_to_avoid": {
    "behavioral": [
      "Spending too long on perfect ML model design - keep it simple",
      "Not asking what 'similar users' means exactly"
    ],
    "technical": [
      "Using actual ML libraries in interview",
      "Computing similarity for every request",
      "Forgetting to filter seen articles"
    ],
    "communication": [
      "Not explaining WHY category vectors work as user embeddings",
      "Skipping explanation of the scoring formula components"
    ]
  },
  "final_checklist": {
    "before_saying_done": [
      "record_engagement stores event and updates weights",
      "get_recommendations filters seen articles and applies ML scoring",
      "get_similar_users uses cosine similarity with caching",
      "Diversity filter ensures variety in top-N",
      "Cold start handled with exploration boost"
    ],
    "quick_code_review": [
      "Engagement dataclass added",
      "user_engagements and user_category_weights fields added",
      "similar_users_cache with invalidation",
      "No modification to Part 1/2 method signatures"
    ]
  },
  "production_considerations": {
    "what_i_would_add": [
      "Feature store for real-time features",
      "Vector database (Pinecone/Milvus) for ANN similarity search",
      "Model serving infrastructure",
      "A/B testing framework",
      "Logging for model training data collection"
    ],
    "why_not_in_interview": "Focus on algorithm design, not infrastructure. Category vectors demonstrate the concept without needing ML frameworks.",
    "how_to_mention": "Say: 'In production, I'd replace category vectors with learned embeddings from a two-tower model, store in a vector DB for O(log N) ANN search, and add online learning for real-time signal incorporation.'"
  },
  "generated_at": "2026-01-19T04:58:54.177622",
  "_meta": {
    "problem_id": "news_feed_aggregator",
    "part_number": 3,
    "model": "claude-opus-4-5-20251101"
  }
}