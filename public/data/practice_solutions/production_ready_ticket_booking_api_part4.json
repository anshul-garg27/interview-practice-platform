{
  "problem_title": "Production-Ready Event Ticket Booking API - Part 4: Production Readiness",
  "part_number": 4,
  "builds_on": "Part 3",
  "difficulty": "hard",
  "problem_understanding": {
    "what_changes": "Part 4 adds production observability features: health checks for monitoring, rate limiting for abuse prevention, metrics collection for performance tracking, and structured logging for debugging. These are NOT core business logic but critical for operating the API at scale.",
    "new_requirements": [
      "Health check endpoint returning system status and dependency health",
      "Per-user per-endpoint rate limiting using sliding window algorithm",
      "Metrics collection (request counts, error rates, latencies)",
      "Structured request logging with trace IDs"
    ],
    "new_constraints": [
      "Rate limits must use sliding window (not fixed window) for fairness",
      "P99 latency calculation requires storing recent latencies",
      "Logging must NOT include sensitive data (passwords, tokens)",
      "Health check must be lightweight (called frequently by load balancers)"
    ],
    "key_insight": "Observability is not optional - you can't fix what you can't see. The sliding window rate limit provides smooth throttling without burst penalties, and structured logging enables distributed tracing across services."
  },
  "requirements_coverage": {
    "checklist": [
      {
        "requirement": "Health check endpoint",
        "how_met": "health_check() returns status, dependency checks, version, uptime",
        "gotchas": [
          "Health check itself should not be rate limited",
          "Must be fast - no expensive operations"
        ]
      },
      {
        "requirement": "Sliding window rate limiting",
        "how_met": "check_rate_limit() filters timestamps within window, compares count to limit",
        "gotchas": [
          "Must record timestamp AFTER checking (not before)",
          "Different limits per endpoint"
        ]
      },
      {
        "requirement": "Metrics collection",
        "how_met": "get_metrics() returns counts, error rate, avg/P99 latencies from in-memory storage",
        "gotchas": [
          "P99 requires sorted list - keep bounded size",
          "Error rate as percentage not fraction"
        ]
      },
      {
        "requirement": "Structured logging",
        "how_met": "log_request() creates JSON-structured log entries with request ID for tracing",
        "gotchas": [
          "Never log passwords or tokens",
          "Include duration for performance analysis"
        ]
      }
    ],
    "complexity_targets": [
      {
        "operation": "health_check",
        "target": "O(1)",
        "achieved": "O(1)",
        "why": "Simple dict reads and datetime arithmetic"
      },
      {
        "operation": "check_rate_limit",
        "target": "O(w) where w=window requests",
        "achieved": "O(w)",
        "why": "Filter timestamps in window, typically small"
      },
      {
        "operation": "get_metrics",
        "target": "O(n log n) for P99",
        "achieved": "O(n log n)",
        "why": "Sorting latencies for percentile calculation"
      }
    ],
    "non_goals": [
      "Persistent storage of metrics",
      "Distributed rate limiting",
      "Real database health checks"
    ]
  },
  "assumptions": [
    "In-memory rate limiting is sufficient (ask: 'Should I consider Redis for distributed rate limiting?')",
    "P99 calculation from last ~1000 requests is representative enough",
    "Health check doesn't need to verify external dependencies (we're in-memory)",
    "Metrics can be reset on restart (not persisted)"
  ],
  "tradeoffs": [
    {
      "decision": "Sliding window vs fixed window",
      "chosen": "Sliding window",
      "why": "Smoother rate limiting without burst penalty at window boundaries",
      "alternative": "Fixed window (simpler)",
      "when_to_switch": "If memory is constrained and approximate is acceptable"
    },
    {
      "decision": "Store all latencies vs sampled",
      "chosen": "Last 1000 latencies",
      "why": "Bounded memory while still accurate P99",
      "alternative": "Reservoir sampling",
      "when_to_switch": "For very high throughput where even 1000 is too many"
    },
    {
      "decision": "Per-request logging vs batched",
      "chosen": "Per-request",
      "why": "Simpler for interview, immediate visibility",
      "alternative": "Batch and flush periodically",
      "when_to_switch": "High throughput where per-request logging is too slow"
    }
  ],
  "extensibility_notes": {
    "what_to_keep_stable": [
      "All Part 1-3 methods unchanged",
      "Response format consistency",
      "Rate limit check returns bool"
    ],
    "what_to_change": [
      "Added metrics tracking state",
      "Added rate limit timestamp tracking",
      "Added request logging list"
    ],
    "interfaces_and_boundaries": "Observability methods are independent - they can be called without affecting core booking logic. Rate limiting is a pre-check that callers invoke before processing.",
    "invariants": [
      "Rate limit timestamps only grow (cleaned lazily)",
      "Metrics counters never decrease",
      "Latency list bounded to 1000 entries"
    ]
  },
  "visual_explanation": {
    "before_after": "```\nBEFORE (Part 3):                    AFTER (Part 4):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 TicketAPI   \u2502                     \u2502 TicketAPI                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 events      \u2502                     \u2502 events                      \u2502\n\u2502 bookings    \u2502                     \u2502 bookings                    \u2502\n\u2502 users       \u2502                     \u2502 users                       \u2502\n\u2502 tokens      \u2502                     \u2502 tokens                      \u2502\n\u2502 _lock       \u2502                     \u2502 _lock                       \u2502\n\u2502 _event_locks\u2502                     \u2502 _event_locks                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502 + _start_time      (uptime) \u2502\n                                    \u2502 + _rate_limits     (window) \u2502\n                                    \u2502 + _metrics         (counts) \u2502\n                                    \u2502 + _request_logs    (trace)  \u2502\n                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
    "algorithm_flow": "```\nSLIDING WINDOW RATE LIMIT:\n\nTime: \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n       \u2502                           \u2502\n       \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500 60 second window \u2500\u2500\u2500\u2500\u2500\u25b6\u2502\n       \u2502                           \u2502\nReqs:  R\u2081  R\u2082  R\u2083  R\u2084  R\u2085         R\u2086  R\u2087  R\u2088\n       \u2717   \u2717   \u2713   \u2713   \u2713          \u2713   \u2713   ?\n       \u2514expired\u2518   \u2514\u2500\u2500\u2500\u2500\u2500kept\u2500\u2500\u2500\u2500\u2500\u2518   \u2514new\u2500\u2518\n\nCheck at R\u2088:\n1. Filter: Remove R\u2081, R\u2082 (older than window)\n2. Count: 5 requests in window (R\u2083-R\u2087)\n3. Compare: 5 < limit(10)? \u2192 Yes, ALLOW\n4. Record: Add R\u2088 timestamp\n\nIf limit were 5:\n3. Compare: 5 >= limit(5)? \u2192 Yes, REJECT\n```"
  },
  "approaches": [
    {
      "name": "Fixed Window Rate Limiting",
      "description": "Divide time into fixed windows, count requests per window",
      "time_complexity": "O(1) check",
      "space_complexity": "O(u*e) for users*endpoints",
      "why_not_optimal": "Allows burst of 2x limit at window boundary (e.g., 10 at :59, 10 at :00)"
    },
    {
      "name": "Sliding Window (Optimal)",
      "description": "Track individual request timestamps, filter to window on each check",
      "time_complexity": "O(w) where w = requests in window (typically small)",
      "space_complexity": "O(u*e*w) for timestamps per user/endpoint",
      "key_insight": "Smooth throttling without boundary exploits; lazy cleanup is efficient enough"
    }
  ],
  "optimal_solution": {
    "explanation_md": "The solution adds **production observability** to the existing API:\n\n1. **Health Check**: Returns system status including dependency checks (event/booking counts), version, and uptime. Called by load balancers to determine if instance is healthy.\n\n2. **Rate Limiting**: Uses **sliding window algorithm** - maintains list of request timestamps per (user, endpoint) pair. On each check, filters to only timestamps within the window, then compares count to limit. This prevents the \"boundary burst\" problem of fixed windows.\n\n3. **Metrics**: Tracks total requests, successes, failures, and stores last 1000 latencies for P99 calculation. Error rate is derived from counts.\n\n4. **Logging**: Structured JSON logs with request ID (for distributed tracing), user ID, endpoint, status, and duration. Sensitive data like passwords/tokens are never logged.",
    "data_structures": [
      {
        "structure": "Dict[(user_id, endpoint), List[datetime]]",
        "purpose": "Sliding window rate limit tracking"
      },
      {
        "structure": "Dict with counters",
        "purpose": "Metrics (total, success, failed requests)"
      },
      {
        "structure": "List[float] bounded to 1000",
        "purpose": "Recent latencies for P99 calculation"
      },
      {
        "structure": "List[dict]",
        "purpose": "Structured request logs"
      }
    ],
    "algorithm_steps": [
      "health_check: Calculate uptime from start_time, return counts and status",
      "check_rate_limit: Get config for endpoint, filter timestamps > cutoff, check if count < limit, record if allowed",
      "get_metrics: Calculate error rate from counts, sort latencies for P99",
      "log_request: Create structured entry, update metric counters, bound latency list"
    ]
  },
  "solution_python_lines": [
    "from decimal import Decimal",
    "from datetime import datetime, timedelta",
    "from typing import Dict, List, Optional, Any",
    "from enum import Enum",
    "from dataclasses import dataclass",
    "import threading",
    "import hashlib",
    "import secrets",
    "from concurrent.futures import ThreadPoolExecutor, as_completed",
    "",
    "",
    "class BookingStatus(Enum):",
    "    CONFIRMED = 'CONFIRMED'",
    "    CANCELLED = 'CANCELLED'",
    "",
    "",
    "class UserRole(Enum):",
    "    ADMIN = 'ADMIN'",
    "    USER = 'USER'",
    "    GUEST = 'GUEST'",
    "",
    "",
    "@dataclass",
    "class Event:",
    "    id: str",
    "    name: str",
    "    date: datetime",
    "    venue: str",
    "    total_tickets: int",
    "    available_tickets: int",
    "    price_per_ticket: Decimal",
    "    version: int = 0",
    "",
    "",
    "@dataclass",
    "class Booking:",
    "    id: str",
    "    event_id: str",
    "    user_id: str",
    "    number_of_tickets: int",
    "    total_amount: Decimal",
    "    status: BookingStatus",
    "    created_at: datetime",
    "",
    "",
    "@dataclass",
    "class User:",
    "    id: str",
    "    username: str",
    "    password_hash: str",
    "    role: UserRole",
    "",
    "",
    "@dataclass",
    "class Response:",
    "    status: int",
    "    body: dict",
    "",
    "",
    "class TicketAPI:",
    "    \"\"\"",
    "    Ticket Booking API with Production Readiness.",
    "    Part 4: Health checks, rate limiting, metrics, logging.",
    "    \"\"\"",
    "    MAX_LOGIN_ATTEMPTS = 5",
    "    LOCKOUT_MINUTES = 15",
    "    ",
    "    # Part 4: Rate limit configuration per endpoint",
    "    RATE_LIMITS = {",
    "        '/bookings': {'limit': 10, 'window_seconds': 60},",
    "        '/events': {'limit': 60, 'window_seconds': 60},",
    "        'default': {'limit': 100, 'window_seconds': 60}",
    "    }",
    "",
    "    def __init__(self):",
    "        # Part 1 state",
    "        self.events: Dict[str, Event] = {}",
    "        self.bookings: Dict[str, Booking] = {}",
    "        self._event_counter = 0",
    "        self._booking_counter = 0",
    "        self._lock = threading.Lock()",
    "        ",
    "        # Part 2 state - Auth",
    "        self.users: Dict[str, User] = {}",
    "        self.tokens: Dict[str, dict] = {}",
    "        self.login_attempts: Dict[str, List[datetime]] = {}",
    "        self._user_counter = 0",
    "        self._seed_users()",
    "        ",
    "        # Part 3 state - Concurrency",
    "        self._event_locks: Dict[str, threading.Lock] = {}",
    "        ",
    "        # Part 4 state - Production Readiness",
    "        self._start_time = datetime.now()",
    "        self._rate_limit_timestamps: Dict[tuple, List[datetime]] = {}",
    "        self._metrics = {",
    "            'total_requests': 0,",
    "            'successful_requests': 0,",
    "            'failed_requests': 0,",
    "            'latencies_ms': []",
    "        }",
    "        self._request_logs: List[dict] = []",
    "        self._request_id_counter = 0",
    "",
    "    def _seed_users(self):",
    "        self._add_user('admin', 'admin123', UserRole.ADMIN)",
    "        self._add_user('user', 'user123', UserRole.USER)",
    "",
    "    def _add_user(self, username: str, password: str, role: UserRole) -> User:",
    "        self._user_counter += 1",
    "        user = User(",
    "            id=f'usr_{self._user_counter:03d}',",
    "            username=username,",
    "            password_hash=hashlib.sha256(password.encode()).hexdigest(),",
    "            role=role",
    "        )",
    "        self.users[username] = user",
    "        return user",
    "",
    "    def _generate_event_id(self) -> str:",
    "        self._event_counter += 1",
    "        return f'evt_{self._event_counter:03d}'",
    "",
    "    def _generate_booking_id(self) -> str:",
    "        self._booking_counter += 1",
    "        return f'bkg_{self._booking_counter:03d}'",
    "",
    "    def _error_response(self, status: int, code: str, message: str,",
    "                        details: List[dict] = None) -> Response:",
    "        body = {'success': False, 'error': {'code': code, 'message': message}}",
    "        if details:",
    "            body['error']['details'] = details",
    "        return Response(status=status, body=body)",
    "",
    "    # ========== Part 2: Auth Methods (unchanged) ==========",
    "",
    "    def _check_login_rate_limit(self, username: str) -> bool:",
    "        if username not in self.login_attempts:",
    "            return True",
    "        cutoff = datetime.now() - timedelta(minutes=self.LOCKOUT_MINUTES)",
    "        recent = [t for t in self.login_attempts[username] if t > cutoff]",
    "        self.login_attempts[username] = recent",
    "        return len(recent) < self.MAX_LOGIN_ATTEMPTS",
    "",
    "    def _record_failed_login(self, username: str):",
    "        if username not in self.login_attempts:",
    "            self.login_attempts[username] = []",
    "        self.login_attempts[username].append(datetime.now())",
    "",
    "    def _validate_token(self, token: str) -> Optional[dict]:",
    "        if not token or token not in self.tokens:",
    "            return None",
    "        info = self.tokens[token]",
    "        if datetime.now() > info['exp']:",
    "            del self.tokens[token]",
    "            return None",
    "        return info",
    "",
    "    def _authorize(self, token: str, required_role: str = 'USER') -> tuple:",
    "        auth = self._validate_token(token)",
    "        if not auth:",
    "            return None, self._error_response(401, 'UNAUTHORIZED', 'Invalid or expired token')",
    "        role_hierarchy = {'GUEST': 0, 'USER': 1, 'ADMIN': 2}",
    "        if role_hierarchy.get(auth['role'], 0) < role_hierarchy.get(required_role, 1):",
    "            return None, self._error_response(403, 'FORBIDDEN', f'{required_role} role required')",
    "        return auth, None",
    "",
    "    def login(self, request: dict) -> Response:",
    "        username = request.get('username', '')",
    "        password = request.get('password', '')",
    "        if not self._check_login_rate_limit(username):",
    "            return self._error_response(429, 'RATE_LIMITED', 'Too many login attempts')",
    "        user = self.users.get(username)",
    "        if not user or user.password_hash != hashlib.sha256(password.encode()).hexdigest():",
    "            self._record_failed_login(username)",
    "            return self._error_response(401, 'UNAUTHORIZED', 'Invalid credentials')",
    "        token = f'tok_{secrets.token_hex(16)}'",
    "        self.tokens[token] = {",
    "            'user_id': user.id, 'username': user.username,",
    "            'role': user.role.value, 'exp': datetime.now() + timedelta(hours=24)",
    "        }",
    "        return Response(status=200, body={",
    "            'success': True,",
    "            'data': {'token': token, 'userId': user.id, 'role': user.role.value}",
    "        })",
    "",
    "    def get_current_user(self, token: str) -> Response:",
    "        auth = self._validate_token(token)",
    "        if not auth:",
    "            return self._error_response(401, 'UNAUTHORIZED', 'Invalid or expired token')",
    "        return Response(status=200, body={",
    "            'success': True,",
    "            'data': {'userId': auth['user_id'], 'username': auth['username'], 'role': auth['role']}",
    "        })",
    "",
    "    def get_user_bookings(self, token: str, user_id: str) -> Response:",
    "        auth, err = self._authorize(token, 'USER')",
    "        if err:",
    "            return err",
    "        if auth['role'] != 'ADMIN' and auth['user_id'] != user_id:",
    "            return self._error_response(403, 'FORBIDDEN', 'Cannot view other users bookings')",
    "        user_bookings = [",
    "            {'bookingId': b.id, 'eventId': b.event_id, 'tickets': b.number_of_tickets,",
    "             'status': b.status.value}",
    "            for b in self.bookings.values() if b.user_id == user_id",
    "        ]",
    "        return Response(status=200, body={'success': True, 'data': {'bookings': user_bookings}})",
    "",
    "    # ========== Part 1 Methods (unchanged) ==========",
    "",
    "    def _validate_event_request(self, request: dict) -> List[dict]:",
    "        errors = []",
    "        required = ['name', 'date', 'venue', 'totalTickets', 'pricePerTicket']",
    "        for field in required:",
    "            if field not in request or request[field] is None:",
    "                errors.append({'field': field, 'issue': 'required'})",
    "        if errors:",
    "            return errors",
    "        if not isinstance(request['totalTickets'], int) or not (1 <= request['totalTickets'] <= 100000):",
    "            errors.append({'field': 'totalTickets', 'issue': 'must be 1-100000'})",
    "        return errors",
    "",
    "    def _validate_booking_request(self, request: dict) -> List[dict]:",
    "        errors = []",
    "        required = ['eventId', 'userId', 'numberOfTickets']",
    "        for field in required:",
    "            if field not in request or request[field] is None:",
    "                errors.append({'field': field, 'issue': 'required'})",
    "        if errors:",
    "            return errors",
    "        tickets = request['numberOfTickets']",
    "        if not isinstance(tickets, int) or not (1 <= tickets <= 10):",
    "            errors.append({'field': 'numberOfTickets', 'issue': 'must be 1-10'})",
    "        return errors",
    "",
    "    def create_event(self, request: dict, token: str = None) -> Response:",
    "        if token:",
    "            auth, err = self._authorize(token, 'ADMIN')",
    "            if err:",
    "                return err",
    "        errors = self._validate_event_request(request)",
    "        if errors:",
    "            return self._error_response(400, 'VALIDATION_ERROR', f\"Validation error: {errors[0]['issue']}\", errors)",
    "        event_id = self._generate_event_id()",
    "        event = Event(",
    "            id=event_id, name=request['name'],",
    "            date=datetime.fromisoformat(request['date'].replace('Z', '+00:00')),",
    "            venue=request['venue'], total_tickets=request['totalTickets'],",
    "            available_tickets=request['totalTickets'],",
    "            price_per_ticket=Decimal(str(request['pricePerTicket']))",
    "        )",
    "        self.events[event_id] = event",
    "        return Response(status=201, body={",
    "            'success': True,",
    "            'data': {'eventId': event_id, 'name': event.name, 'availableTickets': event.available_tickets}",
    "        })",
    "",
    "    def get_event(self, event_id: str) -> Response:",
    "        if event_id not in self.events:",
    "            return self._error_response(404, 'NOT_FOUND', f'Event {event_id} not found')",
    "        e = self.events[event_id]",
    "        return Response(status=200, body={",
    "            'success': True,",
    "            'data': {'eventId': e.id, 'name': e.name, 'availableTickets': e.available_tickets,",
    "                     'pricePerTicket': float(e.price_per_ticket)}",
    "        })",
    "",
    "    def list_events(self, filters: dict = None) -> Response:",
    "        filters = filters or {}",
    "        limit = min(filters.get('limit', 10), 100)",
    "        offset = max(filters.get('offset', 0), 0)",
    "        all_events = list(self.events.values())",
    "        paginated = all_events[offset:offset + limit]",
    "        return Response(status=200, body={",
    "            'success': True,",
    "            'data': {",
    "                'events': [{'eventId': e.id, 'name': e.name, 'availableTickets': e.available_tickets} for e in paginated],",
    "                'total': len(all_events), 'limit': limit, 'offset': offset",
    "            }",
    "        })",
    "",
    "    def book_tickets(self, request: dict, token: str = None) -> Response:",
    "        if token:",
    "            auth, err = self._authorize(token, 'USER')",
    "            if err:",
    "                return err",
    "        errors = self._validate_booking_request(request)",
    "        if errors:",
    "            return self._error_response(400, 'VALIDATION_ERROR', errors[0]['issue'], errors)",
    "        event_id = request['eventId']",
    "        if event_id not in self.events:",
    "            return self._error_response(404, 'NOT_FOUND', f'Event {event_id} not found')",
    "        with self._lock:",
    "            event = self.events[event_id]",
    "            num_tickets = request['numberOfTickets']",
    "            if event.available_tickets < num_tickets:",
    "                return self._error_response(409, 'INSUFFICIENT_TICKETS',",
    "                    f'Only {event.available_tickets} ticket(s) available')",
    "            booking_id = self._generate_booking_id()",
    "            total = event.price_per_ticket * num_tickets",
    "            booking = Booking(",
    "                id=booking_id, event_id=event_id, user_id=request['userId'],",
    "                number_of_tickets=num_tickets, total_amount=total,",
    "                status=BookingStatus.CONFIRMED, created_at=datetime.now()",
    "            )",
    "            event.available_tickets -= num_tickets",
    "            self.bookings[booking_id] = booking",
    "        return Response(status=201, body={",
    "            'success': True,",
    "            'data': {'bookingId': booking_id, 'eventId': event_id,",
    "                     'numberOfTickets': num_tickets, 'totalAmount': float(total), 'status': 'CONFIRMED'}",
    "        })",
    "",
    "    def cancel_booking(self, booking_id: str, token: str = None) -> Response:",
    "        if booking_id not in self.bookings:",
    "            return self._error_response(404, 'NOT_FOUND', f'Booking {booking_id} not found')",
    "        if token:",
    "            auth, err = self._authorize(token, 'USER')",
    "            if err:",
    "                return err",
    "            if auth['role'] != 'ADMIN' and auth['user_id'] != self.bookings[booking_id].user_id:",
    "                return self._error_response(403, 'FORBIDDEN', 'Cannot cancel other users bookings')",
    "        with self._lock:",
    "            booking = self.bookings[booking_id]",
    "            if booking.status == BookingStatus.CANCELLED:",
    "                return self._error_response(409, 'ALREADY_CANCELLED', 'Booking already cancelled')",
    "            self.events[booking.event_id].available_tickets += booking.number_of_tickets",
    "            booking.status = BookingStatus.CANCELLED",
    "        return Response(status=200, body={'success': True, 'data': {'bookingId': booking_id, 'status': 'CANCELLED'}})",
    "",
    "    # ========== Part 3: Concurrency Methods (unchanged) ==========",
    "",
    "    def _get_event_lock(self, event_id: str) -> threading.Lock:",
    "        with self._lock:",
    "            if event_id not in self._event_locks:",
    "                self._event_locks[event_id] = threading.Lock()",
    "            return self._event_locks[event_id]",
    "",
    "    def book_tickets_atomic(self, request: dict) -> Response:",
    "        errors = self._validate_booking_request(request)",
    "        if errors:",
    "            return self._error_response(400, 'VALIDATION_ERROR', errors[0]['issue'], errors)",
    "        event_id = request['eventId']",
    "        if event_id not in self.events:",
    "            return self._error_response(404, 'NOT_FOUND', f'Event {event_id} not found')",
    "        event_lock = self._get_event_lock(event_id)",
    "        with event_lock:",
    "            event = self.events[event_id]",
    "            num_tickets = request['numberOfTickets']",
    "            if event.available_tickets < num_tickets:",
    "                return self._error_response(409, 'INSUFFICIENT_TICKETS',",
    "                    f'Only {event.available_tickets} ticket(s) available')",
    "            with self._lock:",
    "                booking_id = self._generate_booking_id()",
    "            total = event.price_per_ticket * num_tickets",
    "            booking = Booking(",
    "                id=booking_id, event_id=event_id, user_id=request['userId'],",
    "                number_of_tickets=num_tickets, total_amount=total,",
    "                status=BookingStatus.CONFIRMED, created_at=datetime.now()",
    "            )",
    "            event.available_tickets -= num_tickets",
    "            event.version += 1",
    "            self.bookings[booking_id] = booking",
    "        return Response(status=201, body={",
    "            'success': True,",
    "            'data': {'bookingId': booking_id, 'eventId': event_id,",
    "                     'numberOfTickets': num_tickets, 'totalAmount': float(total), 'status': 'CONFIRMED'}",
    "        })",
    "",
    "    def simulate_concurrent_bookings(self, requests: List[dict]) -> List[Response]:",
    "        responses = [None] * len(requests)",
    "        def execute(args):",
    "            idx, req = args",
    "            return idx, self.book_tickets_atomic(req)",
    "        with ThreadPoolExecutor(max_workers=len(requests)) as executor:",
    "            futures = [executor.submit(execute, (i, req)) for i, req in enumerate(requests)]",
    "            for future in as_completed(futures):",
    "                idx, resp = future.result()",
    "                responses[idx] = resp",
    "        return responses",
    "",
    "    # ========== Part 4: Production Readiness Methods ==========",
    "",
    "    def health_check(self) -> Response:",
    "        \"\"\"",
    "        Returns system health status with dependency checks.",
    "        Called frequently by load balancers - must be fast.",
    "        \"\"\"",
    "        uptime = datetime.now() - self._start_time",
    "        hours, remainder = divmod(int(uptime.total_seconds()), 3600)",
    "        minutes, seconds = divmod(remainder, 60)",
    "        ",
    "        return Response(status=200, body={",
    "            'success': True,",
    "            'data': {",
    "                'status': 'healthy',",
    "                'checks': {",
    "                    'database': 'connected',",
    "                    'eventCount': len(self.events),",
    "                    'bookingCount': len(self.bookings)",
    "                },",
    "                'version': '1.0.0',",
    "                'uptime': f'{hours}h {minutes}m {seconds}s'",
    "            }",
    "        })",
    "",
    "    def get_metrics(self) -> Response:",
    "        \"\"\"",
    "        Returns current system metrics including P99 latency.",
    "        P99 requires sorting - bounded to last 1000 for efficiency.",
    "        \"\"\"",
    "        latencies = self._metrics['latencies_ms']",
    "        avg_latency = sum(latencies) / len(latencies) if latencies else 0",
    "        p99_latency = 0",
    "        if latencies:",
    "            sorted_lat = sorted(latencies)",
    "            p99_idx = min(int(len(sorted_lat) * 0.99), len(sorted_lat) - 1)",
    "            p99_latency = sorted_lat[p99_idx]",
    "        ",
    "        total = self._metrics['total_requests']",
    "        error_rate = (self._metrics['failed_requests'] / total * 100) if total > 0 else 0",
    "        ",
    "        return Response(status=200, body={",
    "            'success': True,",
    "            'data': {",
    "                'totalRequests': total,",
    "                'successfulRequests': self._metrics['successful_requests'],",
    "                'failedRequests': self._metrics['failed_requests'],",
    "                'errorRate': round(error_rate, 2),",
    "                'avgLatencyMs': round(avg_latency, 2),",
    "                'p99LatencyMs': round(p99_latency, 2)",
    "            }",
    "        })",
    "",
    "    def check_rate_limit(self, user_id: str, endpoint: str) -> bool:",
    "        \"\"\"",
    "        Sliding window rate limiting per user per endpoint.",
    "        Returns True if allowed, False if rate limited.",
    "        \"\"\"",
    "        config = self.RATE_LIMITS.get(endpoint, self.RATE_LIMITS['default'])",
    "        limit = config['limit']",
    "        window_seconds = config['window_seconds']",
    "        ",
    "        key = (user_id, endpoint)",
    "        now = datetime.now()",
    "        cutoff = now - timedelta(seconds=window_seconds)",
    "        ",
    "        # Get existing timestamps or empty list",
    "        if key not in self._rate_limit_timestamps:",
    "            self._rate_limit_timestamps[key] = []",
    "        ",
    "        # Filter to only recent timestamps (lazy cleanup)",
    "        recent = [ts for ts in self._rate_limit_timestamps[key] if ts > cutoff]",
    "        self._rate_limit_timestamps[key] = recent",
    "        ",
    "        # Check against limit",
    "        if len(recent) >= limit:",
    "            return False",
    "        ",
    "        # Record this request timestamp",
    "        self._rate_limit_timestamps[key].append(now)",
    "        return True",
    "",
    "    def log_request(self, request_id: str, user_id: str, method: str,",
    "                    endpoint: str, status: int, duration_ms: float):",
    "        \"\"\"",
    "        Structured logging with request ID for distributed tracing.",
    "        Never log sensitive data (passwords, tokens, PII).",
    "        \"\"\"",
    "        log_entry = {",
    "            'timestamp': datetime.now().isoformat(),",
    "            'level': 'ERROR' if status >= 400 else 'INFO',",
    "            'requestId': request_id,",
    "            'userId': user_id,",
    "            'method': method,",
    "            'endpoint': endpoint,",
    "            'status': status,",
    "            'durationMs': round(duration_ms, 2)",
    "        }",
    "        self._request_logs.append(log_entry)",
    "        ",
    "        # Update metrics",
    "        self._metrics['total_requests'] += 1",
    "        if status < 400:",
    "            self._metrics['successful_requests'] += 1",
    "        else:",
    "            self._metrics['failed_requests'] += 1",
    "        ",
    "        # Bound latencies to last 1000 for P99 calculation",
    "        self._metrics['latencies_ms'].append(duration_ms)",
    "        if len(self._metrics['latencies_ms']) > 1000:",
    "            self._metrics['latencies_ms'] = self._metrics['latencies_ms'][-1000:]",
    "",
    "    def generate_request_id(self) -> str:",
    "        \"\"\"Generate unique request ID for tracing.\"\"\"",
    "        self._request_id_counter += 1",
    "        return f'req_{self._request_id_counter:06d}'",
    "",
    "",
    "if __name__ == '__main__':",
    "    print('\\n' + '=' * 60)",
    "    print('TICKET API - PART 4: PRODUCTION READINESS DEMO')",
    "    print('=' * 60)",
    "    ",
    "    api = TicketAPI()",
    "    ",
    "    # Create some test data",
    "    api.create_event({",
    "        'name': 'Rock Concert', 'date': '2024-12-31T20:00:00Z',",
    "        'venue': 'Stadium', 'totalTickets': 1000, 'pricePerTicket': 100",
    "    })",
    "    api.book_tickets({'eventId': 'evt_001', 'userId': 'usr_001', 'numberOfTickets': 2})",
    "    ",
    "    # [1] Health Check",
    "    print('\\n[1] Health Check')",
    "    resp = api.health_check()",
    "    print(f\"    Status: {resp.body['data']['status']}\")",
    "    print(f\"    Events: {resp.body['data']['checks']['eventCount']}\")",
    "    print(f\"    Bookings: {resp.body['data']['checks']['bookingCount']}\")",
    "    print(f\"    Uptime: {resp.body['data']['uptime']}\")",
    "    ",
    "    # [2] Rate Limiting Demo",
    "    print('\\n[2] Rate Limiting (limit=10 for /bookings)')",
    "    allowed_count = 0",
    "    for i in range(12):",
    "        allowed = api.check_rate_limit('spam_user', '/bookings')",
    "        if allowed:",
    "            allowed_count += 1",
    "        print(f\"    Request {i+1}: {'ALLOWED' if allowed else 'BLOCKED'}\")",
    "    print(f\"    Total allowed: {allowed_count}/12 (limit is 10)\")",
    "    ",
    "    # [3] Log some requests and check metrics",
    "    print('\\n[3] Logging & Metrics')",
    "    import random",
    "    for i in range(20):",
    "        req_id = api.generate_request_id()",
    "        status = 200 if random.random() > 0.15 else 400",
    "        latency = random.uniform(10, 200)",
    "        api.log_request(req_id, f'usr_{i%5}', 'POST', '/bookings', status, latency)",
    "    ",
    "    metrics = api.get_metrics()",
    "    data = metrics.body['data']",
    "    print(f\"    Total requests: {data['totalRequests']}\")",
    "    print(f\"    Success: {data['successfulRequests']}, Failed: {data['failedRequests']}\")",
    "    print(f\"    Error rate: {data['errorRate']}%\")",
    "    print(f\"    Avg latency: {data['avgLatencyMs']}ms\")",
    "    print(f\"    P99 latency: {data['p99LatencyMs']}ms\")",
    "    ",
    "    # [4] Show sample log entry",
    "    print('\\n[4] Sample Log Entry (last request)')",
    "    last_log = api._request_logs[-1]",
    "    for key, val in last_log.items():",
    "        print(f\"    {key}: {val}\")",
    "    ",
    "    print('\\n' + '=' * 60)",
    "    print('PART 4 DEMO COMPLETE!')",
    "    print('=' * 60)"
  ],
  "solution_java_lines": [
    "import java.math.BigDecimal;",
    "import java.security.MessageDigest;",
    "import java.time.Duration;",
    "import java.time.Instant;",
    "import java.time.temporal.ChronoUnit;",
    "import java.util.*;",
    "import java.util.concurrent.*;",
    "import java.util.concurrent.locks.ReentrantLock;",
    "import java.util.stream.Collectors;",
    "",
    "public class TicketAPI {",
    "    ",
    "    enum BookingStatus { CONFIRMED, CANCELLED }",
    "    enum UserRole { ADMIN, USER, GUEST }",
    "    ",
    "    static class Event {",
    "        String id, name, venue;",
    "        Instant date;",
    "        int totalTickets, availableTickets;",
    "        BigDecimal pricePerTicket;",
    "        int version = 0;",
    "        Event(String id, String name, Instant date, String venue, int total, BigDecimal price) {",
    "            this.id = id; this.name = name; this.date = date; this.venue = venue;",
    "            this.totalTickets = total; this.availableTickets = total; this.pricePerTicket = price;",
    "        }",
    "    }",
    "    ",
    "    static class Booking {",
    "        String id, eventId, userId;",
    "        int numberOfTickets;",
    "        BigDecimal totalAmount;",
    "        BookingStatus status;",
    "        Instant createdAt;",
    "        Booking(String id, String eventId, String userId, int tickets, BigDecimal total) {",
    "            this.id = id; this.eventId = eventId; this.userId = userId;",
    "            this.numberOfTickets = tickets; this.totalAmount = total;",
    "            this.status = BookingStatus.CONFIRMED; this.createdAt = Instant.now();",
    "        }",
    "    }",
    "    ",
    "    static class User {",
    "        String id, username, passwordHash;",
    "        UserRole role;",
    "        User(String id, String username, String passwordHash, UserRole role) {",
    "            this.id = id; this.username = username; this.passwordHash = passwordHash; this.role = role;",
    "        }",
    "    }",
    "    ",
    "    static class Response {",
    "        int status;",
    "        Map<String, Object> body;",
    "        Response(int status, Map<String, Object> body) { this.status = status; this.body = body; }",
    "    }",
    "    ",
    "    static class RateLimitConfig {",
    "        int limit;",
    "        int windowSeconds;",
    "        RateLimitConfig(int limit, int windowSeconds) {",
    "            this.limit = limit; this.windowSeconds = windowSeconds;",
    "        }",
    "    }",
    "    ",
    "    // State - Parts 1-3",
    "    private final Map<String, Event> events = new HashMap<>();",
    "    private final Map<String, Booking> bookings = new HashMap<>();",
    "    private final Map<String, User> users = new HashMap<>();",
    "    private final Map<String, Map<String, Object>> tokens = new HashMap<>();",
    "    private final Map<String, List<Instant>> loginAttempts = new HashMap<>();",
    "    private int eventCounter = 0, bookingCounter = 0, userCounter = 0;",
    "    private final ReentrantLock lock = new ReentrantLock();",
    "    private static final int MAX_ATTEMPTS = 5;",
    "    private static final int LOCKOUT_MINUTES = 15;",
    "    private final Map<String, Object> eventLocks = new ConcurrentHashMap<>();",
    "    ",
    "    // Part 4: Production Readiness State",
    "    private final Instant startTime = Instant.now();",
    "    private final Map<String, List<Instant>> rateLimitTimestamps = new ConcurrentHashMap<>();",
    "    private final Map<String, RateLimitConfig> rateLimits = new HashMap<>();",
    "    private int totalRequests = 0, successfulRequests = 0, failedRequests = 0;",
    "    private final List<Double> latencies = Collections.synchronizedList(new ArrayList<>());",
    "    private final List<Map<String, Object>> requestLogs = Collections.synchronizedList(new ArrayList<>());",
    "    private int requestIdCounter = 0;",
    "    ",
    "    public TicketAPI() {",
    "        seedUsers();",
    "        // Initialize rate limits",
    "        rateLimits.put(\"/bookings\", new RateLimitConfig(10, 60));",
    "        rateLimits.put(\"/events\", new RateLimitConfig(60, 60));",
    "        rateLimits.put(\"default\", new RateLimitConfig(100, 60));",
    "    }",
    "    ",
    "    private void seedUsers() {",
    "        addUser(\"admin\", \"admin123\", UserRole.ADMIN);",
    "        addUser(\"user\", \"user123\", UserRole.USER);",
    "    }",
    "    ",
    "    private void addUser(String username, String password, UserRole role) {",
    "        String id = String.format(\"usr_%03d\", ++userCounter);",
    "        users.put(username, new User(id, username, hashPassword(password), role));",
    "    }",
    "    ",
    "    private String hashPassword(String password) {",
    "        try {",
    "            MessageDigest md = MessageDigest.getInstance(\"SHA-256\");",
    "            byte[] hash = md.digest(password.getBytes());",
    "            StringBuilder sb = new StringBuilder();",
    "            for (byte b : hash) sb.append(String.format(\"%02x\", b));",
    "            return sb.toString();",
    "        } catch (Exception e) { return password; }",
    "    }",
    "    ",
    "    private Response errorResponse(int status, String code, String message) {",
    "        Map<String, Object> body = new HashMap<>();",
    "        body.put(\"success\", false);",
    "        Map<String, Object> error = new HashMap<>();",
    "        error.put(\"code\", code); error.put(\"message\", message);",
    "        body.put(\"error\", error);",
    "        return new Response(status, body);",
    "    }",
    "    ",
    "    // Part 2: Auth (abbreviated, unchanged)",
    "    private Map<String, Object> validateToken(String token) {",
    "        if (token == null || !tokens.containsKey(token)) return null;",
    "        Map<String, Object> info = tokens.get(token);",
    "        if (Instant.now().isAfter((Instant) info.get(\"exp\"))) {",
    "            tokens.remove(token); return null;",
    "        }",
    "        return info;",
    "    }",
    "    ",
    "    public Response login(Map<String, Object> request) {",
    "        String username = (String) request.getOrDefault(\"username\", \"\");",
    "        String password = (String) request.getOrDefault(\"password\", \"\");",
    "        User user = users.get(username);",
    "        if (user == null || !user.passwordHash.equals(hashPassword(password))) {",
    "            return errorResponse(401, \"UNAUTHORIZED\", \"Invalid credentials\");",
    "        }",
    "        String token = \"tok_\" + UUID.randomUUID().toString().replace(\"-\", \"\").substring(0, 32);",
    "        Map<String, Object> tokenInfo = new HashMap<>();",
    "        tokenInfo.put(\"user_id\", user.id);",
    "        tokenInfo.put(\"role\", user.role.name());",
    "        tokenInfo.put(\"exp\", Instant.now().plus(24, ChronoUnit.HOURS));",
    "        tokens.put(token, tokenInfo);",
    "        Map<String, Object> body = new HashMap<>();",
    "        body.put(\"success\", true);",
    "        Map<String, Object> data = new HashMap<>();",
    "        data.put(\"token\", token);",
    "        body.put(\"data\", data);",
    "        return new Response(200, body);",
    "    }",
    "    ",
    "    // Part 1: Core Methods (abbreviated, unchanged)",
    "    public Response createEvent(Map<String, Object> request, String token) {",
    "        String eventId = String.format(\"evt_%03d\", ++eventCounter);",
    "        Event event = new Event(eventId, (String) request.get(\"name\"),",
    "            Instant.parse((String) request.get(\"date\")), (String) request.get(\"venue\"),",
    "            (Integer) request.get(\"totalTickets\"), new BigDecimal(request.get(\"pricePerTicket\").toString()));",
    "        events.put(eventId, event);",
    "        Map<String, Object> body = new HashMap<>();",
    "        body.put(\"success\", true);",
    "        Map<String, Object> data = new HashMap<>();",
    "        data.put(\"eventId\", eventId); data.put(\"availableTickets\", event.availableTickets);",
    "        body.put(\"data\", data);",
    "        return new Response(201, body);",
    "    }",
    "    ",
    "    // Part 3: Concurrency (unchanged)",
    "    private Object getEventLock(String eventId) {",
    "        return eventLocks.computeIfAbsent(eventId, k -> new Object());",
    "    }",
    "    ",
    "    public Response bookTicketsAtomic(Map<String, Object> request) {",
    "        String eventId = (String) request.get(\"eventId\");",
    "        if (!events.containsKey(eventId)) {",
    "            return errorResponse(404, \"NOT_FOUND\", \"Event not found\");",
    "        }",
    "        Object eventLock = getEventLock(eventId);",
    "        synchronized (eventLock) {",
    "            Event event = events.get(eventId);",
    "            int num = (Integer) request.get(\"numberOfTickets\");",
    "            if (event.availableTickets < num) {",
    "                return errorResponse(409, \"INSUFFICIENT_TICKETS\",",
    "                    \"Only \" + event.availableTickets + \" ticket(s) available\");",
    "            }",
    "            String bookingId;",
    "            lock.lock();",
    "            try { bookingId = String.format(\"bkg_%03d\", ++bookingCounter); }",
    "            finally { lock.unlock(); }",
    "            BigDecimal total = event.pricePerTicket.multiply(new BigDecimal(num));",
    "            Booking booking = new Booking(bookingId, eventId, (String) request.get(\"userId\"), num, total);",
    "            event.availableTickets -= num;",
    "            event.version++;",
    "            bookings.put(bookingId, booking);",
    "            Map<String, Object> body = new HashMap<>();",
    "            body.put(\"success\", true);",
    "            Map<String, Object> data = new HashMap<>();",
    "            data.put(\"bookingId\", bookingId); data.put(\"status\", \"CONFIRMED\");",
    "            body.put(\"data\", data);",
    "            return new Response(201, body);",
    "        }",
    "    }",
    "    ",
    "    // ========== Part 4: Production Readiness Methods ==========",
    "    ",
    "    /**",
    "     * Returns system health status with dependency checks.",
    "     * Called by load balancers - must be fast O(1).",
    "     */",
    "    public Response healthCheck() {",
    "        Duration uptime = Duration.between(startTime, Instant.now());",
    "        long hours = uptime.toHours();",
    "        long minutes = uptime.toMinutesPart();",
    "        long seconds = uptime.toSecondsPart();",
    "        ",
    "        Map<String, Object> checks = new HashMap<>();",
    "        checks.put(\"database\", \"connected\");",
    "        checks.put(\"eventCount\", events.size());",
    "        checks.put(\"bookingCount\", bookings.size());",
    "        ",
    "        Map<String, Object> data = new HashMap<>();",
    "        data.put(\"status\", \"healthy\");",
    "        data.put(\"checks\", checks);",
    "        data.put(\"version\", \"1.0.0\");",
    "        data.put(\"uptime\", String.format(\"%dh %dm %ds\", hours, minutes, seconds));",
    "        ",
    "        Map<String, Object> body = new HashMap<>();",
    "        body.put(\"success\", true);",
    "        body.put(\"data\", data);",
    "        return new Response(200, body);",
    "    }",
    "    ",
    "    /**",
    "     * Returns current metrics including P99 latency.",
    "     * P99 calculated from bounded latency list.",
    "     */",
    "    public Response getMetrics() {",
    "        double avgLatency = 0, p99Latency = 0;",
    "        synchronized (latencies) {",
    "            if (!latencies.isEmpty()) {",
    "                avgLatency = latencies.stream().mapToDouble(d -> d).average().orElse(0);",
    "                List<Double> sorted = latencies.stream().sorted().collect(Collectors.toList());",
    "                int p99Idx = Math.min((int)(sorted.size() * 0.99), sorted.size() - 1);",
    "                p99Latency = sorted.get(p99Idx);",
    "            }",
    "        }",
    "        double errorRate = totalRequests > 0 ? (failedRequests * 100.0 / totalRequests) : 0;",
    "        ",
    "        Map<String, Object> data = new HashMap<>();",
    "        data.put(\"totalRequests\", totalRequests);",
    "        data.put(\"successfulRequests\", successfulRequests);",
    "        data.put(\"failedRequests\", failedRequests);",
    "        data.put(\"errorRate\", Math.round(errorRate * 100) / 100.0);",
    "        data.put(\"avgLatencyMs\", Math.round(avgLatency * 100) / 100.0);",
    "        data.put(\"p99LatencyMs\", Math.round(p99Latency * 100) / 100.0);",
    "        ",
    "        Map<String, Object> body = new HashMap<>();",
    "        body.put(\"success\", true);",
    "        body.put(\"data\", data);",
    "        return new Response(200, body);",
    "    }",
    "    ",
    "    /**",
    "     * Sliding window rate limiting per user per endpoint.",
    "     * Returns true if allowed, false if rate limited.",
    "     */",
    "    public boolean checkRateLimit(String userId, String endpoint) {",
    "        RateLimitConfig config = rateLimits.getOrDefault(endpoint, rateLimits.get(\"default\"));",
    "        String key = userId + \":\" + endpoint;",
    "        Instant now = Instant.now();",
    "        Instant cutoff = now.minusSeconds(config.windowSeconds);",
    "        ",
    "        // Get or create timestamp list",
    "        List<Instant> timestamps = rateLimitTimestamps.computeIfAbsent(key,",
    "            k -> Collections.synchronizedList(new ArrayList<>()));",
    "        ",
    "        synchronized (timestamps) {",
    "            // Filter to recent timestamps",
    "            timestamps.removeIf(ts -> ts.isBefore(cutoff));",
    "            ",
    "            // Check limit",
    "            if (timestamps.size() >= config.limit) {",
    "                return false;",
    "            }",
    "            ",
    "            // Record this request",
    "            timestamps.add(now);",
    "            return true;",
    "        }",
    "    }",
    "    ",
    "    /**",
    "     * Structured logging with request ID for tracing.",
    "     */",
    "    public void logRequest(String requestId, String userId, String method,",
    "                          String endpoint, int status, double durationMs) {",
    "        Map<String, Object> entry = new HashMap<>();",
    "        entry.put(\"timestamp\", Instant.now().toString());",
    "        entry.put(\"level\", status >= 400 ? \"ERROR\" : \"INFO\");",
    "        entry.put(\"requestId\", requestId);",
    "        entry.put(\"userId\", userId);",
    "        entry.put(\"method\", method);",
    "        entry.put(\"endpoint\", endpoint);",
    "        entry.put(\"status\", status);",
    "        entry.put(\"durationMs\", Math.round(durationMs * 100) / 100.0);",
    "        requestLogs.add(entry);",
    "        ",
    "        // Update metrics",
    "        totalRequests++;",
    "        if (status < 400) successfulRequests++;",
    "        else failedRequests++;",
    "        ",
    "        // Bound latencies",
    "        synchronized (latencies) {",
    "            latencies.add(durationMs);",
    "            while (latencies.size() > 1000) latencies.remove(0);",
    "        }",
    "    }",
    "    ",
    "    public String generateRequestId() {",
    "        return String.format(\"req_%06d\", ++requestIdCounter);",
    "    }",
    "    ",
    "    public static void main(String[] args) {",
    "        System.out.println(\"\\n\" + \"=\".repeat(50));",
    "        System.out.println(\"TICKET API - PART 4: PRODUCTION READINESS\");",
    "        System.out.println(\"=\".repeat(50));",
    "        ",
    "        TicketAPI api = new TicketAPI();",
    "        ",
    "        // Create test data",
    "        Map<String, Object> eventReq = new HashMap<>();",
    "        eventReq.put(\"name\", \"Concert\");",
    "        eventReq.put(\"date\", \"2024-12-31T20:00:00Z\");",
    "        eventReq.put(\"venue\", \"Stadium\");",
    "        eventReq.put(\"totalTickets\", 1000);",
    "        eventReq.put(\"pricePerTicket\", 100);",
    "        api.createEvent(eventReq, null);",
    "        ",
    "        // [1] Health Check",
    "        System.out.println(\"\\n[1] Health Check\");",
    "        Response health = api.healthCheck();",
    "        Map<String, Object> hData = (Map) health.body.get(\"data\");",
    "        System.out.println(\"    Status: \" + hData.get(\"status\"));",
    "        System.out.println(\"    Uptime: \" + hData.get(\"uptime\"));",
    "        ",
    "        // [2] Rate Limiting",
    "        System.out.println(\"\\n[2] Rate Limiting (limit=10)\");",
    "        int allowed = 0;",
    "        for (int i = 0; i < 12; i++) {",
    "            if (api.checkRateLimit(\"spam_user\", \"/bookings\")) allowed++;",
    "        }",
    "        System.out.println(\"    Allowed: \" + allowed + \"/12\");",
    "        ",
    "        // [3] Metrics",
    "        System.out.println(\"\\n[3] Metrics after logging\");",
    "        Random rand = new Random();",
    "        for (int i = 0; i < 20; i++) {",
    "            int status = rand.nextDouble() > 0.15 ? 200 : 400;",
    "            api.logRequest(api.generateRequestId(), \"usr_\" + i, \"POST\",",
    "                \"/bookings\", status, rand.nextDouble() * 200);",
    "        }",
    "        Response metrics = api.getMetrics();",
    "        Map<String, Object> mData = (Map) metrics.body.get(\"data\");",
    "        System.out.println(\"    Total: \" + mData.get(\"totalRequests\"));",
    "        System.out.println(\"    Error rate: \" + mData.get(\"errorRate\") + \"%\");",
    "        System.out.println(\"    P99 latency: \" + mData.get(\"p99LatencyMs\") + \"ms\");",
    "        ",
    "        System.out.println(\"\\nDemo complete!\");",
    "    }",
    "}"
  ],
  "code_walkthrough": [
    {
      "lines": "1-12",
      "explanation": "Imports including datetime, threading for existing functionality"
    },
    {
      "lines": "56-70",
      "explanation": "Part 4 rate limit configuration as class constant - different limits per endpoint"
    },
    {
      "lines": "85-95",
      "explanation": "New Part 4 state in __init__: start_time for uptime, rate_limit_timestamps for sliding window, metrics counters, request_logs list"
    },
    {
      "lines": "330-350",
      "explanation": "health_check(): Calculates uptime, returns system status with event/booking counts"
    },
    {
      "lines": "352-375",
      "explanation": "get_metrics(): Calculates P99 from sorted latencies, error rate from counters"
    },
    {
      "lines": "377-402",
      "explanation": "check_rate_limit(): Core sliding window - filter old timestamps, check count vs limit, record if allowed"
    },
    {
      "lines": "404-425",
      "explanation": "log_request(): Structured logging with metrics update, bounds latency list to 1000"
    }
  ],
  "complexity_analysis": {
    "time": {
      "new_methods": {
        "health_check": {
          "complexity": "O(1)",
          "explanation": "Simple dict reads and arithmetic"
        },
        "check_rate_limit": {
          "complexity": "O(w)",
          "explanation": "Filter w timestamps in window, typically small (< limit)"
        },
        "get_metrics": {
          "complexity": "O(n log n)",
          "explanation": "Sorting up to 1000 latencies for P99"
        },
        "log_request": {
          "complexity": "O(1) amortized",
          "explanation": "Append to lists, occasional trim to 1000"
        }
      },
      "overall_change": "Production methods are orthogonal to booking - no impact on booking O(1) complexity"
    },
    "space": {
      "additional_space": "O(u*e*w + L)",
      "explanation": "u*e*w for rate limit timestamps (users * endpoints * window requests), L=1000 for latency buffer"
    }
  },
  "dry_run": {
    "example_input": "12 calls to check_rate_limit('spam_user', '/bookings') with limit=10",
    "steps": [
      {
        "step": 1,
        "action": "Call 1",
        "state": "timestamps=[t1], count=1",
        "explanation": "1 < 10, allowed, record t1"
      },
      {
        "step": 2,
        "action": "Call 2",
        "state": "timestamps=[t1,t2], count=2",
        "explanation": "2 < 10, allowed, record t2"
      },
      {
        "step": 3,
        "action": "Calls 3-10",
        "state": "timestamps=[t1..t10], count=10",
        "explanation": "Each allowed, recording"
      },
      {
        "step": 4,
        "action": "Call 11",
        "state": "timestamps=[t1..t10], count=10",
        "explanation": "10 >= 10, BLOCKED"
      },
      {
        "step": 5,
        "action": "Call 12",
        "state": "timestamps=[t1..t10], count=10",
        "explanation": "10 >= 10, BLOCKED"
      }
    ],
    "final_output": "10 allowed, 2 blocked"
  },
  "debugging_playbook": {
    "fast_sanity_checks": [
      "Single rate limit check should return true",
      "Health check should return 200"
    ],
    "likely_bugs": [
      "Checking limit BEFORE recording (allows limit+1)",
      "Not filtering old timestamps",
      "Using fixed window instead of sliding"
    ],
    "recommended_logs_or_asserts": [
      "assert len(timestamps) <= limit after check",
      "log window size on each check"
    ],
    "how_to_localize": "Add counter to track how many requests pass in a burst test. If limit+1 pass, bug is check-before-record."
  },
  "edge_cases": [
    {
      "case": "First request ever",
      "handling": "Create empty list, check passes, record timestamp",
      "gotcha": "Don't forget to initialize list before checking"
    },
    {
      "case": "Request exactly at limit",
      "handling": "count >= limit returns false (strictly at limit is blocked)",
      "gotcha": "Off-by-one: >= not > for limit comparison"
    },
    {
      "case": "Requests span window boundary",
      "handling": "Old requests naturally filtered out on next check",
      "gotcha": "Lazy cleanup means memory grows until next check"
    },
    {
      "case": "Unknown endpoint",
      "handling": "Falls back to 'default' rate limit config",
      "gotcha": "Must have default config defined"
    },
    {
      "case": "Empty latencies for P99",
      "handling": "Return 0 if no latencies recorded",
      "gotcha": "Division by zero, index out of bounds"
    }
  ],
  "test_cases": [
    {
      "name": "Health check basic",
      "input": "api.health_check()",
      "expected": "status=200, data.status='healthy'",
      "explanation": "Should always return healthy for in-memory system"
    },
    {
      "name": "Rate limit allows up to limit",
      "input": "10 calls to check_rate_limit('user', '/bookings')",
      "expected": "All 10 return true",
      "explanation": "Limit is 10, exactly 10 should be allowed"
    },
    {
      "name": "Rate limit blocks over limit",
      "input": "11th call to check_rate_limit('user', '/bookings')",
      "expected": "Returns false",
      "explanation": "11th request exceeds limit of 10"
    },
    {
      "name": "Metrics after logging",
      "input": "Log 5 success, 1 failure, get_metrics()",
      "expected": "totalRequests=6, errorRate~16.67%",
      "explanation": "1 failure / 6 total = 16.67% error rate"
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Fixed window instead of sliding window",
      "why_wrong": "Allows burst of 2x limit at window boundary",
      "correct_approach": "Track individual timestamps, filter on each check",
      "code_example_wrong": "# wrong: reset count at minute boundary\\nif current_minute != last_minute: count = 0",
      "code_example_correct": "# correct: filter timestamps within window\\nrecent = [ts for ts in timestamps if ts > cutoff]"
    },
    {
      "mistake": "Recording timestamp before checking limit",
      "why_wrong": "Allows limit+1 requests (check sees old count)",
      "correct_approach": "Check first, then record only if allowed",
      "code_example_wrong": "timestamps.append(now)\\nif len(timestamps) > limit: return False",
      "code_example_correct": "if len(recent) >= limit: return False\\ntimestamps.append(now)\\nreturn True"
    },
    {
      "mistake": "Not bounding latency list",
      "why_wrong": "Memory grows unbounded, P99 calculation becomes slow",
      "correct_approach": "Keep only last N (e.g., 1000) latencies",
      "code_example_wrong": "latencies.append(duration)  # grows forever",
      "code_example_correct": "latencies.append(duration)\\nif len(latencies) > 1000: latencies = latencies[-1000:]"
    }
  ],
  "interview_tips": {
    "how_to_present": "Start by explaining the three pillars: observability (health/metrics), protection (rate limiting), and debugging (logging). Then implement each with clear separation.",
    "what_to_mention": [
      "Sliding window vs fixed window tradeoffs",
      "P99 calculation needs bounded storage",
      "Never log sensitive data",
      "Health checks should be fast (called frequently)"
    ],
    "time_allocation": "10-15 minutes: 3 min design discussion, 8-10 min implementation, 2-3 min testing",
    "if_stuck": [
      "Start with health_check - it's the simplest",
      "For rate limiting, think about what data you need to track"
    ]
  },
  "connection_to_next_part": "With production readiness complete, a Part 5 could add distributed rate limiting using Redis, persistent metrics storage, or circuit breakers for external dependencies.",
  "communication_script": {
    "transition_from_previous": "Part 3 gave us thread-safe booking. Now Part 4 adds production observability - we need to know the system is healthy and prevent abuse.",
    "explaining_changes": "I'll add four capabilities: health checks for load balancers, sliding window rate limiting for abuse prevention, metrics for monitoring, and structured logging for debugging.",
    "while_extending_code": [
      "Adding start_time to calculate uptime...",
      "Rate limit timestamps keyed by (user, endpoint) tuple...",
      "Using sliding window - filter old timestamps, then check count..."
    ],
    "after_completing": "Now we have full observability. Health checks are O(1), rate limiting is O(w) where w is requests in window. Ready for production deployment."
  },
  "time_milestones": {
    "time_budget": "10-15 minutes for this part",
    "by_2_min": "Clarify requirements - what metrics to track, what rate limits",
    "by_5_min": "Explain sliding window algorithm, start implementing",
    "by_10_min": "Core methods done, testing with demo",
    "warning_signs": "If still discussing theory at 5 min, jump to code with simplest method (health_check)"
  },
  "recovery_strategies": {
    "if_part_builds_wrong": "Production methods are independent - they don't affect Parts 1-3. Can implement separately.",
    "if_new_requirement_unclear": "Ask: 'Should rate limiting be per-user, per-IP, or global?' or 'What P value for latency percentile?'",
    "if_running_behind": "Implement health_check first (simplest), then rate limiting (most critical), skip metrics if needed"
  },
  "signal_points": {
    "wow_factors_for_followup": [
      "Explaining sliding vs fixed window without prompting",
      "Mentioning P99 requires bounded storage",
      "Discussing what NOT to log (security)",
      "Mentioning this would use Redis in production for distributed systems"
    ]
  },
  "pattern_recognition": {
    "pattern": "Sliding Window Rate Limiting",
    "indicators": [
      "Per-user request limiting",
      "Time-based quotas",
      "Preventing abuse"
    ],
    "similar_problems": [
      "LC 359 - Logger Rate Limiter",
      "LC 362 - Design Hit Counter",
      "API Gateway rate limiting"
    ],
    "template": "timestamps = filter(ts > now - window); if len(timestamps) >= limit: reject; else: record and allow"
  },
  "thinking_process": [
    {
      "step": 1,
      "thought": "Rate limiting needs per-user tracking",
      "why": "Global limit doesn't prevent single user abuse"
    },
    {
      "step": 2,
      "thought": "Sliding window prevents boundary burst",
      "why": "Fixed window allows 2x limit at minute:59 to minute:00"
    },
    {
      "step": 3,
      "thought": "P99 needs sorted data",
      "why": "Percentile calculation requires ordering"
    },
    {
      "step": 4,
      "thought": "Must bound latency storage",
      "why": "Can't keep all latencies forever - O(n) grows"
    }
  ],
  "interviewer_perspective": {
    "what_they_evaluate": [
      "Understanding of production concerns",
      "Sliding window implementation",
      "Security awareness (what not to log)"
    ],
    "bonus_points": [
      "Discussing distributed rate limiting",
      "Mentioning circuit breakers",
      "Proposing alerts based on metrics thresholds"
    ],
    "red_flags": [
      "Using fixed window without acknowledging tradeoff",
      "Logging tokens or passwords",
      "Unbounded data structures"
    ]
  },
  "ai_copilot_tips": {
    "what_to_do": [
      "Use AI for datetime formatting",
      "Let it help with percentile calculation"
    ],
    "what_not_to_do": [
      "Don't let AI choose fixed vs sliding window",
      "Understand the rate limit algorithm yourself"
    ]
  },
  "red_flags_to_avoid": {
    "behavioral": [
      "Skipping straight to code without discussing approach",
      "Not asking about rate limit values"
    ],
    "technical": [
      "Fixed window rate limiting",
      "Not handling empty latencies for P99",
      "Logging sensitive data"
    ],
    "communication": [
      "Not explaining why sliding window",
      "Not mentioning this is simplified for interview"
    ]
  },
  "final_checklist": {
    "before_saying_done": [
      "Health check returns expected format?",
      "Rate limit correctly blocks after limit reached?",
      "P99 handles empty latencies gracefully?",
      "Log entries don't include sensitive data?"
    ],
    "quick_code_review": [
      "Timestamps filtered to window before count check",
      "Latency list bounded to reasonable size",
      "Rate limit records AFTER checking (not before)"
    ]
  },
  "production_considerations": {
    "what_i_would_add": [
      "Redis for distributed rate limiting",
      "Prometheus metrics export",
      "ELK stack for log aggregation",
      "Alerting rules based on thresholds"
    ],
    "why_not_in_interview": "Focus on algorithm correctness; distributed systems complexity is separate discussion",
    "how_to_mention": "Say: 'In production, rate limiting would use Redis for distributed coordination across instances.'"
  },
  "generated_at": "2026-01-19T05:07:30.706884",
  "_meta": {
    "problem_id": "production_ready_ticket_booking_api",
    "part_number": 4,
    "model": "claude-opus-4-5-20251101"
  }
}