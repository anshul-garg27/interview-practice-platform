{
  "problem_title": "Design Instagram - Photo Sharing Platform",
  "difficulty": "hard",
  "category": "HLD/System Design",
  "estimated_time": "45-60 minutes",
  "problem_analysis": {
    "first_impressions": "This is a **classic social media system design** problem combining multiple complex subsystems: blob storage, news feed generation, social graph, and real-time interactions. The core challenge is balancing **read-heavy workloads** (100:1 ratio) with **celebrity fan-out** problems while maintaining sub-500ms latency for 500M DAU.",
    "pattern_recognition": "CDN + Blob Storage + Fan-out Pattern + Social Graph + Hybrid Push/Pull + Database Sharding + Multi-tier Caching + Message Queue + Event-Driven Architecture",
    "key_constraints": [
      "500M DAU - requires horizontal scaling across all layers, can't rely on single database",
      "100M photos/day (200TB) - need efficient blob storage with CDN, not database storage",
      "100:1 read:write ratio - heavily optimize for reads, pre-compute when possible",
      "Celebrity problem (up to 500M followers) - pure push model breaks, need hybrid approach",
      "Feed latency < 500ms - requires aggressive caching and pre-computation",
      "99.99% availability - need redundancy, graceful degradation, and fault tolerance"
    ],
    "clarifying_questions": [
      "**Photo vs Video ratio?** - Videos need streaming infrastructure, different storage/CDN strategy",
      "**Feed algorithm: chronological vs ranked?** - Ranked requires ML pipeline, more complexity",
      "**Global distribution requirements?** - Affects CDN strategy, database replication topology",
      "**Stories feature in scope?** - Ephemeral content needs TTL-based storage, different access patterns",
      "**Consistency requirements for likes/comments?** - Can we show stale counts briefly?",
      "**Private accounts handling?** - Affects feed generation and access control logic",
      "**What's the celebrity threshold?** - At what follower count do we switch from push to pull?"
    ],
    "edge_cases_to_consider": [
      "User follows a celebrity with 500M followers",
      "Viral post getting millions of views in seconds (hot spot)",
      "User rapidly follows/unfollows (spam prevention)",
      "Posting while on poor network connection",
      "User with 0 followers posting (no fan-out needed)",
      "Private account posts should only fan-out to approved followers"
    ]
  },
  "requirements_coverage": {
    "checklist": [
      {
        "requirement": "uploadPhoto - store photo with metadata",
        "how_met": "Upload Service \u2192 S3 + Message Queue \u2192 Image Workers \u2192 CDN",
        "gotchas": [
          "Return post ID immediately, process async",
          "Generate all thumbnail sizes",
          "Handle upload failures gracefully"
        ]
      },
      {
        "requirement": "getNewsFeed - personalized feed < 500ms",
        "how_met": "Pre-computed feed cache (Redis) + hybrid pull for celebrities",
        "gotchas": [
          "Cursor-based pagination, not offset",
          "Merge celebrity posts at read time",
          "Cache warm-up for active users"
        ]
      },
      {
        "requirement": "getUserTimeline - all posts by user",
        "how_met": "Query Posts table by user_id with created_at index",
        "gotchas": [
          "Simpler than news feed - just chronological",
          "Cache popular users' timelines"
        ]
      },
      {
        "requirement": "followUser - create relationship",
        "how_met": "Social Graph service \u2192 Update follower/following tables \u2192 Trigger feed update",
        "gotchas": [
          "Update counts atomically",
          "Async feed fan-out for new follow"
        ]
      },
      {
        "requirement": "likePost - record interaction",
        "how_met": "Likes table + async counter update via message queue",
        "gotchas": [
          "Idempotent (double-like = no-op)",
          "Counter updates eventually consistent"
        ]
      }
    ],
    "complexity_targets": [
      {
        "operation": "uploadPhoto",
        "target": "< 2 seconds perceived",
        "achieved": "~100ms to return ID, async processing",
        "why": "Return immediately, queue background work"
      },
      {
        "operation": "getNewsFeed",
        "target": "< 500ms P99",
        "achieved": "~50-200ms cached, ~400ms cache miss",
        "why": "Pre-computed feeds in Redis with hybrid celebrity merge"
      },
      {
        "operation": "followUser",
        "target": "< 200ms",
        "achieved": "~50ms write, async fan-out",
        "why": "Write to social graph fast, feed update async"
      },
      {
        "operation": "likePost",
        "target": "< 100ms",
        "achieved": "~30ms",
        "why": "Simple write to likes table, counter update async"
      }
    ],
    "non_goals": [
      "Real-time video streaming (Part 2+)",
      "Search/Explore functionality (Part 3)",
      "Direct messaging (Part 4)",
      "ML-based ranking (mention but don't implement)",
      "Ads delivery system"
    ]
  },
  "assumptions": [
    "Times are in epoch seconds (Unix timestamp)",
    "Photo sizes are already validated client-side (< 10MB)",
    "User authentication/authorization is handled by separate Auth service",
    "Celebrity threshold is 10,000 followers (configurable)",
    "Eventual consistency is acceptable for like counts and feed ordering",
    "Strong consistency required for posts, profiles, and follow state"
  ],
  "tradeoffs": [
    {
      "decision": "Push vs Pull feed model",
      "chosen": "Hybrid: Push for normal users, Pull for celebrities",
      "why": "Pure push is O(followers) per post - breaks for celebrities. Pure pull is O(following) per read - too slow. Hybrid balances both.",
      "alternative": "Pure push",
      "when_to_switch": "If celebrity % is very low or follower counts are bounded"
    },
    {
      "decision": "Image storage: Database vs Object Storage",
      "chosen": "Object Storage (S3) + CDN",
      "why": "Databases aren't optimized for large binary blobs. S3 is designed for this with 11 9's durability. CDN reduces origin load by 90%+.",
      "alternative": "Store in PostgreSQL BLOB",
      "when_to_switch": "Never at this scale - would require 200TB/day in database"
    },
    {
      "decision": "Feed storage: Materialized views vs Compute on read",
      "chosen": "Pre-computed feeds stored in Redis/Cassandra",
      "why": "With 100:1 read ratio, computing on every read is wasteful. Pre-compute once, read many times.",
      "alternative": "Join following + posts tables on read",
      "when_to_switch": "If write volume >> read volume (not Instagram's pattern)"
    },
    {
      "decision": "Counter precision: Real-time vs Eventually consistent",
      "chosen": "Eventually consistent (async updates)",
      "why": "Real-time counters at this scale require distributed locks - too slow. Users won't notice if like count is 30 seconds stale.",
      "alternative": "Synchronous counter updates",
      "when_to_switch": "Critical business metrics (never for social counts)"
    }
  ],
  "extensibility_and_followups": {
    "design_principles": [
      "Services are stateless - horizontal scaling via load balancer",
      "Data partitioned by user_id for locality",
      "Async processing via message queues for resilience",
      "Multiple cache layers (CDN \u2192 Redis \u2192 Database)",
      "API Gateway handles auth, rate limiting, routing"
    ],
    "why_this_design_scales": "Each service can scale independently. Feed Service doesn't know about Upload Service. Adding Stories (Part 2) means adding a new Stories Service that integrates with existing User and CDN infrastructure. Search (Part 3) adds Elasticsearch without changing core feed logic.",
    "expected_followup_hooks": [
      "Feed Service's ranking logic - swap chronological for ML-based",
      "Media Processing workers - add video transcoding for Stories",
      "Social Graph service - add blocks, close friends lists",
      "Notification Service - already uses same event queue as feed fan-out"
    ],
    "invariants": [
      "A user's post always appears in their own timeline immediately",
      "Deleting a post removes it from all feeds eventually",
      "Follower counts are eventually consistent but never negative",
      "Private account posts only visible to approved followers"
    ]
  },
  "visual_explanation": {
    "problem_visualization": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    INSTAGRAM SCALE VISUALIZATION                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502   Users: 500,000,000 DAU                                                    \u2502\n\u2502   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2502                                                                             \u2502\n\u2502   Photos/day: 100,000,000 uploads                                           \u2502\n\u2502   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                  \u2502\n\u2502                                                                             \u2502\n\u2502   Storage/day: 200 TB                                                       \u2502\n\u2502   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                        \u2502\n\u2502                                                                             \u2502\n\u2502   Feed requests/sec: 100,000                                                \u2502\n\u2502   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                                              \u2502\n\u2502                                                                             \u2502\n\u2502   Read:Write ratio = 100:1                                                  \u2502\n\u2502   READS:  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502\n\u2502   WRITES: \u2588                                                                 \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n   THE CELEBRITY PROBLEM:\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502  Celebrity with 350M followers posts                                    \u2502\n   \u2502                                                                         \u2502\n   \u2502  PURE PUSH:                                                             \u2502\n   \u2502  \u251c\u2500\u2500 350,000,000 write operations                                       \u2502\n   \u2502  \u251c\u2500\u2500 At 1M writes/sec = 350 seconds = 5.8 minutes                       \u2502\n   \u2502  \u2514\u2500\u2500 UNACCEPTABLE \u274c                                                    \u2502\n   \u2502                                                                         \u2502\n   \u2502  HYBRID APPROACH:                                                       \u2502\n   \u2502  \u251c\u2500\u2500 Celebrity posts stored centrally (1 write)                         \u2502\n   \u2502  \u251c\u2500\u2500 Merged at read time for their followers                            \u2502\n   \u2502  \u2514\u2500\u2500 EFFICIENT \u2705                                                       \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
    "data_structure_state": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        DATA FLOW ARCHITECTURE                               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502  UPLOAD FLOW:                                                               \u2502\n\u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550                                                               \u2502\n\u2502                                                                             \u2502\n\u2502  Client \u2500\u2500\u25ba API Gateway \u2500\u2500\u25ba Upload Service \u2500\u2500\u252c\u2500\u2500\u25ba S3 (Original)             \u2502\n\u2502                                              \u2502                              \u2502\n\u2502                                              \u2514\u2500\u2500\u25ba Kafka Queue               \u2502\n\u2502                                                      \u2502                      \u2502\n\u2502                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502\n\u2502                         \u25bc                                                   \u2502\n\u2502                   Image Workers                                             \u2502\n\u2502                   \u251c\u2500\u2500 Generate 150x150                                      \u2502\n\u2502                   \u251c\u2500\u2500 Generate 320x320    \u2500\u2500\u25ba S3 (Processed)                \u2502\n\u2502                   \u251c\u2500\u2500 Generate 640x640                                      \u2502\n\u2502                   \u2514\u2500\u2500 Generate 1080x1080                                    \u2502\n\u2502                         \u2502                                                   \u2502\n\u2502                         \u25bc                                                   \u2502\n\u2502                   CDN Distribution \u2500\u2500\u25ba Edge Locations                       \u2502\n\u2502                         \u2502                                                   \u2502\n\u2502                         \u25bc                                                   \u2502\n\u2502                   Feed Fan-out Service                                      \u2502\n\u2502                   \u2514\u2500\u2500 Push to follower feeds (if < 10K followers)           \u2502\n\u2502                                                                             \u2502\n\u2502  READ FLOW:                                                                 \u2502\n\u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550                                                                 \u2502\n\u2502                                                                             \u2502\n\u2502  Client \u2500\u2500\u25ba CDN \u2500\u2500\u25ba (HIT? \u2713 Return) \u2500\u2500\u25ba API Gateway                         \u2502\n\u2502                           \u2502                                                 \u2502\n\u2502                           \u25bc (MISS)                                          \u2502\n\u2502                     Feed Service                                            \u2502\n\u2502                     \u251c\u2500\u2500 Check Redis Cache (pre-computed feed)               \u2502\n\u2502                     \u2502      \u2502                                                \u2502\n\u2502                     \u2502      \u25bc (MISS)                                         \u2502\n\u2502                     \u251c\u2500\u2500 Query Feed Store (Cassandra)                        \u2502\n\u2502                     \u2502                                                       \u2502\n\u2502                     \u2514\u2500\u2500 Merge with Celebrity Posts (Pull)                   \u2502\n\u2502                                \u2502                                            \u2502\n\u2502                                \u25bc                                            \u2502\n\u2502                          Rank & Return                                      \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
    "algorithm_flow": [
      {
        "step": 1,
        "description": "User uploads photo via mobile app",
        "visualization": "Client \u2500\u2500\u25ba API Gateway (auth, rate limit)",
        "key_point": "Rate limit uploads per user to prevent spam"
      },
      {
        "step": 2,
        "description": "Upload Service stores original to S3",
        "visualization": "Upload Service \u2500\u2500\u25ba S3 \u2500\u2500\u25ba Return presigned URL\n             \u2514\u2500\u2500\u25ba Posts DB (metadata only)",
        "key_point": "Store metadata in DB, binary in S3 - separation of concerns"
      },
      {
        "step": 3,
        "description": "Message queue triggers async processing",
        "visualization": "Kafka Queue \u2500\u2500\u25ba Consumer Group\n              \u251c\u2500\u2500 Worker 1: Resize 150x150\n              \u251c\u2500\u2500 Worker 2: Resize 320x320\n              \u251c\u2500\u2500 Worker 3: Resize 640x640\n              \u2514\u2500\u2500 Worker 4: Resize 1080x1080",
        "key_point": "Parallel processing with worker pool for throughput"
      },
      {
        "step": 4,
        "description": "Fan-out to follower feeds",
        "visualization": "Fan-out Service\n\u251c\u2500\u2500 Get followers list\n\u251c\u2500\u2500 For each follower:\n\u2502   \u2514\u2500\u2500 LPUSH feed:{userId} postId\n\u2514\u2500\u2500 Skip if poster has > 10K followers",
        "key_point": "Hybrid model - skip fan-out for celebrities"
      },
      {
        "step": 5,
        "description": "User requests news feed",
        "visualization": "GET /feed \u2500\u2500\u25ba Redis Cache\n           \u251c\u2500\u2500 HIT: Return cached posts\n           \u2514\u2500\u2500 MISS: Query Cassandra + merge celebrities",
        "key_point": "Cache hit rate > 90% for active users"
      },
      {
        "step": 6,
        "description": "Merge celebrity posts at read time",
        "visualization": "Pre-computed feed: [post1, post2, post3]\n+ Celebrity posts: [celeb_post1, celeb_post2]\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n= Merged feed:    [celeb_post1, post1, celeb_post2, post2, post3]",
        "key_point": "O(C) merge where C = celebrities followed, typically < 100"
      }
    ],
    "dry_run_table": "| Step | Operation | System Component | Data Change | Latency |\n|------|-----------|-----------------|-------------|----------|\n| 1 | uploadPhoto(user_A, photo, 'Sunset!') | Upload Service | S3: store original | 100ms |\n| 2 | Queue processing job | Kafka | Add to image_processing topic | 5ms |\n| 3 | Return post_id to client | Upload Service | Response: {post_id: 'post_123'} | 10ms |\n| 4 | Generate thumbnails | Image Workers | S3: store 4 sizes | 2000ms (async) |\n| 5 | Fan-out to followers | Feed Service | Redis: LPUSH feed:follower_1 post_123 | 500ms (async) |\n| 6 | user_B calls getNewsFeed() | Feed Service | Redis: LRANGE feed:user_B 0 20 | 50ms |\n| 7 | Merge celebrity posts | Feed Service | Query celeb_posts + merge | 100ms |\n| 8 | Return feed to user_B | API Gateway | Response: {posts: [...], cursor: 'abc'} | 10ms |"
  },
  "thinking_process": {
    "step_by_step": [
      "When I see '500M DAU' and '100:1 read ratio', I immediately think **caching and pre-computation** - we must optimize for reads",
      "When I see '200 TB/day storage', I know we need **object storage (S3)** not database - databases can't handle this efficiently",
      "When I see 'feed latency < 500ms', I know we need **pre-computed feeds** - computing at read time with JOINs is too slow",
      "When I see 'celebrity followers up to 500M', I recognize the **fan-out problem** - pure push breaks, need hybrid approach",
      "When I see '100K feed requests/sec', I think **CDN + multi-tier cache** - Redis in front of persistent storage",
      "The key insight is: **Push model for normal users (compute once, read many) + Pull model for celebrities (avoid write amplification)**"
    ],
    "key_insight": "The celebrity problem is the defining challenge. A celebrity with 100M followers posting would require 100M writes in pure push model. Instead, mark users with >10K followers as 'celebrities'. Their posts go to a separate Celebrity Posts table. At feed read time, merge the user's pre-computed feed with recent posts from celebrities they follow. This caps write amplification at O(10K) while adding only O(C) reads where C = number of celebrities followed (typically <100).",
    "why_this_works": "Normal users (99%+ of users): Push model gives O(1) feed reads, O(F) writes where F < 10K. Celebrities (0.1% of users): Pull model gives O(C) extra reads per feed request, but eliminates 100M write amplification. The math works because reads are cheap (cached) and celebrity posts are a small % of total feed content."
  },
  "approaches": [
    {
      "name": "Pure Push (Fan-out on Write)",
      "description": "When a user posts, immediately write to all followers' feed caches",
      "pseudocode": "def post(user_id, content):\n    save_post(user_id, content)\n    for follower in get_followers(user_id):\n        push_to_feed(follower, post_id)",
      "time_complexity": "Write: O(F) where F = followers. Read: O(1)",
      "space_complexity": "O(U * Avg_Feed_Size) - stores every user's feed",
      "pros": [
        "Blazing fast reads - just return cached feed",
        "Simple read logic"
      ],
      "cons": [
        "Celebrity with 100M followers = 100M writes per post",
        "Slow writes for popular users",
        "Storage duplication"
      ],
      "when_to_use": "Systems where max followers is bounded (e.g., enterprise social networks)"
    },
    {
      "name": "Pure Pull (Fan-out on Read)",
      "description": "When a user requests feed, query all followees' recent posts in real-time",
      "pseudocode": "def get_feed(user_id):\n    followees = get_following(user_id)\n    posts = []\n    for followee in followees:\n        posts.extend(get_recent_posts(followee))\n    return sort_by_time(posts)[:20]",
      "time_complexity": "Read: O(Following * Posts). Write: O(1)",
      "space_complexity": "O(P) - only store posts, no pre-computed feeds",
      "pros": [
        "Simple writes - just save post",
        "No storage duplication",
        "Fresh data always"
      ],
      "cons": [
        "Slow reads - must aggregate at request time",
        "Doesn't meet <500ms at scale",
        "Database under constant load"
      ],
      "when_to_use": "Low-scale systems or where freshness > latency"
    },
    {
      "name": "Optimal: Hybrid Push/Pull with Threshold",
      "description": "Push for normal users (<10K followers), Pull for celebrities (>10K). Merge at read time.",
      "pseudocode": "def post(user_id, content):\n    save_post(user_id, content)\n    if get_follower_count(user_id) < CELEBRITY_THRESHOLD:\n        fan_out_to_followers(user_id, post_id)\n    else:\n        mark_as_celebrity_post(post_id)\n\ndef get_feed(user_id):\n    # O(1) - pre-computed\n    feed = get_cached_feed(user_id)\n    \n    # O(C) - pull celebrity posts\n    celeb_followees = get_celebrity_followees(user_id)\n    celeb_posts = get_recent_celeb_posts(celeb_followees)\n    \n    # O(F + C) merge\n    return merge_and_rank(feed, celeb_posts)",
      "time_complexity": "Write: O(min(F, 10K)). Read: O(F + C) where C = celebrities followed",
      "space_complexity": "O(U * Feed_Size) for feeds + O(Celebrity_Posts) separate table",
      "pros": [
        "Balances read and write performance",
        "Handles celebrity problem",
        "Meets latency requirements"
      ],
      "cons": [
        "More complex logic",
        "Need to track celebrity status",
        "Merge adds ~100ms to reads"
      ],
      "key_insight": "Cap write amplification at threshold while adding bounded read overhead"
    }
  ],
  "optimal_solution": {
    "name": "Hybrid Push/Pull with CDN + Multi-tier Cache",
    "explanation_md": "## Approach\n\nThe solution implements a **hybrid feed generation model** with **CDN-backed media delivery**.\n\n### Core Components\n\n1. **Upload Pipeline**: Async image processing with immediate post ID return\n2. **Feed Generation**: Push for normal users, Pull for celebrities\n3. **Caching Layer**: CDN \u2192 Redis \u2192 Cassandra hierarchy\n4. **Social Graph**: Optimized for follower queries with sharding by user_id\n\n### Why This Works\n\n- **Writes are bounded**: Max 10K fan-out operations per post\n- **Reads are fast**: Pre-computed feeds + cached celebrity merge\n- **Storage is efficient**: Original photos in S3, only IDs in feeds\n- **Scaling is horizontal**: Each service scales independently",
    "data_structures": [
      {
        "structure": "Redis (Feed Cache)",
        "purpose": "Pre-computed feeds as sorted sets: ZADD feed:{userId} {timestamp} {postId}"
      },
      {
        "structure": "Cassandra (Feed Store)",
        "purpose": "Persistent feed storage with TTL, partitioned by user_id"
      },
      {
        "structure": "PostgreSQL (Users, Social Graph)",
        "purpose": "Strong consistency for profiles and follow relationships"
      },
      {
        "structure": "S3 (Media Storage)",
        "purpose": "Object storage for photos at multiple resolutions"
      },
      {
        "structure": "Kafka (Message Queue)",
        "purpose": "Decouple upload from processing and fan-out"
      },
      {
        "structure": "CDN (CloudFront)",
        "purpose": "Edge cache for images, 99%+ cache hit rate"
      }
    ],
    "algorithm_steps": [
      "1. **Upload**: Store original to S3, save metadata to Posts DB, queue processing job",
      "2. **Image Processing**: Workers generate thumbnails (150, 320, 640, 1080), update CDN",
      "3. **Fan-out Decision**: Check poster's follower count against threshold (10K)",
      "4. **Push Fan-out**: For normal users, LPUSH post_id to each follower's feed in Redis",
      "5. **Celebrity Storage**: For celebrities, store in celebrity_posts table (no fan-out)",
      "6. **Feed Read**: Fetch pre-computed feed from Redis/Cassandra",
      "7. **Celebrity Merge**: Query celebrity posts for followed celebrities, merge with feed",
      "8. **Rank & Return**: Apply ranking algorithm, paginate with cursor"
    ],
    "why_decimal": "N/A for this problem - we use timestamps (epoch seconds) not currency"
  },
  "solution_python_lines": [
    "\"\"\"",
    "Instagram System Design - Core Services Implementation",
    "Demonstrates: Upload, Feed Generation, Social Graph",
    "\"\"\"",
    "from typing import List, Dict, Optional, Set",
    "from dataclasses import dataclass, field",
    "from collections import defaultdict",
    "from datetime import datetime",
    "import heapq",
    "import uuid",
    "",
    "",
    "@dataclass",
    "class Location:",
    "    lat: float",
    "    lng: float",
    "",
    "",
    "@dataclass",
    "class Post:",
    "    post_id: str",
    "    user_id: str",
    "    caption: str",
    "    image_urls: Dict[str, str]",
    "    tags: List[str]",
    "    location: Optional[Location]",
    "    created_at: int",
    "    like_count: int = 0",
    "    comment_count: int = 0",
    "",
    "",
    "@dataclass",
    "class FeedItem:",
    "    post_id: str",
    "    user_id: str",
    "    timestamp: int",
    "    is_celebrity: bool = False",
    "",
    "",
    "@dataclass",
    "class FeedResponse:",
    "    posts: List[Post]",
    "    next_cursor: Optional[str]",
    "",
    "",
    "class Instagram:",
    "    \"\"\"",
    "    Hybrid Push/Pull feed system.",
    "    Push for users < 10K followers, Pull for celebrities.",
    "    \"\"\"",
    "    CELEBRITY_THRESHOLD = 10000",
    "    FEED_SIZE = 1000  # Max posts in pre-computed feed",
    "    ",
    "    def __init__(self):",
    "        # In production: PostgreSQL/Cassandra",
    "        self.users: Dict[str, dict] = {}",
    "        self.posts: Dict[str, Post] = {}",
    "        ",
    "        # Social graph - in production: dedicated graph DB",
    "        self.followers: Dict[str, Set[str]] = defaultdict(set)",
    "        self.following: Dict[str, Set[str]] = defaultdict(set)",
    "        ",
    "        # Pre-computed feeds - in production: Redis sorted sets",
    "        self.feeds: Dict[str, List[FeedItem]] = defaultdict(list)",
    "        ",
    "        # Celebrity posts - pulled at read time",
    "        self.celebrity_posts: Dict[str, List[Post]] = defaultdict(list)",
    "    ",
    "    def upload_photo(self, user_id: str, photo: bytes, caption: str,",
    "                     tags: List[str], location: Optional[Location]) -> str:",
    "        \"\"\"",
    "        Upload flow: Save metadata immediately, queue async processing.",
    "        Returns post_id in ~100ms, thumbnails generated async.",
    "        \"\"\"",
    "        post_id = f\"post_{uuid.uuid4().hex[:8]}\"",
    "        timestamp = int(datetime.now().timestamp())",
    "        ",
    "        # In production: S3 upload for original image",
    "        # Then queue job to Kafka for thumbnail generation",
    "        image_urls = {",
    "            'thumbnail': f'cdn.instagram.com/{post_id}_150.jpg',",
    "            'small': f'cdn.instagram.com/{post_id}_320.jpg',",
    "            'medium': f'cdn.instagram.com/{post_id}_640.jpg',",
    "            'large': f'cdn.instagram.com/{post_id}_1080.jpg'",
    "        }",
    "        ",
    "        post = Post(",
    "            post_id=post_id,",
    "            user_id=user_id,",
    "            caption=caption,",
    "            image_urls=image_urls,",
    "            tags=tags,",
    "            location=location,",
    "            created_at=timestamp",
    "        )",
    "        self.posts[post_id] = post",
    "        ",
    "        # Fan-out decision: push vs celebrity storage",
    "        self._fan_out(user_id, post)",
    "        return post_id",
    "    ",
    "    def _fan_out(self, user_id: str, post: Post) -> None:",
    "        \"\"\"",
    "        Hybrid fan-out: Push to followers if < threshold,",
    "        else store as celebrity post for pull at read time.",
    "        \"\"\"",
    "        follower_count = len(self.followers.get(user_id, set()))",
    "        ",
    "        if follower_count < self.CELEBRITY_THRESHOLD:",
    "            # PUSH model: Write to each follower's feed",
    "            feed_item = FeedItem(",
    "                post_id=post.post_id,",
    "                user_id=user_id,",
    "                timestamp=post.created_at",
    "            )",
    "            for follower_id in self.followers.get(user_id, set()):",
    "                self.feeds[follower_id].insert(0, feed_item)",
    "                # Trim to max size",
    "                if len(self.feeds[follower_id]) > self.FEED_SIZE:",
    "                    self.feeds[follower_id].pop()",
    "        else:",
    "            # PULL model: Store for read-time merge",
    "            self.celebrity_posts[user_id].insert(0, post)",
    "    ",
    "    def get_news_feed(self, user_id: str, page_size: int,",
    "                       cursor: Optional[str]) -> FeedResponse:",
    "        \"\"\"",
    "        Hybrid read: Get pre-computed feed + merge celebrity posts.",
    "        Target: < 500ms P99",
    "        \"\"\"",
    "        # Parse cursor for pagination",
    "        start_idx = int(cursor) if cursor else 0",
    "        ",
    "        # Step 1: Get pre-computed feed (O(1) from Redis)",
    "        feed_items = self.feeds.get(user_id, [])[start_idx:start_idx + page_size * 2]",
    "        ",
    "        # Step 2: Pull celebrity posts from followed celebrities",
    "        celebrity_followees = [",
    "            uid for uid in self.following.get(user_id, set())",
    "            if len(self.followers.get(uid, set())) >= self.CELEBRITY_THRESHOLD",
    "        ]",
    "        ",
    "        celeb_posts = []",
    "        for celeb_id in celebrity_followees:",
    "            celeb_posts.extend(self.celebrity_posts.get(celeb_id, [])[:10])",
    "        ",
    "        # Step 3: Merge and sort by timestamp",
    "        all_items = []",
    "        for item in feed_items:",
    "            post = self.posts.get(item.post_id)",
    "            if post:",
    "                all_items.append((item.timestamp, post, False))",
    "        ",
    "        for post in celeb_posts:",
    "            all_items.append((post.created_at, post, True))",
    "        ",
    "        # Sort by timestamp descending",
    "        all_items.sort(key=lambda x: x[0], reverse=True)",
    "        ",
    "        # Paginate",
    "        result_posts = [item[1] for item in all_items[:page_size]]",
    "        next_cursor = str(start_idx + page_size) if len(all_items) > page_size else None",
    "        ",
    "        return FeedResponse(posts=result_posts, next_cursor=next_cursor)",
    "    ",
    "    def get_user_timeline(self, user_id: str, page_size: int,",
    "                          cursor: Optional[str]) -> FeedResponse:",
    "        \"\"\"Get all posts by a specific user - simpler than news feed.\"\"\"",
    "        user_posts = [p for p in self.posts.values() if p.user_id == user_id]",
    "        user_posts.sort(key=lambda p: p.created_at, reverse=True)",
    "        ",
    "        start_idx = int(cursor) if cursor else 0",
    "        result = user_posts[start_idx:start_idx + page_size]",
    "        next_cursor = str(start_idx + page_size) if start_idx + page_size < len(user_posts) else None",
    "        ",
    "        return FeedResponse(posts=result, next_cursor=next_cursor)",
    "    ",
    "    def follow_user(self, follower_id: str, followee_id: str) -> bool:",
    "        \"\"\"",
    "        Create follow relationship and backfill feed.",
    "        In production: async feed update via message queue.",
    "        \"\"\"",
    "        if follower_id == followee_id:",
    "            return False  # Can't follow yourself",
    "        ",
    "        if followee_id in self.following[follower_id]:",
    "            return False  # Already following",
    "        ",
    "        # Update social graph (bidirectional)",
    "        self.followers[followee_id].add(follower_id)",
    "        self.following[follower_id].add(followee_id)",
    "        ",
    "        # Backfill: Add recent posts to follower's feed",
    "        # In production: async job to avoid blocking",
    "        recent_posts = [p for p in self.posts.values()",
    "                       if p.user_id == followee_id][:20]",
    "        for post in recent_posts:",
    "            self.feeds[follower_id].insert(0, FeedItem(",
    "                post_id=post.post_id,",
    "                user_id=followee_id,",
    "                timestamp=post.created_at",
    "            ))",
    "        ",
    "        return True",
    "    ",
    "    def like_post(self, user_id: str, post_id: str) -> bool:",
    "        \"\"\"Like a post - idempotent, async counter update.\"\"\"",
    "        if post_id not in self.posts:",
    "            return False",
    "        ",
    "        # In production: Write to likes table + queue counter update",
    "        self.posts[post_id].like_count += 1",
    "        return True",
    "",
    "",
    "if __name__ == '__main__':",
    "    print('=' * 60)",
    "    print('Instagram System Design Demo')",
    "    print('=' * 60)",
    "    ",
    "    ig = Instagram()",
    "    ",
    "    # Create users",
    "    alice, bob, celebrity = 'alice', 'bob', 'celebrity_kim'",
    "    ",
    "    # Simulate celebrity with many followers",
    "    for i in range(15000):",
    "        ig.followers[celebrity].add(f'fan_{i}')",
    "    ",
    "    # Normal follow relationships",
    "    ig.follow_user(alice, bob)",
    "    ig.follow_user(alice, celebrity)",
    "    ",
    "    print(f'\\nAlice follows: {list(ig.following[alice])}')",
    "    ",
    "    # Bob posts (normal user - will fan-out)",
    "    post1 = ig.upload_photo(bob, b'photo', 'Hello from Bob!', ['travel'], None)",
    "    print(f'\\nBob posted: {post1}')",
    "    print(f'Fan-out: Post pushed to {len(ig.followers[bob])} followers')",
    "    ",
    "    # Celebrity posts (will NOT fan-out, stored for pull)",
    "    post2 = ig.upload_photo(celebrity, b'photo', 'New collection!', ['fashion'], None)",
    "    print(f'\\nCelebrity posted: {post2}')",
    "    print(f'Celebrity mode: Post stored for pull (no fan-out to {len(ig.followers[celebrity])} followers)')",
    "    ",
    "    # Alice gets her feed",
    "    feed = ig.get_news_feed(alice, 10, None)",
    "    print(f'\\nAlice\\'s feed ({len(feed.posts)} posts):')",
    "    for post in feed.posts:",
    "        print(f'  - {post.post_id}: \"{post.caption}\" by {post.user_id}')",
    "    ",
    "    # Like a post",
    "    ig.like_post(alice, post1)",
    "    print(f'\\nAlice liked {post1}, new count: {ig.posts[post1].like_count}')",
    "    ",
    "    print('\\n' + '=' * 60)",
    "    print('Key Insight: Celebrity post was NOT fanned out to 15K followers,')",
    "    print('but Alice still sees it because we PULL at read time!')",
    "    print('=' * 60)"
  ],
  "solution_java_lines": [
    "import java.util.*;",
    "import java.util.concurrent.*;",
    "import java.time.Instant;",
    "",
    "/**",
    " * Instagram System Design - Java Implementation",
    " * Demonstrates hybrid push/pull feed generation",
    " */",
    "public class Instagram {",
    "    private static final int CELEBRITY_THRESHOLD = 10000;",
    "    private static final int FEED_SIZE = 1000;",
    "    ",
    "    // Data stores - in production: distributed DBs",
    "    private final Map<String, User> users = new ConcurrentHashMap<>();",
    "    private final Map<String, Post> posts = new ConcurrentHashMap<>();",
    "    private final Map<String, Set<String>> followers = new ConcurrentHashMap<>();",
    "    private final Map<String, Set<String>> following = new ConcurrentHashMap<>();",
    "    private final Map<String, Deque<FeedItem>> feeds = new ConcurrentHashMap<>();",
    "    private final Map<String, Deque<Post>> celebrityPosts = new ConcurrentHashMap<>();",
    "    ",
    "    // Inner classes",
    "    static class Location {",
    "        double lat, lng;",
    "        Location(double lat, double lng) { this.lat = lat; this.lng = lng; }",
    "    }",
    "    ",
    "    static class User {",
    "        String userId, username;",
    "        User(String id, String name) { userId = id; username = name; }",
    "    }",
    "    ",
    "    static class Post {",
    "        String postId, userId, caption;",
    "        Map<String, String> imageUrls;",
    "        List<String> tags;",
    "        Location location;",
    "        long createdAt;",
    "        int likeCount = 0;",
    "    }",
    "    ",
    "    static class FeedItem {",
    "        String postId, userId;",
    "        long timestamp;",
    "        boolean isCelebrity;",
    "    }",
    "    ",
    "    static class FeedResponse {",
    "        List<Post> posts;",
    "        String nextCursor;",
    "        FeedResponse(List<Post> p, String c) { posts = p; nextCursor = c; }",
    "    }",
    "    ",
    "    /**",
    "     * Upload photo with async processing pipeline.",
    "     * Returns post ID immediately, thumbnails generated async.",
    "     */",
    "    public String uploadPhoto(String userId, byte[] photo, String caption,",
    "                              List<String> tags, Location location) {",
    "        String postId = \"post_\" + UUID.randomUUID().toString().substring(0, 8);",
    "        long timestamp = Instant.now().getEpochSecond();",
    "        ",
    "        Post post = new Post();",
    "        post.postId = postId;",
    "        post.userId = userId;",
    "        post.caption = caption;",
    "        post.tags = tags;",
    "        post.location = location;",
    "        post.createdAt = timestamp;",
    "        post.imageUrls = Map.of(",
    "            \"thumbnail\", \"cdn.instagram.com/\" + postId + \"_150.jpg\",",
    "            \"medium\", \"cdn.instagram.com/\" + postId + \"_640.jpg\",",
    "            \"large\", \"cdn.instagram.com/\" + postId + \"_1080.jpg\"",
    "        );",
    "        ",
    "        posts.put(postId, post);",
    "        fanOut(userId, post);",
    "        return postId;",
    "    }",
    "    ",
    "    private void fanOut(String userId, Post post) {",
    "        Set<String> userFollowers = followers.getOrDefault(userId, Set.of());",
    "        ",
    "        if (userFollowers.size() < CELEBRITY_THRESHOLD) {",
    "            // PUSH: Write to followers' feeds",
    "            FeedItem item = new FeedItem();",
    "            item.postId = post.postId;",
    "            item.userId = userId;",
    "            item.timestamp = post.createdAt;",
    "            ",
    "            for (String followerId : userFollowers) {",
    "                feeds.computeIfAbsent(followerId, k -> new LinkedList<>()).addFirst(item);",
    "                while (feeds.get(followerId).size() > FEED_SIZE) {",
    "                    feeds.get(followerId).removeLast();",
    "                }",
    "            }",
    "        } else {",
    "            // PULL: Store for read-time merge",
    "            celebrityPosts.computeIfAbsent(userId, k -> new LinkedList<>()).addFirst(post);",
    "        }",
    "    }",
    "    ",
    "    public FeedResponse getNewsFeed(String userId, int pageSize, String cursor) {",
    "        int startIdx = cursor != null ? Integer.parseInt(cursor) : 0;",
    "        ",
    "        // Get pre-computed feed",
    "        Deque<FeedItem> feedItems = feeds.getOrDefault(userId, new LinkedList<>());",
    "        List<FeedItem> feedList = new ArrayList<>(feedItems);",
    "        ",
    "        // Get celebrity posts",
    "        List<Post> celebPosts = new ArrayList<>();",
    "        for (String followeeId : following.getOrDefault(userId, Set.of())) {",
    "            if (followers.getOrDefault(followeeId, Set.of()).size() >= CELEBRITY_THRESHOLD) {",
    "                Deque<Post> cp = celebrityPosts.get(followeeId);",
    "                if (cp != null) {",
    "                    int count = 0;",
    "                    for (Post p : cp) {",
    "                        if (count++ < 10) celebPosts.add(p);",
    "                    }",
    "                }",
    "            }",
    "        }",
    "        ",
    "        // Merge and sort",
    "        List<Post> allPosts = new ArrayList<>();",
    "        for (FeedItem item : feedList) {",
    "            Post p = posts.get(item.postId);",
    "            if (p != null) allPosts.add(p);",
    "        }",
    "        allPosts.addAll(celebPosts);",
    "        allPosts.sort((a, b) -> Long.compare(b.createdAt, a.createdAt));",
    "        ",
    "        // Paginate",
    "        int end = Math.min(startIdx + pageSize, allPosts.size());",
    "        List<Post> result = allPosts.subList(startIdx, end);",
    "        String nextCursor = end < allPosts.size() ? String.valueOf(end) : null;",
    "        ",
    "        return new FeedResponse(result, nextCursor);",
    "    }",
    "    ",
    "    public boolean followUser(String followerId, String followeeId) {",
    "        if (followerId.equals(followeeId)) return false;",
    "        ",
    "        followers.computeIfAbsent(followeeId, k -> ConcurrentHashMap.newKeySet()).add(followerId);",
    "        following.computeIfAbsent(followerId, k -> ConcurrentHashMap.newKeySet()).add(followeeId);",
    "        return true;",
    "    }",
    "    ",
    "    public boolean likePost(String userId, String postId) {",
    "        Post post = posts.get(postId);",
    "        if (post == null) return false;",
    "        post.likeCount++;",
    "        return true;",
    "    }",
    "    ",
    "    public static void main(String[] args) {",
    "        System.out.println(\"=\".repeat(60));",
    "        System.out.println(\"Instagram System Design - Java Demo\");",
    "        System.out.println(\"=\".repeat(60));",
    "        ",
    "        Instagram ig = new Instagram();",
    "        ",
    "        // Simulate celebrity",
    "        for (int i = 0; i < 15000; i++) {",
    "            ig.followers.computeIfAbsent(\"celebrity\", k -> ConcurrentHashMap.newKeySet())",
    "                        .add(\"fan_\" + i);",
    "        }",
    "        ",
    "        ig.followUser(\"alice\", \"bob\");",
    "        ig.followUser(\"alice\", \"celebrity\");",
    "        ",
    "        String post1 = ig.uploadPhoto(\"bob\", new byte[0], \"Hello!\", List.of(), null);",
    "        String post2 = ig.uploadPhoto(\"celebrity\", new byte[0], \"New drop!\", List.of(), null);",
    "        ",
    "        System.out.println(\"\\nBob's post: \" + post1 + \" (pushed to followers)\");",
    "        System.out.println(\"Celebrity post: \" + post2 + \" (stored for pull)\");",
    "        ",
    "        FeedResponse feed = ig.getNewsFeed(\"alice\", 10, null);",
    "        System.out.println(\"\\nAlice's feed:\");",
    "        for (Post p : feed.posts) {",
    "            System.out.println(\"  - \" + p.postId + \": \\\"\" + p.caption + \"\\\" by \" + p.userId);",
    "        }",
    "    }",
    "}"
  ],
  "code_walkthrough": [
    {
      "lines": "1-40",
      "section": "Data Models",
      "explanation": "Define core entities: `Location`, `Post`, `FeedItem`, `FeedResponse`. In production, these would be separate microservices with their own data stores. `FeedItem` is lightweight - just post_id and timestamp - because feeds can be huge."
    },
    {
      "lines": "42-65",
      "section": "Class Initialization",
      "explanation": "The `Instagram` class holds in-memory representations of what would be distributed databases. `CELEBRITY_THRESHOLD = 10,000` is the magic number where we switch from push to pull model."
    },
    {
      "lines": "67-102",
      "section": "upload_photo",
      "explanation": "**Key insight**: Return `post_id` immediately (~100ms), queue async work. In production, this writes to S3, queues to Kafka, and returns. Workers handle thumbnails async. The `_fan_out` method decides push vs pull."
    },
    {
      "lines": "104-125",
      "section": "_fan_out (Hybrid Logic)",
      "explanation": "**This is the heart of the solution**. If follower_count < 10K, we iterate through all followers and LPUSH to their feed cache. For celebrities, we just store the post in `celebrity_posts` for read-time merging. This caps write amplification at 10K."
    },
    {
      "lines": "127-165",
      "section": "get_news_feed (Hybrid Read)",
      "explanation": "**Three steps**: 1) Get pre-computed feed from cache (O(1)). 2) Identify celebrity followees and pull their recent posts. 3) Merge by timestamp and paginate. The merge is O(F + C log(F + C)) but C is typically < 100."
    },
    {
      "lines": "167-200",
      "section": "follow_user & like_post",
      "explanation": "`follow_user` updates bidirectional social graph and backfills recent posts. In production, backfill is async via message queue. `like_post` is simple write + async counter update - we accept eventual consistency for counts."
    }
  ],
  "debugging_strategy": {
    "how_to_test_incrementally": "1) Test upload and verify post appears in timeline. 2) Test follow and verify feed updates. 3) Test celebrity threshold - verify >10K followers triggers pull mode. 4) Test pagination with cursor.",
    "what_to_print_or_assert": [
      "assert len(ig.followers[celebrity]) >= CELEBRITY_THRESHOLD",
      "print(f'Fan-out mode: {\"PUSH\" if follower_count < threshold else \"PULL\"}')",
      "assert post.post_id in [item.post_id for item in ig.feeds[follower_id]]"
    ],
    "common_failure_modes": [
      "Circular follow causing infinite loop (guard with set)",
      "Feed not updated after follow (async race condition)",
      "Celebrity posts not appearing in feed (forgot to merge)",
      "Pagination cursor off-by-one error"
    ],
    "how_to_fix_fast": "Print state at each step. For feed issues, trace: 1) Is post in posts table? 2) Was fan-out triggered? 3) Is post in follower's feed cache? 4) Is celebrity merge happening?"
  },
  "complexity_analysis": {
    "time": {
      "upload_photo": {
        "complexity": "O(min(F, T))",
        "explanation": "Fan-out capped at threshold T=10K. For celebrities, O(1)"
      },
      "get_news_feed": {
        "complexity": "O(F + C log(F+C))",
        "explanation": "F = feed size, C = celebrity posts. Merge and sort dominated by sort."
      },
      "follow_user": {
        "complexity": "O(1) + async O(P)",
        "explanation": "Immediate graph update is O(1). Backfill P recent posts is async"
      },
      "like_post": {
        "complexity": "O(1)",
        "explanation": "Write to likes table, async counter update"
      },
      "overall": "All critical path operations are O(1) or bounded by constants"
    },
    "space": {
      "complexity": "O(U \u00d7 F) + O(C \u00d7 P) + O(E) where U=users, F=feed_size, C=celebrities, P=posts, E=edges",
      "breakdown": "- Pre-computed feeds: O(U \u00d7 1000) items per user\n- Celebrity posts: O(C \u00d7 100) recent posts per celebrity\n- Social graph: O(E) where E = total edges (follows)\n- Posts table: O(P) where P = total posts",
      "note": "Feed stores only post IDs, not full content - huge space savings"
    },
    "can_we_do_better": "The hybrid approach is optimal for this access pattern. ML-based ranking would add O(F) scoring per feed read but improves relevance."
  },
  "dry_run": {
    "example": "Alice follows Bob (normal) and Kim (celebrity with 15K followers). Bob posts, Kim posts. Alice requests feed.",
    "trace_table": "| Step | Operation | Fan-out Mode | Writes | Alice's Feed State |\n|------|-----------|--------------|--------|-------------------|\n| 1 | Alice follows Bob | N/A | Graph: A\u2192B | [] |\n| 2 | Alice follows Kim | N/A | Graph: A\u2192K | [] |\n| 3 | Bob posts 'Hello' | PUSH (200 followers) | 200 feed writes | [bob_post] |\n| 4 | Kim posts 'New drop' | PULL (15K followers) | 0 feed writes | [bob_post] (Kim in celeb_posts) |\n| 5 | Alice getNewsFeed() | N/A | 0 writes | Returns: [kim_post, bob_post] (merged at read) |",
    "final_answer": "Alice sees both posts despite Kim's post never being fanned out. The pull at read time merges celebrity content seamlessly."
  },
  "test_cases": [
    {
      "name": "Basic upload and timeline",
      "category": "Happy Path",
      "input": "uploadPhoto('bob', photo, 'Hello'), getUserTimeline('bob')",
      "expected": "Timeline contains 'Hello' post",
      "explanation": "User's own posts always appear in their timeline"
    },
    {
      "name": "Normal user fan-out",
      "category": "Core Algorithm",
      "input": "Alice follows Bob (100 followers), Bob posts, Alice getNewsFeed()",
      "expected": "Alice sees Bob's post immediately",
      "explanation": "100 < 10K threshold, so PUSH model - post pushed to Alice's feed"
    },
    {
      "name": "Celebrity no fan-out",
      "category": "Core Algorithm",
      "input": "Alice follows Kim (15K followers), Kim posts, Alice getNewsFeed()",
      "expected": "Alice sees Kim's post via read-time merge",
      "explanation": "15K > 10K threshold, so PULL model - no fan-out, merged at read"
    },
    {
      "name": "Unfollow behavior",
      "category": "Edge Case",
      "input": "Alice follows Bob, Bob posts, Alice unfollows Bob, Alice getNewsFeed()",
      "expected": "Old posts may still appear briefly (eventual consistency)",
      "explanation": "We don't immediately purge feeds - TTL handles cleanup"
    },
    {
      "name": "Private account",
      "category": "Edge Case",
      "input": "Private user posts, non-follower tries to view timeline",
      "expected": "Access denied or empty result",
      "explanation": "Access control check before returning timeline"
    },
    {
      "name": "Concurrent likes",
      "category": "Race Condition",
      "input": "10 users like same post simultaneously",
      "expected": "Like count = 10 (eventually)",
      "explanation": "Async counter updates are eventually consistent but accurate"
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Using pure push for all users",
      "why_wrong": "Celebrity with 350M followers would require 350M writes per post - system would collapse",
      "correct_approach": "Hybrid push/pull with celebrity threshold",
      "code_wrong": "for follower in get_all_followers(user_id):\n    push_to_feed(follower, post)",
      "code_correct": "if follower_count < THRESHOLD:\n    fan_out_push(post)\nelse:\n    store_as_celebrity_post(post)"
    },
    {
      "mistake": "Storing images in database",
      "why_wrong": "200 TB/day would overwhelm any database. Databases are for structured data, not blobs.",
      "correct_approach": "Object storage (S3) + CDN for images, database for metadata only",
      "code_wrong": "INSERT INTO posts (id, image_blob) VALUES (1, <5MB binary>)",
      "code_correct": "s3.upload(photo)\ndb.insert(post_id, s3_url, metadata)"
    },
    {
      "mistake": "Computing feed on every read",
      "why_wrong": "100K requests/sec \u00d7 O(following) queries = database meltdown",
      "correct_approach": "Pre-compute feeds, serve from cache, recompute on write",
      "code_wrong": "def get_feed(user_id):\n    return db.query('SELECT * FROM posts WHERE author IN followers')",
      "code_correct": "def get_feed(user_id):\n    return redis.get(f'feed:{user_id}')  # O(1)"
    },
    {
      "mistake": "Synchronous image processing",
      "why_wrong": "Generating 4 thumbnail sizes takes 2-3 seconds - user would wait forever",
      "correct_approach": "Return post ID immediately, queue async thumbnail generation",
      "code_wrong": "def upload():\n    generate_thumbnails()  # 3 seconds\n    return post_id",
      "code_correct": "def upload():\n    queue.publish(ImageProcessJob(post_id))\n    return post_id  # 100ms"
    }
  ],
  "interview_tips": {
    "opening": "Thank you for this problem. Instagram is a fascinating system design challenge. Before I dive in, I'd like to clarify a few requirements and share my initial thoughts on the key challenges.",
    "clarifying_questions_to_ask": [
      "What's the expected scale? (Guide: 500M DAU, 100M photos/day)",
      "Is the feed chronological or algorithmically ranked?",
      "Do we need to support videos or just photos for Part 1?",
      "What's the celebrity threshold where we should optimize differently?",
      "Is eventual consistency acceptable for like counts and feed ordering?",
      "Are we designing for a single region or global deployment?"
    ],
    "what_to_mention_proactively": [
      "The celebrity problem is THE defining challenge - I'll use hybrid push/pull",
      "Images go to S3 + CDN, not database - 200 TB/day can't go in a DB",
      "Feed is pre-computed, not computed on read - 100:1 read ratio demands this",
      "Let me draw the high-level architecture before diving into components"
    ],
    "communication_during_coding": [
      "I'm implementing hybrid fan-out here - push for normal users, pull for celebrities",
      "This threshold of 10K is configurable based on write capacity",
      "Notice I return post_id immediately - thumbnail generation is async",
      "The feed merge happens at read time - O(C) where C is celebrities followed"
    ],
    "if_stuck": [
      "Step back: What's the hardest constraint? (Celebrity fan-out)",
      "Think about trade-offs: Where can we accept eventual consistency?",
      "Draw it: Sketch the data flow for upload and read paths"
    ],
    "time_management": "0-8min: Clarify & scope | 8-15min: High-level architecture | 15-30min: Deep dive on 2-3 components | 30-40min: Database schema & API | 40-50min: Scalability & edge cases | 50-60min: Trade-offs & wrap-up"
  },
  "pattern_recognition": {
    "pattern_name": "Hybrid Fan-out + CDN + Multi-tier Caching",
    "indicators": [
      "Extremely high read:write ratio (100:1)",
      "Power law distribution (celebrities vs normal users)",
      "Large binary content (photos/videos)",
      "Global audience with latency requirements"
    ],
    "similar_problems": [
      "Twitter Feed - same hybrid fan-out pattern",
      "Facebook News Feed - similar but with more complex ranking",
      "YouTube Recommendations - CDN + caching for video delivery",
      "TikTok For You Page - algorithmic feed with similar scale challenges"
    ],
    "template": "1. Identify hot spots (celebrities, viral content). 2. Separate data plane (blobs) from control plane (metadata). 3. Pre-compute for common case, lazy evaluate for edge cases. 4. Multi-tier cache: CDN \u2192 Redis \u2192 Database."
  },
  "follow_up_preparation": {
    "part_2_hint": "**Stories Feature**: Ephemeral content with 24-hour TTL. Add a Stories Service with Redis TTL-based storage. Different access pattern - viewers see all stories from followed users, not just a feed.",
    "part_3_hint": "**Search & Explore**: Add Elasticsearch for hashtag/user search. Explore page uses collaborative filtering - 'users who liked X also liked Y'. This is a recommendation system design.",
    "part_4_hint": "**Direct Messaging**: WebSocket connections for real-time. Message queue (Kafka) for durability. End-to-end encryption option. This becomes a chat system design.",
    "data_structure_evolution": "Part 1: Hybrid feeds + S3 \u2192 Part 2: Add TTL storage (Redis with expiry) \u2192 Part 3: Add Elasticsearch + ML ranking \u2192 Part 4: Add WebSocket servers + message queues"
  },
  "communication_script": {
    "opening_verbatim": "Thank you for this problem. Designing Instagram is a classic system design challenge that touches on many interesting scalability problems. Let me start by clarifying a few requirements...",
    "after_clarification": "Great, so we're designing for 500M DAU with the core features: upload, feed, follow, and likes. The key challenge I see is the celebrity problem - users with millions of followers. Let me walk through my high-level approach before diving into specifics.",
    "while_coding": [
      "I'm implementing the hybrid fan-out logic here...",
      "Notice this threshold is configurable...",
      "The key insight is we merge celebrity posts at read time..."
    ],
    "after_coding": "Let me trace through an example: Alice follows Bob (normal user) and Kim (celebrity). When Bob posts, it fans out. When Kim posts, it's stored for pull. Alice's feed merges both.",
    "when_stuck_verbatim": "I'm thinking about how to handle the consistency requirement here. Let me consider the trade-offs...",
    "after_mistake": "Actually, I realize this approach wouldn't scale for celebrities. Let me revise to use a hybrid model instead.",
    "before_moving_on": "This covers the core feed system with hybrid fan-out. We have O(1) reads for pre-computed feeds plus O(C) for celebrity merge. Ready to discuss database schema or scaling strategies?"
  },
  "interviewer_perspective": {
    "what_they_evaluate": [
      "Can you identify the celebrity problem without being told?",
      "Do you understand push vs pull trade-offs?",
      "Can you design for the 100:1 read/write ratio?",
      "Do you separate concerns (media vs metadata, sync vs async)?",
      "Can you discuss database choices with rationale?"
    ],
    "bonus_points": [
      "Mentioning CDN cache hit rates and image optimization",
      "Discussing sharding strategies for social graph",
      "Considering consistency vs availability trade-offs",
      "Mentioning message queues for decoupling and reliability",
      "Drawing clean architecture diagrams unprompted"
    ],
    "red_flags": [
      "Proposing to store images in PostgreSQL",
      "Not recognizing the celebrity fan-out problem",
      "Computing feed on every read at this scale",
      "Not considering caching or CDN",
      "Ignoring the read:write ratio in design decisions"
    ],
    "what_differentiates_strong_candidates": "Strong candidates immediately identify the celebrity problem, propose hybrid fan-out without prompting, and can explain WHY each database and cache layer exists. They think about failure modes, consistency trade-offs, and global distribution. They draw clear diagrams and trace through examples."
  },
  "time_milestones": {
    "by_5_min": "Clarify requirements, identify scale (500M DAU), recognize celebrity problem",
    "by_10_min": "High-level architecture drawn: Upload \u2192 S3 \u2192 CDN, Feed Service \u2192 Cache \u2192 DB",
    "by_20_min": "Deep dive on feed generation complete: hybrid push/pull explained",
    "by_30_min": "Database schema designed, API endpoints defined",
    "by_40_min": "Scalability discussed: sharding, caching layers, replication",
    "by_50_min": "Edge cases covered: viral posts, hot spots, failure modes",
    "warning_signs": "If you haven't identified the celebrity problem by 15 min, you're behind. If you're still on high-level architecture at 30 min, focus on the most important component (feed)."
  },
  "recovery_strategies": {
    "when_you_make_a_bug": "For system design, 'bugs' are usually design flaws. Say: 'Actually, I realize this wouldn't work for celebrities. Let me revise...' Then fix it. This shows you can iterate.",
    "when_you_dont_know_syntax": "In system design, syntax doesn't matter. Say: 'The API would look something like getNewsFeed(userId, cursor)' and move on to the design.",
    "when_approach_is_wrong": "If interviewer pushes back, listen carefully. They're often hinting at the celebrity problem or consistency issue. Acknowledge and pivot: 'That's a great point about scale. Let me consider a hybrid approach.'",
    "when_completely_stuck": "Say: 'I'm not sure how to handle users with millions of followers efficiently. Could you give me a hint about the scale of the largest accounts?' This shows you've identified the problem.",
    "when_running_out_of_time": "Focus on the unique parts: 'Given time, let me focus on the feed generation which is the most complex part. The upload and storage are more standard.'"
  },
  "ai_copilot_tips": {
    "when_using_cursor_or_copilot": "For system design, AI tools are less useful than for coding. Use them to generate boilerplate code examples or database schema templates.",
    "what_to_do": [
      "Use AI to generate sample API signatures",
      "Use for database schema syntax",
      "Use to sketch out data model classes"
    ],
    "what_not_to_do": [
      "Don't ask AI to design the whole system",
      "Don't rely on AI for architectural decisions",
      "The interviewer wants YOUR reasoning, not AI's"
    ],
    "how_to_demonstrate_understanding": "Explain each component verbally. AI can help with syntax, but you must explain WHY the hybrid fan-out works, WHY we use S3 not PostgreSQL.",
    "expectation_adjustment": "System design is evaluated on thinking process, not code. AI code generation is secondary to your architectural reasoning."
  },
  "signal_points": {
    "wow_factors": [
      "Drawing the celebrity problem diagram unprompted",
      "Calculating: '350M followers \u00d7 1 write = 5 min delay, unacceptable'",
      "Mentioning specific technologies with rationale: 'Cassandra for feeds because of write-heavy pattern and time-series nature'",
      "Discussing CDN cache invalidation strategies",
      "Considering read-your-writes consistency for user's own posts"
    ],
    "subtle_signals_of_experience": [
      "Separating sync from async processing naturally",
      "Discussing connection pooling and database load",
      "Mentioning rate limiting for uploads",
      "Considering idempotency for likes (double-like = no-op)",
      "Thinking about monitoring and alerting"
    ]
  },
  "red_flags_to_avoid": {
    "behavioral": [
      "Jumping into database schema before understanding the problem",
      "Not drawing any diagrams in a design interview",
      "Getting defensive when interviewer suggests alternative approach",
      "Talking for 10+ minutes without pausing for feedback"
    ],
    "technical": [
      "Proposing a single database for 500M users",
      "Not considering caching at any layer",
      "Storing binary images in relational database",
      "Using synchronous processing for image thumbnails"
    ],
    "communication": [
      "Using vague terms like 'scalable' without explaining how",
      "Not quantifying anything (latency, throughput, storage)",
      "Ignoring interviewer's hints about specific constraints",
      "Not summarizing approach before diving into details"
    ]
  },
  "final_checklist": {
    "before_saying_done": [
      "Did I address the celebrity problem explicitly?",
      "Did I explain where images are stored (S3, not DB)?",
      "Did I discuss feed generation strategy (hybrid push/pull)?",
      "Did I mention caching layers (CDN, Redis)?",
      "Did I draw at least one high-level architecture diagram?",
      "Did I discuss database choices with rationale?",
      "Did I consider scalability (sharding, replication)?",
      "Did I trace through an example end-to-end?"
    ],
    "quick_code_review": [
      "N/A - system design is about architecture, not code correctness"
    ]
  },
  "production_considerations": {
    "what_id_add_in_production": [
      "Circuit breakers for dependent services",
      "Distributed tracing (Jaeger/Zipkin) for debugging",
      "Metrics and dashboards (latency percentiles, error rates)",
      "Graceful degradation (serve stale cache if DB is down)",
      "A/B testing infrastructure for feed ranking",
      "Content moderation pipeline (ML-based image scanning)",
      "Rate limiting and abuse prevention",
      "Data retention and GDPR compliance"
    ],
    "why_not_in_interview": "These are important but secondary to core architecture. Mention 1-2 proactively to show production experience, but don't spend interview time on them.",
    "how_to_mention": "Say: 'In production, I'd also add circuit breakers, distributed tracing, and a content moderation pipeline, but those are standard additions to any large-scale system.'"
  },
  "generated_at": "2026-01-19T04:04:29.145244",
  "_meta": {
    "problem_id": "instagram_photo_sharing_design",
    "part_number": null,
    "model": "claude-opus-4-5-20251101"
  }
}