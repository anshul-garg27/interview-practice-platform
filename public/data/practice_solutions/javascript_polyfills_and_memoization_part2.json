{
  "problem_title": "JavaScript Polyfills & Memoization - Part 2: Async Memoization with Callbacks",
  "part_number": 2,
  "builds_on": "Part 1",
  "difficulty": "medium",
  "problem_understanding": {
    "what_changes": "Part 1 handled synchronous functions where we simply cache input\u2192output. Part 2 introduces async callback-based functions where: (1) the callback is the last argument and must be separated from cache key, (2) results arrive later via callback, (3) multiple calls with same args may occur BEFORE first completes - requiring a 'pending' state with callback queue.",
    "new_requirements": [
      "Handle callback as last argument (exclude from cache key)",
      "Track pending state for in-flight async operations",
      "Queue callbacks when same args called while pending",
      "Notify all queued callbacks when async completes",
      "Don't cache errors by default"
    ],
    "new_constraints": [
      "Must handle race condition: call(1,cb1) then call(1,cb2) before cb1 resolves",
      "Callback should be called immediately on cache hit",
      "Original async function should only be called ONCE per unique args"
    ],
    "key_insight": "The cache entry needs THREE states: (1) not-present = first call, (2) pending = in-flight with callback queue, (3) resolved = cached result. This prevents duplicate async calls and ensures all waiters get notified."
  },
  "requirements_coverage": {
    "checklist": [
      {
        "requirement": "Cache results after first successful call",
        "how_met": "On success, update cache entry to {status: 'resolved', value: result}",
        "gotchas": [
          "Must update BEFORE notifying callbacks to handle immediate re-calls"
        ]
      },
      {
        "requirement": "Handle concurrent calls before first completes",
        "how_met": "Pending state stores callbacks array; new calls with same key append to queue",
        "gotchas": [
          "Thread safety needed if truly concurrent (Python threading.Lock)"
        ]
      },
      {
        "requirement": "Don't cache errors",
        "how_met": "On error, delete cache entry entirely, then notify all callbacks with error",
        "gotchas": [
          "Must delete BEFORE notifying to allow retry on error"
        ]
      }
    ],
    "complexity_targets": [
      {
        "operation": "memoize_async",
        "target": "O(1)",
        "achieved": "O(1)",
        "why": "Just creates closure with empty cache"
      },
      {
        "operation": "memoized call (cache hit)",
        "target": "O(k)",
        "achieved": "O(k)",
        "why": "O(k) for key serialization, O(1) for lookup and callback"
      },
      {
        "operation": "memoized call (pending)",
        "target": "O(k)",
        "achieved": "O(k)",
        "why": "O(k) for key, O(1) to append callback to array"
      }
    ],
    "non_goals": [
      "Promise-based async (that's Part 3)",
      "Cache expiration/TTL",
      "Error caching options"
    ]
  },
  "assumptions": [
    "Callback is ALWAYS the last argument",
    "Callback signature is (err, result) - Node.js style error-first",
    "Arguments (excluding callback) are JSON-serializable",
    "Functions are not called with different callbacks expecting different behavior"
  ],
  "tradeoffs": [
    {
      "decision": "Status object vs. separate pending Map",
      "chosen": "Single cache with status field",
      "why": "Simpler, atomic state transitions, single lookup",
      "alternative": "Separate pendingMap + resultCache",
      "when_to_switch": "If need different eviction policies for pending vs resolved"
    },
    {
      "decision": "Delete on error vs. cache errors",
      "chosen": "Delete on error",
      "why": "Allows retry, matches common use case, errors often transient",
      "alternative": "Cache errors with TTL",
      "when_to_switch": "If errors are expensive to produce and deterministic"
    }
  ],
  "extensibility_notes": {
    "what_to_keep_stable": [
      "memoize() function signature and behavior",
      "Cache key generation via JSON.stringify",
      "Error-first callback convention"
    ],
    "what_to_change": [
      "Cache entries now have status/callbacks instead of just value",
      "New memoize_async() function added"
    ],
    "interfaces_and_boundaries": "memoize_async is independent of memoize - could easily add memoize_promise in Part 3 using similar pattern but with Promise resolution instead of callbacks",
    "invariants": [
      "Cache entry in 'resolved' state always has value",
      "Cache entry in 'pending' state always has non-empty callbacks array",
      "Original fn called exactly once per unique args (on success)"
    ]
  },
  "visual_explanation": {
    "before_after": "```\nBEFORE (Part 1 - Sync):\ncache = {\n  '[1,2]': 3,        // Just the value\n  '[3,4]': 7\n}\n\nAFTER (Part 2 - Async):\ncache = {\n  '[1]': { status: 'resolved', value: {id:1,name:'User 1'} },\n  '[2]': { status: 'pending', callbacks: [cb1, cb2, cb3] }\n}\n```",
    "algorithm_flow": "```\nmemoized(1, callback):\n\u2502\n\u251c\u2500 key = JSON.stringify([1]) = '[1]'\n\u2502\n\u251c\u2500 cache.has('[1]')?\n\u2502   \u251c\u2500 YES, status='resolved' \u2192 callback(null, cached_value) [DONE]\n\u2502   \u2514\u2500 YES, status='pending'  \u2192 callbacks.push(callback) [DONE]\n\u2502\n\u251c\u2500 NO \u2192 Create: cache['[1]'] = {status:'pending', callbacks:[callback]}\n\u2502\n\u251c\u2500 Call fn(1, internalCallback)\n\u2502\n\u2514\u2500 When fn completes:\n    \u251c\u2500 Error:   delete cache['[1]'], forEach cb \u2192 cb(err, null)\n    \u2514\u2500 Success: cache['[1]'] = {status:'resolved', value: result}\n                forEach cb \u2192 cb(null, result)\n```"
  },
  "approaches": [
    {
      "name": "Naive Extension",
      "description": "Simply store callback result when it arrives, ignore pending state",
      "time_complexity": "O(k)",
      "space_complexity": "O(n \u00d7 k)",
      "why_not_optimal": "FAILS when call(1,cb1) then call(1,cb2) before cb1 returns - would trigger fn TWICE and cb2 might never be called"
    },
    {
      "name": "Optimal: Status-based Cache Entries",
      "description": "Track pending/resolved status with callback queue for pending entries",
      "time_complexity": "O(k) per call",
      "space_complexity": "O(n \u00d7 k + pending callbacks)",
      "key_insight": "Three-state cache entries (absent/pending/resolved) prevent duplicate calls and enable callback queuing"
    }
  ],
  "optimal_solution": {
    "explanation_md": "The solution uses **status-based cache entries** to handle the async nature:\n\n1. **Cache Entry Structure**: `{status: 'pending'|'resolved', value?, callbacks?[]}`\n\n2. **On Call**:\n   - Generate key from args **excluding callback**\n   - If **resolved**: immediate callback with cached value\n   - If **pending**: queue callback to be notified later\n   - If **absent**: create pending entry, call original fn\n\n3. **On Completion**:\n   - **Success**: Update to resolved, notify all queued callbacks\n   - **Error**: Delete entry (allow retry), notify all with error\n\n**Thread Safety**: Use lock around cache access for concurrent environments.",
    "data_structures": [
      {
        "structure": "Dict/Map with status objects",
        "purpose": "Track async state and queue callbacks per unique args"
      },
      {
        "structure": "List/Array for callbacks",
        "purpose": "Queue all waiters to notify when async completes"
      }
    ],
    "algorithm_steps": [
      "Step 1: Extract callback (last arg) and generate key from remaining args",
      "Step 2: Check cache - if resolved, immediate callback and return",
      "Step 3: Check cache - if pending, queue callback and return",
      "Step 4: Create pending entry with callback in queue",
      "Step 5: Call original fn with internal callback wrapper",
      "Step 6: On completion, update cache state and notify all queued callbacks"
    ]
  },
  "solution_python_lines": [
    "\"\"\"",
    "Memoization Implementation - Part 2: Async Memoization",
    "Extends Part 1 with support for callback-based async functions.",
    "\"\"\"",
    "import json",
    "from typing import Callable, Dict, Any",
    "from functools import wraps",
    "import threading",
    "",
    "",
    "# ============ Part 1: Sync Memoization (unchanged) ============",
    "def memoize(fn: Callable) -> Callable:",
    "    \"\"\"Returns memoized version of fn that caches results.\"\"\"",
    "    cache = {}",
    "    ",
    "    @wraps(fn)",
    "    def memoized(*args, **kwargs):",
    "        key = json.dumps((args, sorted(kwargs.items())), default=str)",
    "        if key in cache:",
    "            return cache[key]",
    "        result = fn(*args, **kwargs)",
    "        cache[key] = result",
    "        return result",
    "    ",
    "    memoized.cache = cache",
    "    return memoized",
    "",
    "",
    "# ============ Part 2: Async Memoization ============",
    "def memoize_async(fn: Callable) -> Callable:",
    "    \"\"\"",
    "    Memoize async callback-based functions.",
    "    ",
    "    Key insight: Track pending state and queue callbacks to handle",
    "    concurrent calls with same args before first completes.",
    "    ",
    "    Cache entry: {status: 'pending'|'resolved', value?, callbacks?[]}",
    "    \"\"\"",
    "    cache: Dict[str, Dict[str, Any]] = {}",
    "    lock = threading.Lock()  # Thread safety for concurrent access",
    "    ",
    "    @wraps(fn)",
    "    def memoized(*args):",
    "        # Separate callback (last arg) from cache key args",
    "        *rest_args, callback = args",
    "        key = json.dumps(rest_args, default=str)",
    "        ",
    "        with lock:",
    "            if key in cache:",
    "                entry = cache[key]",
    "                if entry['status'] == 'resolved':",
    "                    # Cache hit - immediate callback",
    "                    callback(None, entry['value'])",
    "                    return",
    "                elif entry['status'] == 'pending':",
    "                    # Already pending - queue this callback",
    "                    entry['callbacks'].append(callback)",
    "                    return",
    "            ",
    "            # First call with these args - create pending entry",
    "            cache[key] = {'status': 'pending', 'callbacks': [callback]}",
    "        ",
    "        def internal_callback(err, result):",
    "            \"\"\"Called when original async function completes.\"\"\"",
    "            with lock:",
    "                entry = cache[key]",
    "                callbacks = entry['callbacks']",
    "                ",
    "                if err:",
    "                    # Don't cache errors - allow retry",
    "                    del cache[key]",
    "                else:",
    "                    # Cache successful result",
    "                    cache[key] = {'status': 'resolved', 'value': result}",
    "            ",
    "            # Notify all waiting callbacks (outside lock)",
    "            for cb in callbacks:",
    "                cb(err, result if not err else None)",
    "        ",
    "        # Call original async function",
    "        fn(*rest_args, internal_callback)",
    "    ",
    "    memoized.cache = cache",
    "    return memoized",
    "",
    "",
    "# ============ Demo and Tests ============",
    "if __name__ == '__main__':",
    "    import time",
    "    ",
    "    print('=' * 60)",
    "    print('ASYNC MEMOIZATION DEMO')",
    "    print('=' * 60)",
    "    ",
    "    # Simulate async function with callback (like Node.js)",
    "    call_count = 0",
    "    ",
    "    def fetch_user(user_id, callback):",
    "        \"\"\"Simulates async API call with 500ms delay.\"\"\"",
    "        global call_count",
    "        call_count += 1",
    "        print(f'  [fetch_user] Starting fetch for id={user_id}')",
    "        ",
    "        def delayed():",
    "            time.sleep(0.5)",
    "            callback(None, {'id': user_id, 'name': f'User {user_id}'})",
    "        ",
    "        threading.Thread(target=delayed).start()",
    "    ",
    "    memo_fetch = memoize_async(fetch_user)",
    "    ",
    "    # Test 1: First call (slow - async)",
    "    print('\\n[Test 1] First call (async, ~500ms):')",
    "    event1 = threading.Event()",
    "    start = time.time()",
    "    ",
    "    memo_fetch(1, lambda err, user: (",
    "        print(f'  cb1: {user} in {time.time()-start:.2f}s'),",
    "        event1.set()",
    "    )[-1])",
    "    ",
    "    event1.wait()",
    "    ",
    "    # Test 2: Cache hit (immediate)",
    "    print('\\n[Test 2] Same args - cache hit (immediate):')",
    "    start = time.time()",
    "    ",
    "    memo_fetch(1, lambda err, user:",
    "        print(f'  cb2: {user} in {time.time()-start:.4f}s')",
    "    )",
    "    time.sleep(0.1)  # Brief wait for callback",
    "    ",
    "    # Test 3: Concurrent calls - callbacks queued",
    "    print('\\n[Test 3] Concurrent calls (3 callbacks queued):')",
    "    events = [threading.Event() for _ in range(3)]",
    "    ",
    "    for i, evt in enumerate(events):",
    "        def make_cb(idx, e):",
    "            return lambda err, user: (print(f'  cb{idx+3}: {user}'), e.set())",
    "        memo_fetch(2, make_cb(i, evt))",
    "    ",
    "    for evt in events:",
    "        evt.wait()",
    "    ",
    "    print(f'\\nTotal fetch_user calls: {call_count} (should be 2)')",
    "    print(f'Cache keys: {list(memo_fetch.cache.keys())}')",
    "    print('\\n' + '=' * 60)",
    "    print('All tests passed!')"
  ],
  "solution_java_lines": [
    "import java.util.*;",
    "import java.util.concurrent.*;",
    "import java.util.function.Function;",
    "import com.google.gson.Gson;",
    "",
    "/**",
    " * Memoization utility - Part 2: Async Memoization",
    " * Extends Part 1 with callback-based async function support.",
    " */",
    "public class Memoize {",
    "    ",
    "    private static final Gson gson = new Gson();",
    "    ",
    "    // ============ Part 1: Sync Memoization (unchanged) ============",
    "    public static <T, R> Function<T, R> memoize(Function<T, R> fn) {",
    "        Map<String, R> cache = new HashMap<>();",
    "        return (T arg) -> {",
    "            String key = gson.toJson(arg);",
    "            if (cache.containsKey(key)) return cache.get(key);",
    "            R result = fn.apply(arg);",
    "            cache.put(key, result);",
    "            return result;",
    "        };",
    "    }",
    "    ",
    "    // ============ Part 2: Async Memoization ============",
    "    ",
    "    /** Error-first callback interface (Node.js style) */",
    "    @FunctionalInterface",
    "    public interface AsyncCallback<R> {",
    "        void call(Exception err, R result);",
    "    }",
    "    ",
    "    /** Async function with callback as separate param */",
    "    @FunctionalInterface",
    "    public interface AsyncFunction<R> {",
    "        void apply(Object[] args, AsyncCallback<R> callback);",
    "    }",
    "    ",
    "    /** Cache entry tracking pending/resolved state */",
    "    private static class CacheEntry<R> {",
    "        volatile String status;  // \"pending\" or \"resolved\"",
    "        volatile R value;",
    "        final List<AsyncCallback<R>> callbacks = new ArrayList<>();",
    "        ",
    "        CacheEntry() { this.status = \"pending\"; }",
    "    }",
    "    ",
    "    /**",
    "     * Memoize async callback-based functions.",
    "     * Handles concurrent calls by queuing callbacks while pending.",
    "     */",
    "    public static <R> AsyncFunction<R> memoizeAsync(AsyncFunction<R> fn) {",
    "        Map<String, CacheEntry<R>> cache = new ConcurrentHashMap<>();",
    "        ",
    "        return (Object[] args, AsyncCallback<R> callback) -> {",
    "            String key = gson.toJson(args);",
    "            ",
    "            synchronized (cache) {",
    "                CacheEntry<R> entry = cache.get(key);",
    "                ",
    "                if (entry != null) {",
    "                    if (\"resolved\".equals(entry.status)) {",
    "                        // Cache hit - immediate callback",
    "                        callback.call(null, entry.value);",
    "                        return;",
    "                    } else {",
    "                        // Pending - queue callback",
    "                        entry.callbacks.add(callback);",
    "                        return;",
    "                    }",
    "                }",
    "                ",
    "                // First call - create pending entry",
    "                entry = new CacheEntry<>();",
    "                entry.callbacks.add(callback);",
    "                cache.put(key, entry);",
    "            }",
    "            ",
    "            // Call original async function",
    "            fn.apply(args, (Exception err, R result) -> {",
    "                List<AsyncCallback<R>> toNotify;",
    "                ",
    "                synchronized (cache) {",
    "                    CacheEntry<R> entry = cache.get(key);",
    "                    toNotify = new ArrayList<>(entry.callbacks);",
    "                    ",
    "                    if (err != null) {",
    "                        cache.remove(key);  // Don't cache errors",
    "                    } else {",
    "                        entry.status = \"resolved\";",
    "                        entry.value = result;",
    "                        entry.callbacks.clear();",
    "                    }",
    "                }",
    "                ",
    "                // Notify all waiting callbacks",
    "                for (AsyncCallback<R> cb : toNotify) {",
    "                    cb.call(err, result);",
    "                }",
    "            });",
    "        };",
    "    }",
    "    ",
    "    // ============ Demo ============",
    "    public static void main(String[] args) throws InterruptedException {",
    "        System.out.println(\"=\".repeat(60));",
    "        System.out.println(\"ASYNC MEMOIZATION DEMO (Java)\");",
    "        System.out.println(\"=\".repeat(60));",
    "        ",
    "        int[] callCount = {0};",
    "        ",
    "        // Simulated async fetch function",
    "        AsyncFunction<Map<String, Object>> fetchUser = (Object[] a, AsyncCallback<Map<String, Object>> cb) -> {",
    "            int userId = (int) a[0];",
    "            callCount[0]++;",
    "            System.out.println(\"  [fetchUser] Starting fetch for id=\" + userId);",
    "            ",
    "            new Thread(() -> {",
    "                try {",
    "                    Thread.sleep(500);",
    "                    Map<String, Object> user = new HashMap<>();",
    "                    user.put(\"id\", userId);",
    "                    user.put(\"name\", \"User \" + userId);",
    "                    cb.call(null, user);",
    "                } catch (InterruptedException e) {",
    "                    cb.call(e, null);",
    "                }",
    "            }).start();",
    "        };",
    "        ",
    "        AsyncFunction<Map<String, Object>> memoFetch = memoizeAsync(fetchUser);",
    "        ",
    "        // Test 1: First call",
    "        System.out.println(\"\\n[Test 1] First call (async):\");",
    "        CountDownLatch latch1 = new CountDownLatch(1);",
    "        long start = System.currentTimeMillis();",
    "        ",
    "        memoFetch.apply(new Object[]{1}, (err, user) -> {",
    "            System.out.println(\"  cb1: \" + user + \" in \" + (System.currentTimeMillis()-start) + \"ms\");",
    "            latch1.countDown();",
    "        });",
    "        latch1.await();",
    "        ",
    "        // Test 2: Cache hit",
    "        System.out.println(\"\\n[Test 2] Cache hit (immediate):\");",
    "        long start2 = System.currentTimeMillis();",
    "        memoFetch.apply(new Object[]{1}, (err, user) -> {",
    "            System.out.println(\"  cb2: \" + user + \" in \" + (System.currentTimeMillis()-start2) + \"ms\");",
    "        });",
    "        Thread.sleep(50);",
    "        ",
    "        // Test 3: Concurrent calls",
    "        System.out.println(\"\\n[Test 3] Concurrent calls:\");",
    "        CountDownLatch latch3 = new CountDownLatch(3);",
    "        for (int i = 0; i < 3; i++) {",
    "            int idx = i;",
    "            memoFetch.apply(new Object[]{2}, (err, user) -> {",
    "                System.out.println(\"  cb\" + (idx+3) + \": \" + user);",
    "                latch3.countDown();",
    "            });",
    "        }",
    "        latch3.await();",
    "        ",
    "        System.out.println(\"\\nTotal fetchUser calls: \" + callCount[0] + \" (should be 2)\");",
    "        System.out.println(\"\\n\" + \"=\".repeat(60));",
    "    }",
    "}"
  ],
  "code_walkthrough": [
    {
      "lines": "1-20",
      "explanation": "Part 1 memoize function preserved unchanged - sync caching with JSON key"
    },
    {
      "lines": "23-35",
      "explanation": "memoize_async setup: cache dict for status objects, threading lock for safety"
    },
    {
      "lines": "37-52",
      "explanation": "Main logic: separate callback from args, check cache for resolved/pending states"
    },
    {
      "lines": "54-65",
      "explanation": "First call path: create pending entry, then invoke original function"
    },
    {
      "lines": "67-80",
      "explanation": "Internal callback: on completion, update cache and notify all queued callbacks"
    }
  ],
  "complexity_analysis": {
    "time": {
      "new_methods": {
        "memoize_async": {
          "complexity": "O(1)",
          "explanation": "Just creates closure with empty cache"
        },
        "memoized_async_call_hit": {
          "complexity": "O(k)",
          "explanation": "O(k) key serialization + O(1) lookup + O(1) callback"
        },
        "memoized_async_call_pending": {
          "complexity": "O(k)",
          "explanation": "O(k) key + O(1) append to callbacks array"
        },
        "async_completion": {
          "complexity": "O(m)",
          "explanation": "O(m) to notify m queued callbacks"
        }
      },
      "overall_change": "Same O(k) per call as Part 1, plus O(m) on completion to notify waiters"
    },
    "space": {
      "additional_space": "O(n \u00d7 k + pending callbacks)",
      "explanation": "Cache entries now include status field and callbacks array for pending entries. Callbacks array is transient - cleared on resolution."
    }
  },
  "dry_run": {
    "example_input": "Three calls: fetch(1,cb1), fetch(1,cb2) at t=500ms, fetch(1,cb3) at t=1000ms",
    "steps": [
      {
        "step": 1,
        "action": "fetch(1, cb1) at t=0ms",
        "state": "cache = {'[1]': {status:'pending', callbacks:[cb1]}}",
        "explanation": "First call - create pending entry, start async fetch"
      },
      {
        "step": 2,
        "action": "fetch(1, cb2) at t=500ms (before first completes)",
        "state": "cache = {'[1]': {status:'pending', callbacks:[cb1,cb2]}}",
        "explanation": "Pending - cb2 queued, NO new fetch started"
      },
      {
        "step": 3,
        "action": "Async completes at t=600ms",
        "state": "cache = {'[1]': {status:'resolved', value:{id:1,name:'User 1'}}}",
        "explanation": "Update to resolved, notify cb1 and cb2"
      },
      {
        "step": 4,
        "action": "fetch(1, cb3) at t=1000ms",
        "state": "cache unchanged",
        "explanation": "Cache hit - cb3 called immediately with cached value"
      }
    ],
    "final_output": "All three callbacks receive {id:1, name:'User 1'}, only ONE actual fetch"
  },
  "debugging_playbook": {
    "fast_sanity_checks": [
      "Single call: fetch(1,cb) \u2192 cb should be called with result",
      "Duplicate call: fetch(1,cb1); wait; fetch(1,cb2) \u2192 cb2 immediate"
    ],
    "likely_bugs": [
      "Forgetting to exclude callback from cache key \u2192 always cache miss",
      "Not locking \u2192 race condition corrupts callbacks list",
      "Notifying callbacks inside lock \u2192 potential deadlock",
      "Caching errors \u2192 retries fail permanently"
    ],
    "recommended_logs_or_asserts": [
      "Log: 'Cache miss for key: ...'",
      "Log: 'Queuing callback for pending key: ...'",
      "Log: 'Resolving key: ... with N callbacks'",
      "Assert: len(callbacks) > 0 when completing"
    ],
    "how_to_localize": "Add timestamps to all logs. If second call triggers new fetch, key generation is wrong. If callbacks not called, check notification loop."
  },
  "edge_cases": [
    {
      "case": "Error from async function",
      "handling": "Delete cache entry, notify all callbacks with error",
      "gotcha": "Must delete BEFORE notifying so retry works"
    },
    {
      "case": "Empty args (only callback)",
      "handling": "key = '[]', works correctly",
      "gotcha": "Ensure callback separation handles single-arg case"
    },
    {
      "case": "Callback called synchronously by original fn",
      "handling": "Still works - pending entry exists before callback",
      "gotcha": "Lock must be released before fn called to avoid deadlock"
    },
    {
      "case": "Same args with different callbacks",
      "handling": "All callbacks queued and notified",
      "gotcha": "Callbacks are NOT part of cache key"
    }
  ],
  "test_cases": [
    {
      "name": "Basic async caching",
      "input": "fetch(1,cb1) \u2192 wait \u2192 fetch(1,cb2)",
      "expected": "cb1 after delay, cb2 immediate, fetch called once",
      "explanation": "Second call hits resolved cache"
    },
    {
      "name": "Concurrent calls queued",
      "input": "fetch(1,cb1), fetch(1,cb2), fetch(1,cb3) rapidly",
      "expected": "All three callbacks called together after delay",
      "explanation": "cb2 and cb3 queued while cb1 pending"
    },
    {
      "name": "Different args not cached",
      "input": "fetch(1,cb1) \u2192 fetch(2,cb2)",
      "expected": "Both trigger separate async calls",
      "explanation": "Different keys, independent cache entries"
    },
    {
      "name": "Error not cached",
      "input": "fetch_error(1,cb1) \u2192 fetch(1,cb2)",
      "expected": "Both callbacks get error, second call retries",
      "explanation": "Error deletes cache entry allowing retry"
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Including callback in cache key",
      "why_wrong": "Every call would be unique, defeating memoization",
      "correct_approach": "Separate callback from other args before key generation",
      "code_example_wrong": "key = json.dumps(args)  # includes callback!",
      "code_example_correct": "*rest_args, callback = args\\nkey = json.dumps(rest_args)"
    },
    {
      "mistake": "Not handling pending state",
      "why_wrong": "Multiple concurrent calls would trigger multiple fetches",
      "correct_approach": "Track pending with callbacks array, queue new callbacks",
      "code_example_wrong": "if key not in cache:\\n    fn(*args, callback)",
      "code_example_correct": "if entry['status'] == 'pending':\\n    entry['callbacks'].append(callback)"
    },
    {
      "mistake": "Calling callbacks inside lock",
      "why_wrong": "Callback might call memoized function again \u2192 deadlock",
      "correct_approach": "Copy callbacks list, release lock, then notify",
      "code_example_wrong": "with lock:\\n    for cb in callbacks: cb(None, result)",
      "code_example_correct": "with lock:\\n    callbacks = list(entry['callbacks'])\\nfor cb in callbacks: cb(None, result)"
    }
  ],
  "interview_tips": {
    "how_to_present": "Start by identifying the THREE cache states (absent/pending/resolved). Draw the timeline diagram showing how concurrent calls are handled. Then implement.",
    "what_to_mention": [
      "Why callback must be excluded from cache key",
      "The race condition of concurrent calls",
      "Thread safety considerations",
      "Why errors shouldn't be cached"
    ],
    "time_allocation": "2 min understanding, 3 min design/diagram, 10 min implementation, 2 min testing",
    "if_stuck": [
      "Think: what happens if second call arrives before first completes?",
      "What data structure can hold 'waiting' callbacks?",
      "How do I know if a result is cached vs still loading?"
    ]
  },
  "connection_to_next_part": "Part 3 likely adds Promise-based memoization. The status-based cache pattern extends naturally: instead of callbacks array, store the Promise itself. Pending calls await the same Promise.",
  "communication_script": {
    "transition_from_previous": "Part 1's sync memoization is working. For Part 2, I need to handle async callback-based functions. The key challenge is handling concurrent calls before the first completes.",
    "explaining_changes": "I'll change the cache entries from simple values to status objects: {status: 'pending'|'resolved', value?, callbacks?[]}. This lets me track in-flight operations and queue waiters.",
    "while_extending_code": [
      "Adding memoize_async as a new function...",
      "The cache entries now have status to track pending vs resolved...",
      "Using a lock here for thread safety with concurrent calls..."
    ],
    "after_completing": "This now handles async callbacks. Cache hit is O(k), pending calls are queued, and original function is called only once per unique args. Ready for Part 3?"
  },
  "time_milestones": {
    "time_budget": "15-20 minutes for this part",
    "by_2_min": "Understand: async callback, pending state requirement, race condition",
    "by_5_min": "Design: three-state cache, callback queue, explain approach",
    "by_12_min": "Implementation complete with pending/resolved logic",
    "by_15_min": "Testing with concurrent calls demo",
    "warning_signs": "If stuck on callback separation by 5 min, ask for clarification"
  },
  "recovery_strategies": {
    "if_part_builds_wrong": "Part 1 is independent - Part 2 is a new function. If Part 1 has issues, can fix later.",
    "if_new_requirement_unclear": "Ask: 'Just to confirm, the callback is always the last argument, right? And we use error-first (err, result) convention?'",
    "if_running_behind": "Implement basic pending/resolved first. Skip thread safety initially, mention it verbally."
  },
  "signal_points": {
    "wow_factors_for_followup": [
      "Immediately identifying the race condition problem",
      "Drawing the timeline diagram before coding",
      "Mentioning thread safety unprompted",
      "Discussing why errors shouldn't be cached",
      "Clean separation: callback extraction, state management, notification"
    ]
  },
  "pattern_recognition": {
    "pattern": "Request Coalescing / Deduplication",
    "indicators": [
      "Multiple calls for same resource",
      "Async operation that shouldn't be duplicated",
      "Need to notify multiple waiters"
    ],
    "similar_problems": [
      "HTTP request deduplication",
      "Database connection pooling",
      "React Query/SWR stale-while-revalidate"
    ],
    "template": "cache[key] = {status: 'pending'|'resolved', value?, waiters?[]}"
  },
  "thinking_process": [
    {
      "step": 1,
      "thought": "When I see callback-based async, I think about what happens with concurrent calls",
      "why": "The timing gap between call and callback creates a window for race conditions"
    },
    {
      "step": 2,
      "thought": "Need to track 'in-flight' state - not just 'cached' vs 'not cached'",
      "why": "Binary state would either duplicate calls or miss notifications"
    },
    {
      "step": 3,
      "thought": "Queue pattern - store waiting callbacks, notify all when done",
      "why": "Classic producer-consumer: one producer (async fn), multiple consumers (callbacks)"
    }
  ],
  "interviewer_perspective": {
    "what_they_evaluate": [
      "Understanding of async patterns and race conditions",
      "Ability to extend code without rewriting",
      "Clean state management (pending vs resolved)",
      "Consideration of edge cases (errors, concurrency)"
    ],
    "bonus_points": [
      "Thread safety without being asked",
      "Drawing timeline diagram",
      "Mentioning this is similar to Promise.all behavior",
      "Clean callback separation"
    ],
    "red_flags": [
      "Not recognizing the concurrent calls problem",
      "Including callback in cache key",
      "Potential deadlocks from lock misuse",
      "Ignoring error handling"
    ]
  },
  "ai_copilot_tips": {
    "what_to_do": [
      "Use AI to help with threading.Lock syntax",
      "Let it generate the test harness with threading.Event"
    ],
    "what_not_to_do": [
      "Don't let AI miss the pending state requirement",
      "Verify callback separation logic manually"
    ]
  },
  "red_flags_to_avoid": {
    "behavioral": [
      "Not asking about callback convention (error-first?)",
      "Starting to code without addressing concurrent calls"
    ],
    "technical": [
      "Using args directly including callback as key",
      "Forgetting to handle the pending \u2192 resolved transition",
      "Blocking async completion on lock held by waiting call"
    ],
    "communication": [
      "Not explaining why pending state is needed",
      "Skipping the race condition explanation"
    ]
  },
  "final_checklist": {
    "before_saying_done": [
      "Does callback get excluded from cache key?",
      "Is pending state tracked with callback queue?",
      "Are all queued callbacks notified on completion?",
      "Is error handling correct (don't cache)?",
      "Is there thread safety if needed?"
    ],
    "quick_code_review": [
      "Lock acquired before cache check, released before fn call",
      "Callbacks notified outside of lock",
      "Part 1 memoize still works unchanged"
    ]
  },
  "production_considerations": {
    "what_i_would_add": [
      "Configurable error caching with TTL",
      "Cache size limits",
      "Metrics: hit rate, pending queue size",
      "Timeout handling for stuck async calls"
    ],
    "why_not_in_interview": "Focus on core async handling logic; mention these verbally",
    "how_to_mention": "In production, I'd add timeout handling and metrics to track cache effectiveness."
  },
  "generated_at": "2026-01-19T04:44:40.101751",
  "_meta": {
    "problem_id": "javascript_polyfills_and_memoization",
    "part_number": 2,
    "model": "claude-opus-4-5-20251101"
  }
}