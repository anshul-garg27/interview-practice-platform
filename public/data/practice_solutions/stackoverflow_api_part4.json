{
  "problem_title": "Stack Overflow API Design - Part 4: Scalability and Security Discussion",
  "part_number": 4,
  "builds_on": "Part 3",
  "difficulty": "hard",
  "problem_understanding": {
    "what_changes": "This part shifts from implementation to architectural discussion. No new API methods are required - instead, we discuss how to scale the existing system and add security layers. The focus is on demonstrating deep understanding of production systems.",
    "new_requirements": [
      "Discuss database sharding strategies",
      "Explain caching approach for read-heavy workload",
      "Address rate limiting to prevent abuse",
      "Cover authentication and authorization",
      "Discuss input validation and spam detection"
    ],
    "new_constraints": [
      "Must handle millions of users and questions",
      "Read-heavy workload (100:1 read:write ratio typical for Q&A)",
      "Sub-second response times for searches",
      "Protection against malicious actors"
    ],
    "key_insight": "Separate concerns: read/write paths, cache aggressively, use specialized systems (Elasticsearch for search, Redis for cache), and apply defense-in-depth for security."
  },
  "requirements_coverage": {
    "checklist": [
      {
        "requirement": "Database Sharding",
        "how_met": "Shard by question_id for content, separate user DB, geographic distribution",
        "gotchas": [
          "Cross-shard queries are expensive",
          "Need consistent hashing for rebalancing"
        ]
      },
      {
        "requirement": "Caching Strategy",
        "how_met": "Redis cache for hot questions, user profiles, search results with TTL",
        "gotchas": [
          "Cache invalidation is hard",
          "Thundering herd on cache miss"
        ]
      },
      {
        "requirement": "Rate Limiting",
        "how_met": "Token bucket algorithm per user+IP, different limits by endpoint",
        "gotchas": [
          "Distributed rate limiting needs coordination",
          "Legitimate high-traffic users"
        ]
      },
      {
        "requirement": "Authentication",
        "how_met": "JWT tokens with short expiry, refresh token rotation",
        "gotchas": [
          "Token revocation with stateless JWT",
          "Secure token storage client-side"
        ]
      },
      {
        "requirement": "Authorization",
        "how_met": "Role-based access control, ownership checks before edit/delete",
        "gotchas": [
          "Privilege escalation attacks",
          "Inconsistent enforcement"
        ]
      },
      {
        "requirement": "Input Validation",
        "how_met": "Sanitize all inputs, parameterized queries, CSP headers",
        "gotchas": [
          "Stored XSS through question/answer content",
          "Unicode normalization attacks"
        ]
      }
    ],
    "complexity_targets": [
      {
        "operation": "search (with Elasticsearch)",
        "target": "O(log n)",
        "achieved": "O(log n)",
        "why": "Inverted index with distributed sharding"
      },
      {
        "operation": "get_question (cached)",
        "target": "O(1)",
        "achieved": "O(1)",
        "why": "Redis lookup by ID"
      },
      {
        "operation": "rate_limit_check",
        "target": "O(1)",
        "achieved": "O(1)",
        "why": "Token bucket state in Redis"
      }
    ],
    "non_goals": [
      "Implementing full Elasticsearch integration",
      "Building actual distributed system",
      "Complete OAuth flow implementation"
    ]
  },
  "assumptions": [
    "Read:write ratio is approximately 100:1 (typical for Q&A platforms)",
    "Hot questions (top 1%) account for 90% of reads",
    "Global user base requiring geographic distribution",
    "Budget allows for managed services (Redis, Elasticsearch)",
    "Eventually consistent reads are acceptable for non-critical data"
  ],
  "tradeoffs": [
    {
      "decision": "Shard by question_id vs user_id",
      "chosen": "question_id",
      "why": "Questions are the primary access pattern; user profile lookups are separate",
      "alternative": "user_id",
      "when_to_switch": "If user activity feeds become primary feature"
    },
    {
      "decision": "Redis vs Memcached",
      "chosen": "Redis",
      "why": "Richer data structures (sorted sets for leaderboards), persistence option",
      "alternative": "Memcached",
      "when_to_switch": "Pure key-value caching at extreme scale"
    },
    {
      "decision": "Elasticsearch vs in-memory index",
      "chosen": "Elasticsearch at scale",
      "why": "Handles distributed full-text search, relevance ranking, faceted search",
      "alternative": "In-memory (current)",
      "when_to_switch": "Small scale (<100K questions) where latency is critical"
    },
    {
      "decision": "JWT vs Session tokens",
      "chosen": "JWT",
      "why": "Stateless, scalable across servers, self-contained claims",
      "alternative": "Server sessions",
      "when_to_switch": "Need immediate token revocation"
    }
  ],
  "extensibility_notes": {
    "what_to_keep_stable": [
      "All Part 1-3 public APIs",
      "Data model structure",
      "Return types"
    ],
    "what_to_change": [
      "Add caching layer wrapper",
      "Add rate limiting decorator",
      "Add input validation"
    ],
    "interfaces_and_boundaries": "Introduce service layer abstraction: API Gateway \u2192 Service Layer \u2192 Data Layer, each with clear responsibilities",
    "invariants": [
      "Cache consistency with database",
      "Rate limit state accuracy",
      "Authorization checks before any mutation"
    ]
  },
  "visual_explanation": {
    "before_after": "BEFORE (Part 3):                    AFTER (Production):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502StackOverflow\u2502                    \u2502   Load Balancer  \u2502\n\u2502   (single)  \u2502                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n      \u2502                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n      \u25bc                           \u2502         \u2502         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n\u2502  In-Memory  \u2502               \u2502 API-1 \u2502 \u2502 API-2 \u2502 \u2502 API-3 \u2502\n\u2502   Storage   \u2502               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n                                  \u2502   Redis   \u2502\n                                  \u2502   Cache   \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502         \u2502         \u2502\n                          \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510\n                          \u2502 DB-1  \u2502 \u2502 DB-2  \u2502 \u2502Elastic\u2502\n                          \u2502(users)\u2502 \u2502(quest)\u2502 \u2502Search \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
    "algorithm_flow": "REQUEST FLOW:\n\n1. Client Request\n       \u2502\n       \u25bc\n2. \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Rate Limiter    \u2502\u2500\u2500\u2192 429 if exceeded\n   \u2502 (Redis counter) \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n       \u25bc\n3. \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Auth Middleware \u2502\u2500\u2500\u2192 401 if invalid\n   \u2502 (JWT verify)    \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n       \u25bc\n4. \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Input Validator \u2502\u2500\u2500\u2192 400 if malformed\n   \u2502 (sanitize)      \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n       \u25bc\n5. \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Cache Check     \u2502\u2500\u2500\u2192 Return if hit\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502 miss\n       \u25bc\n6. \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Business Logic  \u2502\n   \u2502 (Part 1-3 code) \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n       \u25bc\n7. \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Update Cache    \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n       \u25bc\n8. Response"
  },
  "approaches": [
    {
      "name": "Naive Extension - Add Everything In-Process",
      "description": "Add caching, rate limiting, and validation all within the same Python class using local data structures",
      "time_complexity": "O(1) for cache/rate limit",
      "space_complexity": "O(users + requests) unbounded",
      "why_not_optimal": "Doesn't scale horizontally - each server has its own cache/rate limit state, leading to inconsistency and memory pressure"
    },
    {
      "name": "Optimal Approach - Layered Architecture with External Services",
      "description": "Separate concerns into layers, use Redis for shared state (cache, rate limits), Elasticsearch for search, sharded databases for persistence",
      "time_complexity": "O(log n) for most ops with proper indexing",
      "space_complexity": "O(data) distributed across services",
      "key_insight": "Each component does what it's best at: Redis for fast state, Elasticsearch for search, PostgreSQL for ACID transactions"
    }
  ],
  "optimal_solution": {
    "explanation_md": "## Production Architecture for Stack Overflow\n\n### 1. **Database Sharding Strategy**\n\nShard by **question_id hash** for content:\n- Questions and answers together (same shard = no cross-shard joins)\n- Consistent hashing allows adding shards without full rebalance\n- Users in **separate database** (different access pattern)\n\n```\nShard Key = hash(question_id) % num_shards\n```\n\n### 2. **Caching Strategy (Redis)**\n\n**Cache these hot paths:**\n- `question:{id}` \u2192 Full question object (TTL: 5 min)\n- `user:{id}:profile` \u2192 User profile (TTL: 15 min)\n- `top_questions` \u2192 Sorted set by score (TTL: 1 min)\n- `search:{query_hash}` \u2192 Search results (TTL: 30 sec)\n\n**Cache invalidation:**\n- Write-through for critical updates\n- TTL-based expiry for eventual consistency\n\n### 3. **Rate Limiting**\n\n**Token Bucket per (user_id, endpoint):**\n```\nPOST /questions: 10 per hour\nPOST /answers: 30 per hour  \nGET /search: 100 per minute\nVotes: 30 per day\n```\n\n### 4. **Security Layers**\n\n| Layer | Protection |\n|-------|------------|\n| API Gateway | Rate limiting, IP blocking |\n| Auth | JWT validation, RBAC |\n| Input | XSS sanitization, SQL injection |\n| Data | Encryption at rest, audit logs |",
    "data_structures": [
      {
        "structure": "Redis Sorted Sets",
        "purpose": "Leaderboards (top questions by score)"
      },
      {
        "structure": "Redis Hash",
        "purpose": "User session/rate limit counters"
      },
      {
        "structure": "Elasticsearch Index",
        "purpose": "Full-text search with relevance ranking"
      },
      {
        "structure": "PostgreSQL with partitioning",
        "purpose": "Durable storage with sharding"
      }
    ],
    "algorithm_steps": [
      "Step 1: Request hits load balancer, routed to available API server",
      "Step 2: Rate limiter checks Redis counter, rejects if exceeded",
      "Step 3: JWT middleware validates token, extracts user claims",
      "Step 4: Input validator sanitizes all string inputs",
      "Step 5: Check Redis cache for read requests",
      "Step 6: On cache miss, query database/Elasticsearch",
      "Step 7: Update cache with result, set appropriate TTL",
      "Step 8: Return response, log for audit"
    ]
  },
  "solution_python_lines": [
    "\"\"\"",
    "Stack Overflow API - Part 4: Scalability and Security Discussion",
    "Demonstrates production patterns: caching, rate limiting, auth, validation.",
    "\"\"\"",
    "import time",
    "import re",
    "import hashlib",
    "import html",
    "from typing import List, Optional, Dict, Set, Callable",
    "from collections import defaultdict",
    "from functools import wraps",
    "",
    "",
    "# ============ Production Infrastructure Abstractions ============",
    "",
    "class RateLimiter:",
    "    \"\"\"Token bucket rate limiter. In prod: use Redis for distributed state.\"\"\"",
    "    ",
    "    def __init__(self):",
    "        self.buckets: Dict[str, Dict] = {}  # key -> {tokens, last_refill}",
    "        self.limits = {",
    "            'post_question': (10, 3600),   # 10 per hour",
    "            'post_answer': (30, 3600),      # 30 per hour",
    "            'vote': (30, 86400),            # 30 per day",
    "            'search': (100, 60),            # 100 per minute",
    "        }",
    "    ",
    "    def check(self, user_id: str, action: str) -> bool:",
    "        \"\"\"Returns True if action allowed, False if rate limited.\"\"\"",
    "        if action not in self.limits:",
    "            return True",
    "        max_tokens, refill_secs = self.limits[action]",
    "        key = f\"{user_id}:{action}\"",
    "        now = time.time()",
    "        ",
    "        if key not in self.buckets:",
    "            self.buckets[key] = {'tokens': max_tokens, 'last': now}",
    "        ",
    "        bucket = self.buckets[key]",
    "        elapsed = now - bucket['last']",
    "        bucket['tokens'] = min(max_tokens, bucket['tokens'] + elapsed * max_tokens / refill_secs)",
    "        bucket['last'] = now",
    "        ",
    "        if bucket['tokens'] >= 1:",
    "            bucket['tokens'] -= 1",
    "            return True",
    "        return False",
    "",
    "",
    "class Cache:",
    "    \"\"\"Simple cache abstraction. In prod: use Redis with proper TTL.\"\"\"",
    "    ",
    "    def __init__(self, default_ttl: int = 300):",
    "        self.store: Dict[str, tuple] = {}  # key -> (value, expires_at)",
    "        self.default_ttl = default_ttl",
    "        self.hits = 0",
    "        self.misses = 0",
    "    ",
    "    def get(self, key: str):",
    "        if key in self.store:",
    "            value, expires = self.store[key]",
    "            if time.time() < expires:",
    "                self.hits += 1",
    "                return value",
    "            del self.store[key]",
    "        self.misses += 1",
    "        return None",
    "    ",
    "    def set(self, key: str, value, ttl: int = None):",
    "        self.store[key] = (value, time.time() + (ttl or self.default_ttl))",
    "    ",
    "    def invalidate(self, key: str):",
    "        self.store.pop(key, None)",
    "    ",
    "    def stats(self) -> dict:",
    "        total = self.hits + self.misses",
    "        return {'hits': self.hits, 'misses': self.misses, ",
    "                'hit_rate': self.hits / total if total > 0 else 0}",
    "",
    "",
    "class InputValidator:",
    "    \"\"\"Sanitize inputs to prevent XSS and injection attacks.\"\"\"",
    "    ",
    "    @staticmethod",
    "    def sanitize_html(text: str) -> str:",
    "        \"\"\"Escape HTML entities to prevent XSS.\"\"\"",
    "        return html.escape(text)",
    "    ",
    "    @staticmethod",
    "    def validate_username(username: str) -> bool:",
    "        \"\"\"Alphanumeric, 3-20 chars.\"\"\"",
    "        return bool(re.match(r'^[a-zA-Z0-9_]{3,20}$', username))",
    "    ",
    "    @staticmethod",
    "    def validate_question(title: str, body: str) -> tuple:",
    "        \"\"\"Returns (is_valid, error_message).\"\"\"",
    "        if len(title) < 10:",
    "            return False, 'Title must be at least 10 characters'",
    "        if len(title) > 150:",
    "            return False, 'Title must be under 150 characters'",
    "        if len(body) < 30:",
    "            return False, 'Body must be at least 30 characters'",
    "        return True, None",
    "",
    "",
    "class SpamDetector:",
    "    \"\"\"Basic spam detection. In prod: use ML model or service like Akismet.\"\"\"",
    "    ",
    "    SPAM_PATTERNS = [",
    "        r'\\b(buy now|click here|free money|casino)\\b',",
    "        r'(http[s]?://){3,}',  # Too many links",
    "    ]",
    "    ",
    "    @classmethod",
    "    def is_spam(cls, text: str) -> bool:",
    "        text_lower = text.lower()",
    "        for pattern in cls.SPAM_PATTERNS:",
    "            if re.search(pattern, text_lower):",
    "                return True",
    "        return False",
    "",
    "",
    "# ============ Core Data Models (unchanged from Part 3) ============",
    "",
    "class User:",
    "    def __init__(self, id: str, username: str):",
    "        self.id = id",
    "        self.username = username",
    "        self.reputation = 0",
    "",
    "",
    "class Question:",
    "    def __init__(self, id: str, author_id: str, title: str, body: str, tags: List[str]):",
    "        self.id = id",
    "        self.author_id = author_id",
    "        self.title = title",
    "        self.body = body",
    "        self.tags = tags",
    "        self.score = 0",
    "        self.created_at = time.time()",
    "",
    "",
    "class Answer:",
    "    def __init__(self, id: str, question_id: str, author_id: str, body: str):",
    "        self.id = id",
    "        self.question_id = question_id",
    "        self.author_id = author_id",
    "        self.body = body",
    "        self.score = 0",
    "        self.is_accepted = False",
    "        self.created_at = time.time()",
    "",
    "",
    "# ============ Main API with Production Patterns ============",
    "",
    "class StackOverflow:",
    "    \"\"\"Stack Overflow API with scalability and security patterns.\"\"\"",
    "    ",
    "    REP_UPVOTE = 10",
    "    REP_DOWNVOTE = -2",
    "    REP_ACCEPTED = 15",
    "    ",
    "    def __init__(self):",
    "        # Core data (Part 1-3)",
    "        self.users: Dict[str, User] = {}",
    "        self.questions: Dict[str, Question] = {}",
    "        self.answers: Dict[str, Answer] = {}",
    "        self.user_questions: Dict[str, List[str]] = defaultdict(list)",
    "        self.tag_questions: Dict[str, List[str]] = defaultdict(list)",
    "        self.question_answers: Dict[str, List[str]] = defaultdict(list)",
    "        self.question_votes: Dict[str, Dict[str, int]] = defaultdict(dict)",
    "        self.answer_votes: Dict[str, Dict[str, int]] = defaultdict(dict)",
    "        self.word_index: Dict[str, Set[str]] = defaultdict(set)",
    "        self.question_counter = 1",
    "        self.answer_counter = 1",
    "        # NEW Part 4: Production infrastructure",
    "        self.cache = Cache(default_ttl=300)",
    "        self.rate_limiter = RateLimiter()",
    "        self.validator = InputValidator()",
    "    ",
    "    def _tokenize(self, text: str) -> Set[str]:",
    "        return set(re.findall(r'[a-zA-Z0-9]+', text.lower()))",
    "    ",
    "    # ============ Part 1 Methods (with production enhancements) ============",
    "    ",
    "    def create_user(self, user_id: str, username: str) -> User:",
    "        if user_id in self.users:",
    "            return self.users[user_id]",
    "        # Validate input",
    "        if not self.validator.validate_username(username):",
    "            return None",
    "        user = User(user_id, self.validator.sanitize_html(username))",
    "        self.users[user_id] = user",
    "        return user",
    "    ",
    "    def post_question(self, user_id: str, title: str, body: str, tags: List[str]) -> Optional[Question]:",
    "        if user_id not in self.users:",
    "            return None",
    "        # Rate limiting",
    "        if not self.rate_limiter.check(user_id, 'post_question'):",
    "            return None  # In prod: raise RateLimitExceeded",
    "        # Input validation",
    "        is_valid, error = self.validator.validate_question(title, body)",
    "        if not is_valid:",
    "            return None  # In prod: raise ValidationError(error)",
    "        # Spam detection",
    "        if SpamDetector.is_spam(title + ' ' + body):",
    "            return None  # In prod: flag for review",
    "        ",
    "        q_id = f\"q_{self.question_counter}\"",
    "        self.question_counter += 1",
    "        # Sanitize inputs",
    "        question = Question(q_id, user_id, ",
    "                           self.validator.sanitize_html(title),",
    "                           self.validator.sanitize_html(body), tags)",
    "        self.questions[q_id] = question",
    "        self.user_questions[user_id].append(q_id)",
    "        for tag in tags:",
    "            self.tag_questions[tag].append(q_id)",
    "        for word in self._tokenize(title + ' ' + body):",
    "            self.word_index[word].add(q_id)",
    "        # Invalidate relevant caches",
    "        self.cache.invalidate('top_questions')",
    "        self.cache.invalidate('recent_questions')",
    "        return question",
    "    ",
    "    def post_answer(self, user_id: str, question_id: str, body: str) -> Optional[Answer]:",
    "        if question_id not in self.questions or user_id not in self.users:",
    "            return None",
    "        if not self.rate_limiter.check(user_id, 'post_answer'):",
    "            return None",
    "        if SpamDetector.is_spam(body):",
    "            return None",
    "        a_id = f\"a_{self.answer_counter}\"",
    "        self.answer_counter += 1",
    "        answer = Answer(a_id, question_id, user_id, self.validator.sanitize_html(body))",
    "        self.answers[a_id] = answer",
    "        self.question_answers[question_id].append(a_id)",
    "        self.cache.invalidate(f'question:{question_id}')",
    "        return answer",
    "    ",
    "    def get_question(self, question_id: str) -> Optional[Question]:",
    "        # Check cache first",
    "        cached = self.cache.get(f'question:{question_id}')",
    "        if cached:",
    "            return cached",
    "        q = self.questions.get(question_id)",
    "        if q:",
    "            self.cache.set(f'question:{question_id}', q, ttl=300)",
    "        return q",
    "    ",
    "    def get_answers_for_question(self, question_id: str) -> List[Answer]:",
    "        return [self.answers[aid] for aid in self.question_answers.get(question_id, [])]",
    "    ",
    "    def get_questions_by_user(self, user_id: str) -> List[Question]:",
    "        qids = self.user_questions.get(user_id, [])",
    "        return sorted([self.questions[qid] for qid in qids], key=lambda q: q.created_at, reverse=True)",
    "    ",
    "    def get_questions_by_tag(self, tag: str) -> List[Question]:",
    "        qids = self.tag_questions.get(tag, [])",
    "        return sorted([self.questions[qid] for qid in qids], key=lambda q: q.score, reverse=True)",
    "    ",
    "    # ============ Part 2: Voting (with rate limiting) ============",
    "    ",
    "    def _apply_vote(self, voter_id: str, item_id: str, new_vote: int,",
    "                    votes_map: Dict, items_map: Dict) -> int:",
    "        if item_id not in items_map or voter_id not in self.users:",
    "            return -1",
    "        if not self.rate_limiter.check(voter_id, 'vote'):",
    "            return -1",
    "        item = items_map[item_id]",
    "        if voter_id == item.author_id:",
    "            return -1",
    "        prev_vote = votes_map[item_id].get(voter_id, 0)",
    "        if prev_vote == new_vote:",
    "            return item.score",
    "        item.score += (new_vote - prev_vote)",
    "        author = self.users[item.author_id]",
    "        if prev_vote == 1: author.reputation -= self.REP_UPVOTE",
    "        elif prev_vote == -1: author.reputation -= self.REP_DOWNVOTE",
    "        if new_vote == 1: author.reputation += self.REP_UPVOTE",
    "        elif new_vote == -1: author.reputation += self.REP_DOWNVOTE",
    "        votes_map[item_id][voter_id] = new_vote",
    "        return item.score",
    "    ",
    "    def upvote_question(self, voter_id: str, question_id: str) -> int:",
    "        result = self._apply_vote(voter_id, question_id, 1, self.question_votes, self.questions)",
    "        if result >= 0:",
    "            self.cache.invalidate(f'question:{question_id}')",
    "            self.cache.invalidate('top_questions')",
    "        return result",
    "    ",
    "    def downvote_question(self, voter_id: str, question_id: str) -> int:",
    "        return self._apply_vote(voter_id, question_id, -1, self.question_votes, self.questions)",
    "    ",
    "    def upvote_answer(self, voter_id: str, answer_id: str) -> int:",
    "        return self._apply_vote(voter_id, answer_id, 1, self.answer_votes, self.answers)",
    "    ",
    "    def downvote_answer(self, voter_id: str, answer_id: str) -> int:",
    "        return self._apply_vote(voter_id, answer_id, -1, self.answer_votes, self.answers)",
    "    ",
    "    def accept_answer(self, question_author_id: str, answer_id: str) -> bool:",
    "        if answer_id not in self.answers:",
    "            return False",
    "        answer = self.answers[answer_id]",
    "        question = self.questions.get(answer.question_id)",
    "        if not question or question.author_id != question_author_id:",
    "            return False",
    "        if answer.is_accepted:",
    "            return True",
    "        answer.is_accepted = True",
    "        self.users[answer.author_id].reputation += self.REP_ACCEPTED",
    "        return True",
    "    ",
    "    def get_user_reputation(self, user_id: str) -> int:",
    "        return self.users[user_id].reputation if user_id in self.users else 0",
    "    ",
    "    # ============ Part 3: Search (with caching) ============",
    "    ",
    "    def search_questions(self, query: str) -> List[Question]:",
    "        if not self.rate_limiter.check('anon', 'search'):",
    "            return []",
    "        cache_key = f'search:{hashlib.md5(query.encode()).hexdigest()}'",
    "        cached = self.cache.get(cache_key)",
    "        if cached:",
    "            return cached",
    "        query_words = self._tokenize(query)",
    "        if not query_words:",
    "            return []",
    "        match_counts: Dict[str, int] = defaultdict(int)",
    "        for qw in query_words:",
    "            matched: Set[str] = set()",
    "            for idx_word, q_ids in self.word_index.items():",
    "                if qw in idx_word:",
    "                    matched.update(q_ids)",
    "            for q_id in matched:",
    "                match_counts[q_id] += 1",
    "        results = [self.questions[q_id] for q_id in match_counts]",
    "        results.sort(key=lambda q: (-match_counts[q.id], -q.score))",
    "        self.cache.set(cache_key, results, ttl=30)",
    "        return results",
    "    ",
    "    def search_by_tags(self, tags: List[str], match_all: bool) -> List[Question]:",
    "        if not tags:",
    "            return []",
    "        tag_sets = [set(self.tag_questions.get(t, [])) for t in tags]",
    "        if match_all:",
    "            result_ids = tag_sets[0].copy()",
    "            for s in tag_sets[1:]:",
    "                result_ids &= s",
    "        else:",
    "            result_ids = set()",
    "            for s in tag_sets:",
    "                result_ids |= s",
    "        results = [self.questions[q_id] for q_id in result_ids]",
    "        results.sort(key=lambda q: -q.score)",
    "        return results",
    "    ",
    "    def get_top_questions(self, limit: int) -> List[Question]:",
    "        cached = self.cache.get('top_questions')",
    "        if cached:",
    "            return cached[:limit]",
    "        all_qs = list(self.questions.values())",
    "        all_qs.sort(key=lambda q: (-q.score, -q.created_at))",
    "        self.cache.set('top_questions', all_qs[:100], ttl=60)",
    "        return all_qs[:limit]",
    "    ",
    "    def get_recent_questions(self, limit: int) -> List[Question]:",
    "        all_qs = list(self.questions.values())",
    "        all_qs.sort(key=lambda q: -q.created_at)",
    "        return all_qs[:limit]",
    "    ",
    "    def get_unanswered_questions(self) -> List[Question]:",
    "        unanswered = [q for q in self.questions.values()",
    "                      if not self.question_answers.get(q.id)]",
    "        unanswered.sort(key=lambda q: -q.created_at)",
    "        return unanswered",
    "    ",
    "    # ============ Part 4: Production Monitoring ============",
    "    ",
    "    def get_cache_stats(self) -> dict:",
    "        \"\"\"Returns cache performance metrics.\"\"\"",
    "        return self.cache.stats()",
    "",
    "",
    "if __name__ == '__main__':",
    "    print('=' * 60)",
    "    print('Part 4: Scalability and Security Demo')",
    "    print('=' * 60)",
    "    ",
    "    so = StackOverflow()",
    "    ",
    "    # Test 1: Input validation",
    "    print('\\n--- Test 1: Input Validation ---')",
    "    bad_user = so.create_user('u1', 'ab')  # Too short",
    "    print(f'Bad username rejected: {bad_user is None}')",
    "    good_user = so.create_user('u1', 'alice')",
    "    print(f'Good username accepted: {good_user is not None}')",
    "    ",
    "    # Test 2: XSS prevention",
    "    print('\\n--- Test 2: XSS Prevention ---')",
    "    so.create_user('u2', 'bob')",
    "    q = so.post_question('u1', 'Safe Title Here', 'This is a safe body text for testing XSS', ['security'])",
    "    print(f'Question created: {q is not None}')",
    "    ",
    "    # Test 3: Spam detection",
    "    print('\\n--- Test 3: Spam Detection ---')",
    "    spam_q = so.post_question('u1', 'Buy Now Free Money!!!', 'Click here for casino free money buy now', ['spam'])",
    "    print(f'Spam rejected: {spam_q is None}')",
    "    ",
    "    # Test 4: Rate limiting",
    "    print('\\n--- Test 4: Rate Limiting ---')",
    "    so.rate_limiter.limits['post_question'] = (2, 3600)  # 2 per hour for testing",
    "    q2 = so.post_question('u1', 'Second Question', 'This is another question body text here', ['test'])",
    "    q3 = so.post_question('u1', 'Third Question', 'This question should be rate limited now', ['test'])",
    "    print(f'Q2 allowed: {q2 is not None}, Q3 rate limited: {q3 is None}')",
    "    ",
    "    # Test 5: Caching",
    "    print('\\n--- Test 5: Caching Performance ---')",
    "    _ = so.get_question('q_1')  # Cache miss",
    "    _ = so.get_question('q_1')  # Cache hit",
    "    _ = so.get_question('q_1')  # Cache hit",
    "    stats = so.get_cache_stats()",
    "    print(f'Cache stats: {stats}')",
    "    ",
    "    print('\\n' + '=' * 60)",
    "    print('ARCHITECTURE DISCUSSION SUMMARY')",
    "    print('=' * 60)",
    "    print('''",
    "SCALABILITY:",
    "1. Sharding: By question_id hash for content, separate user DB",
    "2. Caching: Redis for hot questions, search results (TTL-based)",
    "3. Read Replicas: Route reads to replicas, writes to primary",
    "4. Search: Elasticsearch for full-text (replace in-memory index)",
    "",
    "SECURITY:",
    "1. Rate Limiting: Token bucket per user+action in Redis",
    "2. Authentication: JWT with short expiry, refresh rotation",
    "3. Authorization: Ownership checks, RBAC for moderation",
    "4. Input Validation: HTML escape, length limits, regex patterns",
    "5. Spam Detection: Pattern matching + ML model (Akismet/custom)",
    "''')",
    "    print('All Part 4 demonstrations complete!')"
  ],
  "solution_java_lines": [
    "import java.util.*;",
    "import java.util.regex.*;",
    "import java.security.MessageDigest;",
    "import java.util.stream.*;",
    "",
    "public class StackOverflow {",
    "    ",
    "    private static final int REP_UPVOTE = 10;",
    "    private static final int REP_DOWNVOTE = -2;",
    "    private static final int REP_ACCEPTED = 15;",
    "    ",
    "    // ============ Production Infrastructure ============",
    "    ",
    "    static class RateLimiter {",
    "        private Map<String, double[]> buckets = new HashMap<>(); // [tokens, lastTime]",
    "        private Map<String, int[]> limits = new HashMap<>(); // [maxTokens, refillSecs]",
    "        ",
    "        RateLimiter() {",
    "            limits.put(\"post_question\", new int[]{10, 3600});",
    "            limits.put(\"post_answer\", new int[]{30, 3600});",
    "            limits.put(\"vote\", new int[]{30, 86400});",
    "            limits.put(\"search\", new int[]{100, 60});",
    "        }",
    "        ",
    "        boolean check(String userId, String action) {",
    "            if (!limits.containsKey(action)) return true;",
    "            int[] limit = limits.get(action);",
    "            String key = userId + \":\" + action;",
    "            double now = System.currentTimeMillis() / 1000.0;",
    "            buckets.putIfAbsent(key, new double[]{limit[0], now});",
    "            double[] bucket = buckets.get(key);",
    "            double elapsed = now - bucket[1];",
    "            bucket[0] = Math.min(limit[0], bucket[0] + elapsed * limit[0] / limit[1]);",
    "            bucket[1] = now;",
    "            if (bucket[0] >= 1) { bucket[0]--; return true; }",
    "            return false;",
    "        }",
    "        ",
    "        void setLimit(String action, int max, int secs) { limits.put(action, new int[]{max, secs}); }",
    "    }",
    "    ",
    "    static class Cache {",
    "        private Map<String, Object[]> store = new HashMap<>(); // [value, expiresAt]",
    "        private int defaultTtl = 300;",
    "        private int hits = 0, misses = 0;",
    "        ",
    "        @SuppressWarnings(\"unchecked\")",
    "        <T> T get(String key) {",
    "            if (store.containsKey(key)) {",
    "                Object[] entry = store.get(key);",
    "                if (System.currentTimeMillis() < (long)entry[1]) { hits++; return (T)entry[0]; }",
    "                store.remove(key);",
    "            }",
    "            misses++;",
    "            return null;",
    "        }",
    "        void set(String key, Object value, int ttl) {",
    "            store.put(key, new Object[]{value, System.currentTimeMillis() + (ttl > 0 ? ttl : defaultTtl) * 1000L});",
    "        }",
    "        void invalidate(String key) { store.remove(key); }",
    "        String stats() { int t = hits + misses; return String.format(\"hits=%d, misses=%d, rate=%.2f\", hits, misses, t > 0 ? (double)hits/t : 0); }",
    "    }",
    "    ",
    "    static class InputValidator {",
    "        static String sanitize(String s) { return s.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace(\"\\\"\", \"&quot;\"); }",
    "        static boolean validUsername(String u) { return u.matches(\"[a-zA-Z0-9_]{3,20}\"); }",
    "        static boolean validQuestion(String title, String body) { return title.length() >= 10 && body.length() >= 30; }",
    "    }",
    "    ",
    "    static class SpamDetector {",
    "        static boolean isSpam(String text) {",
    "            String lower = text.toLowerCase();",
    "            return lower.contains(\"buy now\") || lower.contains(\"free money\") || lower.contains(\"casino\");",
    "        }",
    "    }",
    "    ",
    "    // ============ Data Models ============",
    "    ",
    "    static class User {",
    "        String id, username; int reputation = 0;",
    "        User(String id, String username) { this.id = id; this.username = username; }",
    "    }",
    "    static class Question {",
    "        String id, authorId, title, body; List<String> tags; int score = 0; long createdAt;",
    "        Question(String id, String authorId, String title, String body, List<String> tags) {",
    "            this.id = id; this.authorId = authorId; this.title = title;",
    "            this.body = body; this.tags = new ArrayList<>(tags); this.createdAt = System.currentTimeMillis();",
    "        }",
    "    }",
    "    static class Answer {",
    "        String id, questionId, authorId, body; int score = 0; boolean isAccepted = false; long createdAt;",
    "        Answer(String id, String questionId, String authorId, String body) {",
    "            this.id = id; this.questionId = questionId; this.authorId = authorId;",
    "            this.body = body; this.createdAt = System.currentTimeMillis();",
    "        }",
    "    }",
    "    ",
    "    // ============ Core Storage ============",
    "    ",
    "    private Map<String, User> users = new HashMap<>();",
    "    private Map<String, Question> questions = new HashMap<>();",
    "    private Map<String, Answer> answers = new HashMap<>();",
    "    private Map<String, List<String>> userQuestions = new HashMap<>();",
    "    private Map<String, List<String>> tagQuestions = new HashMap<>();",
    "    private Map<String, List<String>> questionAnswers = new HashMap<>();",
    "    private Map<String, Map<String, Integer>> questionVotes = new HashMap<>();",
    "    private Map<String, Map<String, Integer>> answerVotes = new HashMap<>();",
    "    private Map<String, Set<String>> wordIndex = new HashMap<>();",
    "    private int questionCounter = 1, answerCounter = 1;",
    "    // NEW: Production infrastructure",
    "    private Cache cache = new Cache();",
    "    private RateLimiter rateLimiter = new RateLimiter();",
    "    ",
    "    private Set<String> tokenize(String text) {",
    "        Set<String> words = new HashSet<>();",
    "        Matcher m = Pattern.compile(\"[a-zA-Z0-9]+\").matcher(text.toLowerCase());",
    "        while (m.find()) words.add(m.group());",
    "        return words;",
    "    }",
    "    ",
    "    public User createUser(String userId, String username) {",
    "        if (users.containsKey(userId)) return users.get(userId);",
    "        if (!InputValidator.validUsername(username)) return null;",
    "        User user = new User(userId, InputValidator.sanitize(username));",
    "        users.put(userId, user);",
    "        return user;",
    "    }",
    "    ",
    "    public Question postQuestion(String userId, String title, String body, List<String> tags) {",
    "        if (!users.containsKey(userId)) return null;",
    "        if (!rateLimiter.check(userId, \"post_question\")) return null;",
    "        if (!InputValidator.validQuestion(title, body)) return null;",
    "        if (SpamDetector.isSpam(title + \" \" + body)) return null;",
    "        ",
    "        String qId = \"q_\" + questionCounter++;",
    "        Question q = new Question(qId, userId, InputValidator.sanitize(title), InputValidator.sanitize(body), tags);",
    "        questions.put(qId, q);",
    "        userQuestions.computeIfAbsent(userId, k -> new ArrayList<>()).add(qId);",
    "        for (String tag : tags) tagQuestions.computeIfAbsent(tag, k -> new ArrayList<>()).add(qId);",
    "        for (String word : tokenize(title + \" \" + body)) wordIndex.computeIfAbsent(word, k -> new HashSet<>()).add(qId);",
    "        cache.invalidate(\"top_questions\");",
    "        return q;",
    "    }",
    "    ",
    "    public Answer postAnswer(String userId, String questionId, String body) {",
    "        if (!questions.containsKey(questionId) || !users.containsKey(userId)) return null;",
    "        if (!rateLimiter.check(userId, \"post_answer\")) return null;",
    "        if (SpamDetector.isSpam(body)) return null;",
    "        String aId = \"a_\" + answerCounter++;",
    "        Answer a = new Answer(aId, questionId, userId, InputValidator.sanitize(body));",
    "        answers.put(aId, a);",
    "        questionAnswers.computeIfAbsent(questionId, k -> new ArrayList<>()).add(aId);",
    "        cache.invalidate(\"question:\" + questionId);",
    "        return a;",
    "    }",
    "    ",
    "    public Question getQuestion(String questionId) {",
    "        Question cached = cache.get(\"question:\" + questionId);",
    "        if (cached != null) return cached;",
    "        Question q = questions.get(questionId);",
    "        if (q != null) cache.set(\"question:\" + questionId, q, 300);",
    "        return q;",
    "    }",
    "    ",
    "    public List<Answer> getAnswersForQuestion(String questionId) {",
    "        List<String> ids = questionAnswers.getOrDefault(questionId, new ArrayList<>());",
    "        return ids.stream().map(answers::get).collect(Collectors.toList());",
    "    }",
    "    ",
    "    public List<Question> getQuestionsByUser(String userId) {",
    "        return userQuestions.getOrDefault(userId, new ArrayList<>()).stream()",
    "            .map(questions::get).sorted((a, b) -> Long.compare(b.createdAt, a.createdAt)).collect(Collectors.toList());",
    "    }",
    "    ",
    "    public List<Question> getQuestionsByTag(String tag) {",
    "        return tagQuestions.getOrDefault(tag, new ArrayList<>()).stream()",
    "            .map(questions::get).sorted((a, b) -> Integer.compare(b.score, a.score)).collect(Collectors.toList());",
    "    }",
    "    ",
    "    private int applyVote(String voterId, String itemId, int newVote,",
    "                          Map<String, Map<String, Integer>> votesMap, Map<String, ?> itemsMap, boolean isQuestion) {",
    "        if (!itemsMap.containsKey(itemId) || !users.containsKey(voterId)) return -1;",
    "        if (!rateLimiter.check(voterId, \"vote\")) return -1;",
    "        String authorId = isQuestion ? ((Question)itemsMap.get(itemId)).authorId : ((Answer)itemsMap.get(itemId)).authorId;",
    "        if (voterId.equals(authorId)) return -1;",
    "        Map<String, Integer> votes = votesMap.computeIfAbsent(itemId, k -> new HashMap<>());",
    "        int prevVote = votes.getOrDefault(voterId, 0);",
    "        if (prevVote == newVote) return isQuestion ? ((Question)itemsMap.get(itemId)).score : ((Answer)itemsMap.get(itemId)).score;",
    "        int delta = newVote - prevVote;",
    "        if (isQuestion) ((Question)itemsMap.get(itemId)).score += delta;",
    "        else ((Answer)itemsMap.get(itemId)).score += delta;",
    "        User author = users.get(authorId);",
    "        if (prevVote == 1) author.reputation -= REP_UPVOTE;",
    "        else if (prevVote == -1) author.reputation -= REP_DOWNVOTE;",
    "        if (newVote == 1) author.reputation += REP_UPVOTE;",
    "        else if (newVote == -1) author.reputation += REP_DOWNVOTE;",
    "        votes.put(voterId, newVote);",
    "        return isQuestion ? ((Question)itemsMap.get(itemId)).score : ((Answer)itemsMap.get(itemId)).score;",
    "    }",
    "    ",
    "    public int upvoteQuestion(String voterId, String qId) {",
    "        int result = applyVote(voterId, qId, 1, questionVotes, questions, true);",
    "        if (result >= 0) { cache.invalidate(\"question:\" + qId); cache.invalidate(\"top_questions\"); }",
    "        return result;",
    "    }",
    "    public int downvoteQuestion(String voterId, String qId) { return applyVote(voterId, qId, -1, questionVotes, questions, true); }",
    "    public int upvoteAnswer(String voterId, String aId) { return applyVote(voterId, aId, 1, answerVotes, answers, false); }",
    "    public int downvoteAnswer(String voterId, String aId) { return applyVote(voterId, aId, -1, answerVotes, answers, false); }",
    "    ",
    "    public void acceptAnswer(String questionAuthorId, String answerId) {",
    "        if (!answers.containsKey(answerId)) return;",
    "        Answer answer = answers.get(answerId);",
    "        Question question = questions.get(answer.questionId);",
    "        if (question == null || !question.authorId.equals(questionAuthorId) || answer.isAccepted) return;",
    "        answer.isAccepted = true;",
    "        users.get(answer.authorId).reputation += REP_ACCEPTED;",
    "    }",
    "    ",
    "    public int getUserReputation(String userId) { return users.containsKey(userId) ? users.get(userId).reputation : 0; }",
    "    ",
    "    @SuppressWarnings(\"unchecked\")",
    "    public List<Question> searchQuestions(String query) {",
    "        if (!rateLimiter.check(\"anon\", \"search\")) return new ArrayList<>();",
    "        Set<String> queryWords = tokenize(query);",
    "        if (queryWords.isEmpty()) return new ArrayList<>();",
    "        Map<String, Integer> matchCounts = new HashMap<>();",
    "        for (String qw : queryWords) {",
    "            Set<String> matched = new HashSet<>();",
    "            for (var e : wordIndex.entrySet()) if (e.getKey().contains(qw)) matched.addAll(e.getValue());",
    "            for (String qId : matched) matchCounts.merge(qId, 1, Integer::sum);",
    "        }",
    "        List<Question> results = matchCounts.keySet().stream().map(questions::get).collect(Collectors.toList());",
    "        results.sort((a, b) -> { int c = Integer.compare(matchCounts.get(b.id), matchCounts.get(a.id)); return c != 0 ? c : Integer.compare(b.score, a.score); });",
    "        return results;",
    "    }",
    "    ",
    "    public List<Question> searchByTags(List<String> tags, boolean matchAll) {",
    "        if (tags.isEmpty()) return new ArrayList<>();",
    "        List<Set<String>> tagSets = tags.stream().map(t -> new HashSet<>(tagQuestions.getOrDefault(t, new ArrayList<>()))).collect(Collectors.toList());",
    "        Set<String> resultIds = matchAll ? new HashSet<>(tagSets.get(0)) : new HashSet<>();",
    "        for (int i = matchAll ? 1 : 0; i < tagSets.size(); i++) {",
    "            if (matchAll) resultIds.retainAll(tagSets.get(i));",
    "            else resultIds.addAll(tagSets.get(i));",
    "        }",
    "        return resultIds.stream().map(questions::get).sorted((a, b) -> Integer.compare(b.score, a.score)).collect(Collectors.toList());",
    "    }",
    "    ",
    "    @SuppressWarnings(\"unchecked\")",
    "    public List<Question> getTopQuestions(int limit) {",
    "        List<Question> cached = cache.get(\"top_questions\");",
    "        if (cached != null) return cached.subList(0, Math.min(limit, cached.size()));",
    "        List<Question> all = new ArrayList<>(questions.values());",
    "        all.sort((a, b) -> b.score != a.score ? Integer.compare(b.score, a.score) : Long.compare(b.createdAt, a.createdAt));",
    "        cache.set(\"top_questions\", all.subList(0, Math.min(100, all.size())), 60);",
    "        return all.subList(0, Math.min(limit, all.size()));",
    "    }",
    "    ",
    "    public List<Question> getRecentQuestions(int limit) {",
    "        List<Question> all = new ArrayList<>(questions.values());",
    "        all.sort((a, b) -> Long.compare(b.createdAt, a.createdAt));",
    "        return all.subList(0, Math.min(limit, all.size()));",
    "    }",
    "    ",
    "    public List<Question> getUnansweredQuestions() {",
    "        return questions.values().stream()",
    "            .filter(q -> questionAnswers.getOrDefault(q.id, new ArrayList<>()).isEmpty())",
    "            .sorted((a, b) -> Long.compare(b.createdAt, a.createdAt)).collect(Collectors.toList());",
    "    }",
    "    ",
    "    public String getCacheStats() { return cache.stats(); }",
    "    ",
    "    public static void main(String[] args) {",
    "        System.out.println(\"=\".repeat(60));",
    "        System.out.println(\"Part 4: Scalability and Security Demo\");",
    "        System.out.println(\"=\".repeat(60));",
    "        ",
    "        StackOverflow so = new StackOverflow();",
    "        ",
    "        System.out.println(\"\\n--- Test 1: Input Validation ---\");",
    "        User badUser = so.createUser(\"u1\", \"ab\");",
    "        System.out.println(\"Bad username rejected: \" + (badUser == null));",
    "        User goodUser = so.createUser(\"u1\", \"alice\");",
    "        System.out.println(\"Good username accepted: \" + (goodUser != null));",
    "        ",
    "        System.out.println(\"\\n--- Test 2: Spam Detection ---\");",
    "        so.createUser(\"u2\", \"bob\");",
    "        Question q1 = so.postQuestion(\"u1\", \"Valid Title Here\", \"This is valid body text for testing purposes\", Arrays.asList(\"java\"));",
    "        Question spam = so.postQuestion(\"u1\", \"Buy Now Free Money\", \"Casino free money buy now click here\", Arrays.asList(\"spam\"));",
    "        System.out.println(\"Valid question: \" + (q1 != null) + \", Spam rejected: \" + (spam == null));",
    "        ",
    "        System.out.println(\"\\n--- Test 3: Rate Limiting ---\");",
    "        so.rateLimiter.setLimit(\"post_question\", 2, 3600);",
    "        Question q2 = so.postQuestion(\"u1\", \"Second Question\", \"Another valid question body text here\", Arrays.asList(\"test\"));",
    "        Question q3 = so.postQuestion(\"u1\", \"Third Question\", \"This should be rate limited now\", Arrays.asList(\"test\"));",
    "        System.out.println(\"Q2 allowed: \" + (q2 != null) + \", Q3 rate limited: \" + (q3 == null));",
    "        ",
    "        System.out.println(\"\\n--- Test 4: Caching ---\");",
    "        so.getQuestion(\"q_1\");",
    "        so.getQuestion(\"q_1\");",
    "        so.getQuestion(\"q_1\");",
    "        System.out.println(\"Cache stats: \" + so.getCacheStats());",
    "        ",
    "        System.out.println(\"\\n\" + \"=\".repeat(60));",
    "        System.out.println(\"PRODUCTION ARCHITECTURE SUMMARY\");",
    "        System.out.println(\"Sharding: By question_id for content, separate user DB\");",
    "        System.out.println(\"Caching: Redis with TTL for hot data\");",
    "        System.out.println(\"Search: Elasticsearch for full-text at scale\");",
    "        System.out.println(\"Security: Rate limiting + Input validation + Spam detection\");",
    "        System.out.println(\"=\".repeat(60));",
    "    }",
    "}"
  ],
  "code_walkthrough": [
    {
      "lines": "1-15",
      "explanation": "Imports and infrastructure class declarations for rate limiting and caching"
    },
    {
      "lines": "16-50",
      "explanation": "RateLimiter: Token bucket implementation with configurable limits per action type"
    },
    {
      "lines": "51-75",
      "explanation": "Cache: Simple TTL-based cache with hit/miss tracking for metrics"
    },
    {
      "lines": "76-95",
      "explanation": "InputValidator: Sanitization (XSS prevention), username validation, question validation"
    },
    {
      "lines": "96-105",
      "explanation": "SpamDetector: Basic pattern matching for known spam phrases"
    },
    {
      "lines": "106-135",
      "explanation": "Core data models (unchanged from Part 3)"
    },
    {
      "lines": "136-160",
      "explanation": "StackOverflow class initialization with new infrastructure components"
    },
    {
      "lines": "161-200",
      "explanation": "Part 1 methods enhanced with validation, rate limiting, and cache invalidation"
    },
    {
      "lines": "201-250",
      "explanation": "Part 2 voting methods with rate limiting on votes"
    },
    {
      "lines": "251-310",
      "explanation": "Part 3 search methods with caching for search results"
    },
    {
      "lines": "311-350",
      "explanation": "Demo code showing all production patterns in action"
    }
  ],
  "complexity_analysis": {
    "time": {
      "new_methods": {
        "rate_limit_check": {
          "complexity": "O(1)",
          "explanation": "Single hash lookup and arithmetic"
        },
        "cache_get/set": {
          "complexity": "O(1)",
          "explanation": "Hash table operations"
        },
        "input_validation": {
          "complexity": "O(n)",
          "explanation": "Linear scan of input string"
        },
        "spam_detection": {
          "complexity": "O(n*p)",
          "explanation": "n = text length, p = number of patterns"
        }
      },
      "overall_change": "Adds constant overhead to each operation for security checks. Cache hits significantly reduce read latency."
    },
    "space": {
      "additional_space": "O(U*A + C) where U=users, A=actions for rate limits, C=cached items",
      "explanation": "Rate limiter stores per-user-per-action buckets, cache stores recent results"
    }
  },
  "dry_run": {
    "example_input": "Rate limiting: User posts 3 questions with limit of 2",
    "steps": [
      {
        "step": 1,
        "action": "postQuestion('u1', 'Q1', ...)",
        "state": "bucket tokens=10, used 1, remaining=9",
        "explanation": "First question allowed, token consumed"
      },
      {
        "step": 2,
        "action": "postQuestion('u1', 'Q2', ...)",
        "state": "bucket tokens=9, used 1, remaining=8",
        "explanation": "Second question allowed"
      },
      {
        "step": 3,
        "action": "setLimit('post_question', 2, 3600)",
        "state": "limit changed to 2 per hour",
        "explanation": "Reduce limit for testing"
      },
      {
        "step": 4,
        "action": "postQuestion('u1', 'Q3', ...)",
        "state": "bucket tokens=8 > max(2), capped to 2, used 1, remaining=1",
        "explanation": "Bucket capped, allowed"
      },
      {
        "step": 5,
        "action": "postQuestion('u1', 'Q4', ...)",
        "state": "tokens=1, used 1, remaining=0",
        "explanation": "Last token used"
      },
      {
        "step": 6,
        "action": "postQuestion('u1', 'Q5', ...)",
        "state": "tokens=0 < 1 required",
        "explanation": "Rate limited, returns null"
      }
    ],
    "final_output": "Questions Q1-Q4 created, Q5 rejected due to rate limit"
  },
  "debugging_playbook": {
    "fast_sanity_checks": [
      "Post one question, verify it works",
      "Get a question, check cache hit on second call"
    ],
    "likely_bugs": [
      "Rate limit bucket not refilling over time",
      "Cache TTL not being checked",
      "Spam patterns not matching case-insensitively"
    ],
    "recommended_logs_or_asserts": [
      "Log rate limit decisions with remaining tokens",
      "Log cache hit/miss for each key",
      "Assert user exists before any mutation"
    ],
    "how_to_localize": "Add timestamps to rate limit logs, verify token calculation. For cache issues, force TTL=0 to disable and compare behavior."
  },
  "edge_cases": [
    {
      "case": "Rate limit exactly at boundary",
      "handling": "Use >= 1 check, not > 0, to ensure atomic decrement",
      "gotcha": "Floating point precision in token calculation"
    },
    {
      "case": "Cache TTL of 0",
      "handling": "Treat as 'no cache' and always fetch fresh",
      "gotcha": "Don't cache with TTL=0 or it expires immediately"
    },
    {
      "case": "XSS in all fields",
      "handling": "Sanitize username, title, body - all user inputs",
      "gotcha": "Don't forget to sanitize tags if user-provided"
    },
    {
      "case": "Empty spam patterns",
      "handling": "Return false (not spam) when no patterns defined",
      "gotcha": "Avoid regex compilation on every call"
    }
  ],
  "test_cases": [
    {
      "name": "Input Validation",
      "input": "createUser('u1', 'ab')",
      "expected": "null (username too short)",
      "explanation": "Username must be 3-20 alphanumeric characters"
    },
    {
      "name": "XSS Prevention",
      "input": "postQuestion('u1', '<script>alert(1)</script>', ...)",
      "expected": "Title stored as '&lt;script&gt;alert(1)&lt;/script&gt;'",
      "explanation": "HTML entities escaped to prevent XSS"
    },
    {
      "name": "Spam Detection",
      "input": "postQuestion('u1', 'Buy Now', 'Free money casino...')",
      "expected": "null (spam detected)",
      "explanation": "Known spam phrases trigger rejection"
    },
    {
      "name": "Cache Hit",
      "input": "getQuestion('q_1') called twice",
      "expected": "First call: cache miss, Second call: cache hit",
      "explanation": "Result cached after first fetch"
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Not invalidating cache on writes",
      "why_wrong": "Stale data served after updates",
      "correct_approach": "Invalidate affected cache keys on every write",
      "code_example_wrong": "// Just update DB, forget cache",
      "code_example_correct": "self.cache.invalidate(f'question:{q_id}')"
    },
    {
      "mistake": "Per-server rate limiting",
      "why_wrong": "User can bypass by hitting different servers",
      "correct_approach": "Use shared state (Redis) for distributed rate limiting",
      "code_example_wrong": "// Local dict for rate limits",
      "code_example_correct": "# Redis: INCR user:action:count EX 3600"
    },
    {
      "mistake": "Blocking on external services",
      "why_wrong": "Cache miss blocks entire request",
      "correct_approach": "Async operations, circuit breakers, fallbacks",
      "code_example_wrong": "result = slow_db_query()",
      "code_example_correct": "# Use timeout, fallback to stale cache on failure"
    }
  ],
  "interview_tips": {
    "how_to_present": "Start with the big picture: 'Stack Overflow is read-heavy, so I'd focus on caching and read replicas first.' Then dive into specifics for each concern.",
    "what_to_mention": [
      "Read/write ratio drives architecture decisions",
      "CAP theorem tradeoffs for eventual consistency",
      "Defense in depth for security (multiple layers)",
      "Metrics and monitoring in production"
    ],
    "time_allocation": "Spend 2-3 min on each topic (sharding, caching, rate limiting, auth, validation). Don't go too deep on any one area.",
    "if_stuck": [
      "Think about what data is accessed most frequently",
      "Consider what happens when a server crashes",
      "Ask: what are the attack vectors?"
    ]
  },
  "connection_to_next_part": "Part 4 establishes the production architecture patterns. If there were a Part 5, it might focus on implementing specific features like real-time notifications (WebSockets), reputation badges, or moderation queues.",
  "communication_script": {
    "transition_from_previous": "So Part 3 gave us a working search. Now for Part 4, I want to discuss how we'd make this production-ready at scale.",
    "explaining_changes": "The key additions are: 1) Rate limiting to prevent abuse, 2) Caching for performance, 3) Input validation for security. Let me walk through each...",
    "while_extending_code": [
      "I'm adding a RateLimiter class to track tokens per user...",
      "The Cache class wraps our lookups with TTL-based expiry...",
      "InputValidator sanitizes all user-provided strings..."
    ],
    "after_completing": "This covers the main production concerns. In a real system, we'd also add logging, metrics, and circuit breakers. Any questions about specific aspects?"
  },
  "time_milestones": {
    "time_budget": "10-15 minutes for discussion",
    "by_2_min": "Outline the three main areas: scalability (sharding, caching), search (Elasticsearch), security (rate limiting, validation)",
    "by_5_min": "Deep dive on database sharding strategy and caching approach",
    "by_10_min": "Cover security topics: rate limiting, auth, input validation, spam detection",
    "warning_signs": "If spending more than 3 min on any single topic, move on. Breadth > depth for discussion questions."
  },
  "recovery_strategies": {
    "if_part_builds_wrong": "Part 4 is mostly discussion, so focus on concepts even if code isn't perfect. Say: 'The implementation details would vary, but the key idea is...'",
    "if_new_requirement_unclear": "Ask: 'Are we optimizing for writes (like posting) or reads (like searching)?' This clarifies which strategies to prioritize.",
    "if_running_behind": "Focus on the highest-impact topics: caching for reads, rate limiting for security. Skip advanced topics like sharding."
  },
  "signal_points": {
    "wow_factors_for_followup": [
      "Mentioning specific numbers: '100:1 read/write ratio suggests aggressive caching'",
      "Discussing tradeoffs: 'We could use eventual consistency for votes, but need strong consistency for money/reputation'",
      "Real-world experience: 'At scale, I'd use Elasticsearch because the inverted index doesn't scale horizontally'",
      "Security depth: 'Rate limiting per IP handles anonymous users, but per user_id handles authenticated abuse'"
    ]
  },
  "pattern_recognition": {
    "pattern": "Layered Architecture with External Services",
    "indicators": [
      "Need for horizontal scaling",
      "Read-heavy workload",
      "Full-text search requirement",
      "Security requirements"
    ],
    "similar_problems": [
      "Design Twitter",
      "Design URL Shortener",
      "Design Instagram"
    ],
    "template": "Load Balancer \u2192 API Gateway (auth, rate limit) \u2192 Service Layer \u2192 Cache \u2192 Database/Search"
  },
  "thinking_process": [
    {
      "step": 1,
      "thought": "What's the access pattern?",
      "why": "Stack Overflow is 100x more reads than writes. This drives everything toward caching."
    },
    {
      "step": 2,
      "thought": "What's the scaling bottleneck?",
      "why": "Search and hot question reads. Elasticsearch handles search, Redis handles caching."
    },
    {
      "step": 3,
      "thought": "What are the attack vectors?",
      "why": "Spam, vote manipulation, XSS. Need multiple defense layers."
    }
  ],
  "interviewer_perspective": {
    "what_they_evaluate": [
      "Do you understand production systems?",
      "Can you reason about scale?",
      "Do you think about security proactively?"
    ],
    "bonus_points": [
      "Specific numbers and metrics",
      "Real-world trade-off discussions",
      "Mentioning monitoring and observability"
    ],
    "red_flags": [
      "Only theoretical knowledge, no practical considerations",
      "Ignoring security entirely",
      "Over-engineering for small scale"
    ]
  },
  "ai_copilot_tips": {
    "what_to_do": [
      "Use AI to recall specific technology names (Elasticsearch vs Solr)",
      "Get help with rate limiting algorithm details"
    ],
    "what_not_to_do": [
      "Don't let AI generate vague 'use microservices' answers",
      "Ensure you can explain any technology you mention"
    ]
  },
  "red_flags_to_avoid": {
    "behavioral": [
      "Jumping to solutions without discussing requirements",
      "Not asking about scale expectations"
    ],
    "technical": [
      "Proposing solutions that don't scale (single server)",
      "Ignoring data consistency concerns"
    ],
    "communication": [
      "Using jargon without explanation",
      "Not acknowledging trade-offs"
    ]
  },
  "final_checklist": {
    "before_saying_done": [
      "Covered sharding strategy?",
      "Explained caching approach with invalidation?",
      "Addressed rate limiting?",
      "Mentioned input validation and XSS prevention?",
      "Discussed authentication/authorization?"
    ],
    "quick_code_review": [
      "Rate limiter has configurable limits",
      "Cache has TTL and invalidation",
      "Input validation happens before processing",
      "All user inputs are sanitized"
    ]
  },
  "production_considerations": {
    "what_i_would_add": [
      "Structured logging with request IDs",
      "Metrics (Prometheus) for cache hit rates, latency percentiles",
      "Circuit breakers for external service calls",
      "Health check endpoints",
      "Graceful degradation when cache is unavailable"
    ],
    "why_not_in_interview": "Focus on core architecture; these are implementation details",
    "how_to_mention": "Say: 'In production, I'd add observability: logging, metrics, tracing. Let me know if you want me to elaborate.'"
  },
  "generated_at": "2026-01-19T05:16:01.877813",
  "_meta": {
    "problem_id": "stackoverflow_api",
    "part_number": 4,
    "model": "claude-opus-4-5-20251101"
  }
}