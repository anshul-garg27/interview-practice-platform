{
  "problem_title": "News Feed Aggregator System",
  "difficulty": "medium",
  "category": "HLD/System Design",
  "estimated_time": "45-60 minutes",
  "problem_analysis": {
    "first_impressions": "This is a **classic system design problem** combining real-time data ingestion, personalization, and high-scale read optimization. It's essentially a simplified version of Google News/Apple News. The 1700:1 read/write ratio immediately signals this is **read-heavy** - meaning caching and pre-computation are critical. The 100K QPS read requirement with <200ms p99 latency means we cannot compute feeds on-the-fly for every request.",
    "pattern_recognition": "**Feed Generation (Fan-out)** + **Caching (Multi-tier)** + **Ranking/Scoring** + **Time-series Data** + **Pub-Sub for Ingestion**. Similar to Twitter's timeline architecture (LC 355) but with content categorization and personalization layers.",
    "key_constraints": [
      "**100K QPS reads** - Cache aggressively, pre-compute where possible",
      "**<200ms p99 latency** - Cannot do expensive joins or computations at read time",
      "**5M articles/day** - Moderate write load, but need deduplication",
      "**10M users, 1K publishers** - Manageable cardinality for caching strategies",
      "**Feed freshness <5 minutes** - Cache TTL and invalidation strategy needed",
      "**1700:1 read/write ratio** - Optimize for reads, tolerate slightly slower writes"
    ],
    "clarifying_questions": [
      "**Feed personalization depth?** - Simple (followed publishers) or ML-based recommendations? This affects architecture complexity significantly.",
      "**Deduplication criteria?** - Same URL? Similar title? This affects whether we need NLP/ML pipeline.",
      "**Geographic distribution?** - Single region or global? Affects CDN and replication strategy.",
      "**Offline support?** - Should we pre-generate feeds for all active users or on-demand?",
      "**Breaking news handling?** - Should viral/breaking news bypass normal ranking?",
      "**Cold start strategy?** - What do new users with no preferences see?",
      "**Article retention?** - How long do we keep articles? Affects storage and indexing."
    ],
    "edge_cases_to_consider": [
      "New user with no follows/interests (cold start)",
      "Publisher publishes 1000 articles at once (flood protection)",
      "Viral article causing 10x traffic spike",
      "User follows all 1000 publishers (degenerate case)",
      "Same story from multiple publishers (deduplication)",
      "Publisher goes offline or changes RSS format",
      "User's interests change frequently (cache invalidation)"
    ]
  },
  "requirements_coverage": {
    "checklist": [
      {
        "requirement": "registerPublisher()",
        "how_met": "Store publisher metadata in DB, validate RSS connectivity, add to crawler schedule",
        "gotchas": [
          "RSS validation can timeout - use async",
          "Need rate limiting per publisher"
        ]
      },
      {
        "requirement": "fetchArticles()",
        "how_met": "Distributed crawler workers poll RSS feeds, deduplicate, store in DB",
        "gotchas": [
          "Deduplication needs content hashing",
          "Handle publisher rate limits"
        ]
      },
      {
        "requirement": "getUserFeed() < 200ms",
        "how_met": "Multi-tier cache: user feed cache \u2192 category cache \u2192 hot articles cache \u2192 DB fallback",
        "gotchas": [
          "Cache invalidation on new articles",
          "Pagination cursor management"
        ]
      },
      {
        "requirement": "followPublisher()",
        "how_met": "Update user's followed set, invalidate user's feed cache",
        "gotchas": [
          "Invalidate cache or merge incrementally?"
        ]
      },
      {
        "requirement": "setUserInterests()",
        "how_met": "Update user's interest categories, invalidate user's feed cache",
        "gotchas": [
          "Category changes affect ranking weights"
        ]
      },
      {
        "requirement": "likeArticle()",
        "how_met": "Record engagement, update user's publisher affinity scores for personalization",
        "gotchas": [
          "Batch engagement updates, don't write on every like"
        ]
      }
    ],
    "complexity_targets": [
      {
        "operation": "getUserFeed()",
        "target": "O(1) cache hit, O(k log n) cache miss",
        "achieved": "O(1) with cache",
        "why": "Pre-computed feeds for active users, category-based caching"
      },
      {
        "operation": "registerPublisher()",
        "target": "O(1)",
        "achieved": "O(1)",
        "why": "Simple DB insert"
      },
      {
        "operation": "followPublisher()",
        "target": "O(1)",
        "achieved": "O(1)",
        "why": "Set addition + cache invalidation"
      },
      {
        "operation": "likeArticle()",
        "target": "O(1)",
        "achieved": "O(1)",
        "why": "Async engagement tracking"
      }
    ],
    "non_goals": [
      "Full-text search (Elasticsearch add-on, not core requirement)",
      "Real-time notifications (Part 2 follow-up)",
      "ML-based recommendations (Part 3 follow-up)",
      "Comment/social features (out of scope)",
      "Article content storage (store metadata + link only)"
    ]
  },
  "assumptions": [
    "Articles are identified by URL - same URL = same article (for deduplication)",
    "Users have at most 50 followed publishers and 10 interest categories (stated constraint)",
    "Feed shows articles from last 24-48 hours primarily (recency bias)",
    "We store article metadata, not full content (link to publisher's site)",
    "Single datacenter for Part 1 (global distribution is follow-up)",
    "Eventual consistency is acceptable for feed updates (5-minute freshness SLA)"
  ],
  "tradeoffs": [
    {
      "decision": "Feed Generation Strategy",
      "chosen": "Hybrid Push-Pull",
      "why": "Pure pull is too slow at 100K QPS. Pure push wastes storage for inactive users. Hybrid: pre-compute for active users, generate on-demand for others.",
      "alternative": "Pure Push (Twitter's original approach)",
      "when_to_switch": "If we had celebrity publishers with millions of followers, pure push would cause write amplification"
    },
    {
      "decision": "Caching Granularity",
      "chosen": "Multi-tier: Category \u2192 Publisher \u2192 User Feed",
      "why": "Category cache (top 100 tech articles) is shared across millions of users. Amortizes computation.",
      "alternative": "Only cache per-user feeds",
      "when_to_switch": "If personalization is so deep that category caches aren't useful"
    },
    {
      "decision": "Ranking Complexity",
      "chosen": "Simple weighted scoring (publisher_match + category_match + recency + engagement)",
      "why": "Interpretable, fast to compute, easy to tune. ML ranking is Part 3.",
      "alternative": "ML-based ranking model",
      "when_to_switch": "When A/B tests show simple scoring plateau in engagement metrics"
    },
    {
      "decision": "Database Choice",
      "chosen": "PostgreSQL (articles, users) + Redis (caches) + Optional Elasticsearch (search)",
      "why": "PostgreSQL handles our write volume easily (58 QPS). Redis handles read volume. Well-understood stack.",
      "alternative": "Cassandra for time-series articles",
      "when_to_switch": "If article volume grows 100x or we need multi-region writes"
    }
  ],
  "extensibility_and_followups": {
    "design_principles": [
      "**Separate ingestion from serving** - Crawler workers are independent from API servers",
      "**Cache everything shareable** - Category caches benefit all users with that interest",
      "**Scoring is pluggable** - RankingService interface allows swapping simple scoring for ML model",
      "**Event-driven ingestion** - Message queue decouples crawlers from processors"
    ],
    "why_this_design_scales": "The architecture separates concerns cleanly: Ingestion Layer (crawlers + queue) \u2192 Processing Layer (dedup + categorization) \u2192 Storage Layer (DB + cache) \u2192 API Layer (servers). Each layer scales independently. Adding more crawlers doesn't affect API servers. Adding more API servers doesn't affect ingestion. Cache tiers absorb read load.",
    "expected_followup_hooks": [
      "**Part 2 (Real-time notifications)**: Add WebSocket layer on top of API servers, pub-sub for breaking news",
      "**Part 3 (ML recommendations)**: Plug in ML RankingService, add feature store for user embeddings",
      "**Search**: Add Elasticsearch indexing in processing layer",
      "**Global distribution**: Add geo-routing, regional caches, CDC replication"
    ],
    "invariants": [
      "Every article has exactly one publisher_id",
      "User's followed_publishers is a set (no duplicates)",
      "Feed ranking score is deterministic given same inputs",
      "Cache TTL ensures feed freshness within SLA"
    ]
  },
  "visual_explanation": {
    "problem_visualization": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    NEWS FEED AGGREGATOR OVERVIEW                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                     \u2502\n\u2502    PUBLISHERS (1000)              USERS (10M)                       \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n\u2502    \u2502 NYT \u2502 \u2502 BBC \u2502 \u2502 TC  \u2502  \u2500\u2500\u25ba  \u2502User1\u2502 \u2502User2\u2502 \u2502User3\u2502           \u2502\n\u2502    \u2514\u2500\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2518       \u2514\u2500\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2518           \u2502\n\u2502       \u2502       \u2502       \u2502             \u2502       \u2502       \u2502               \u2502\n\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n\u2502               \u2502                             \u2502                       \u2502\n\u2502               \u25bc                             \u2502                       \u2502\n\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502                       \u2502\n\u2502    \u2502   ARTICLES (5M/day) \u2502                  \u2502                       \u2502\n\u2502    \u2502  \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2510      \u2502                  \u2502                       \u2502\n\u2502    \u2502  \u2502Art1\u2502 \u2502Art2\u2502 ...  \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u2502\n\u2502    \u2502  \u2514\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2518      \u2502     PERSONALIZED                         \u2502\n\u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        FEED                              \u2502\n\u2502                                                                     \u2502\n\u2502    CHALLENGE: Match articles to users based on:                     \u2502\n\u2502    \u2022 Followed publishers                                            \u2502\n\u2502    \u2022 Interest categories                                            \u2502\n\u2502    \u2022 Engagement history                                             \u2502\n\u2502    \u2022 Recency & popularity                                           \u2502\n\u2502                                                                     \u2502\n\u2502    CONSTRAINTS:                                                     \u2502\n\u2502    \u2022 100K reads/sec, < 200ms latency                                \u2502\n\u2502    \u2022 5M writes/day (58/sec)                                         \u2502\n\u2502    \u2022 Feed freshness < 5 minutes                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
    "data_structure_state": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      DATA MODEL OVERVIEW                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                     \u2502\n\u2502  PUBLISHER                    ARTICLE                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502 id: \"pub_nyt\"    \u2502        \u2502 id: \"art_123\"                \u2502       \u2502\n\u2502  \u2502 name: \"NY Times\" \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502 publisher_id: \"pub_nyt\"      \u2502       \u2502\n\u2502  \u2502 rss_url: \"...\"   \u2502        \u2502 title: \"Breaking: ...\"       \u2502       \u2502\n\u2502  \u2502 categories: [    \u2502        \u2502 url: \"https://...\"           \u2502       \u2502\n\u2502  \u2502   \"Politics\",    \u2502        \u2502 categories: [\"Politics\"]     \u2502       \u2502\n\u2502  \u2502   \"World\"        \u2502        \u2502 published_at: 1699123456     \u2502       \u2502\n\u2502  \u2502 ]                \u2502        \u2502 popularity_score: 0.85       \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502                                                                     \u2502\n\u2502  USER                         ENGAGEMENT                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502 id: \"user_1\"         \u2502    \u2502 user_id: \"user_1\"            \u2502       \u2502\n\u2502  \u2502 followed_publishers: \u2502    \u2502 article_id: \"art_123\"        \u2502       \u2502\n\u2502  \u2502   {\"pub_nyt\",        \u2502    \u2502 action: \"like\"               \u2502       \u2502\n\u2502  \u2502    \"pub_tc\"}         \u2502    \u2502 timestamp: 1699123500        \u2502       \u2502\n\u2502  \u2502 interests:           \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502  \u2502   {\"Technology\",     \u2502                                           \u2502\n\u2502  \u2502    \"Politics\"}       \u2502    DERIVED: Publisher Affinity            \u2502\n\u2502  \u2502 publisher_affinity:  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502   {\"pub_nyt\": 3,     \u2502    \u2502 Calculated from like counts  \u2502       \u2502\n\u2502  \u2502    \"pub_tc\": 1}      \u2502    \u2502 per publisher. Boosts ranking\u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502                                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
    "algorithm_flow": [
      {
        "step": 1,
        "description": "User requests feed via getUserFeed(userId, page, pageSize)",
        "visualization": "```\nUser \u2500\u2500\u25ba API Server \u2500\u2500\u25ba Check User Feed Cache\n                              \u2502\n                              \u25bc\n                        Cache HIT? \u2500\u2500\u25ba Return cached feed\n                              \u2502\n                              \u25bc (MISS)\n                        Generate feed on-demand\n```",
        "key_point": "Cache check is O(1) - critical for 100K QPS"
      },
      {
        "step": 2,
        "description": "On cache miss: Fetch user's preferences (followed publishers, interests)",
        "visualization": "```\nUser Profile:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 followed_publishers: [nyt, tc]      \u2502\n\u2502 interests: [Tech, Politics]         \u2502\n\u2502 publisher_affinity: {nyt: 3, tc: 1} \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
        "key_point": "User profile is small and cacheable"
      },
      {
        "step": 3,
        "description": "Gather candidate articles from category caches",
        "visualization": "```\nCategory Caches (shared across users):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Tech: Top 100    \u2502  \u2502 Politics: Top 100\u2502\n\u2502 [art_1, art_2...]\u2502  \u2502 [art_50, art_51] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                     \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u25bc\n         Candidate Pool: ~200 articles\n```",
        "key_point": "Category caches are pre-computed, shared across millions"
      },
      {
        "step": 4,
        "description": "Score and rank candidates using personalization",
        "visualization": "```\nScore = (publisher_match \u00d7 2.0)   \u2190 Followed publisher?\n      + (category_match \u00d7 1.5)    \u2190 Matches interest?\n      + (recency \u00d7 1.0)           \u2190 Recent = higher\n      + (popularity \u00d7 0.5)        \u2190 Global engagement\n      + (affinity_boost \u00d7 1.0)    \u2190 User liked this publisher?\n\nExample:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Article    \u2502 Pub \u2713   \u2502 Cat \u2713    \u2502 Recency \u2502 SCORE \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 art_tc_1   \u2502 2.0     \u2502 1.5      \u2502 0.9     \u2502 4.4   \u2502\n\u2502 art_nyt_3  \u2502 2.0     \u2502 1.5      \u2502 0.5     \u2502 4.0   \u2502\n\u2502 art_bbc_1  \u2502 0.0     \u2502 1.5      \u2502 1.0     \u2502 2.5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
        "key_point": "Scoring is fast O(k) where k = candidate pool size"
      },
      {
        "step": 5,
        "description": "Sort by score, paginate, cache result",
        "visualization": "```\nSorted Feed:\n[art_tc_1, art_nyt_3, art_bbc_1, ...]\n      \u2502\n      \u25bc page=0, pageSize=10\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Return: [art_tc_1 ... art_10]   \u2502\n\u2502 Cache with TTL=300s             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
        "key_point": "Cache the computed feed for subsequent requests"
      }
    ],
    "dry_run_table": "| Step | Operation | Action | State Change | Cache State |\n|------|-----------|--------|--------------|-------------|\n| 1 | registerPublisher(nyt) | Store publisher | publishers={nyt} | - |\n| 2 | registerPublisher(tc) | Store publisher | publishers={nyt,tc} | - |\n| 3 | followPublisher(u1, tc) | Add to follows | u1.follows={tc} | Invalidate u1 feed |\n| 4 | setUserInterests(u1, [Tech]) | Set interests | u1.interests={Tech} | Invalidate u1 feed |\n| 5 | [Crawler ingests articles] | Store articles | articles={tc_1, tc_2, nyt_1} | Update category caches |\n| 6 | getUserFeed(u1, 0, 10) | Cache MISS | - | Generate & cache feed |\n| 7 | getUserFeed(u1, 0, 10) | Cache HIT | - | Return cached (fast!) |"
  },
  "thinking_process": {
    "step_by_step": [
      "When I see **100K QPS reads with <200ms latency**, I think of aggressive caching. We cannot compute feeds from scratch each time.",
      "When I see **5M writes/day (58/sec)**, I think this is manageable write load. We can afford some processing overhead on writes.",
      "The **1700:1 read/write ratio** confirms: optimize reads ruthlessly, push computation to write time.",
      "When I see **personalization (follows + interests + engagement)**, I think of a scoring function. Not ML for Part 1, just weighted factors.",
      "When I see **1000 publishers**, I realize we can cache 'top articles per publisher' - that's only 1000 cache entries.",
      "The key insight: **Category caches are sharable**. Top 100 Tech articles serve all users interested in Tech. Massive cache efficiency.",
      "**Hybrid push-pull**: Pre-compute feeds for active users (push), generate on-demand for inactive (pull). Best of both worlds.",
      "For the prototype, I'll implement the core data structures and ranking algorithm. In a real system, this would be distributed."
    ],
    "key_insight": "**Multi-tier caching with sharable intermediate results**. Instead of caching N user feeds (10M), cache K category feeds (~20) + P publisher feeds (1K). Users' feeds are computed from these shared caches - a massive reduction in cache storage and computation.",
    "why_this_works": "The magic is that most users' interests overlap. 100K users interested in 'Technology' all benefit from the same 'top Tech articles' cache. We compute that cache once, reuse it millions of times. Personalization is a lightweight scoring pass over the shared candidate pool."
  },
  "approaches": [
    {
      "name": "Brute Force: Pull on Every Request",
      "description": "On getUserFeed(), query DB for all articles from followed publishers + matching categories, score them, sort, return. No caching.",
      "pseudocode": "getUserFeed(userId):\n  user = db.getUser(userId)\n  articles = db.query(\n    publisher IN user.follows OR\n    category IN user.interests\n  )\n  for article in articles:\n    article.score = computeScore(article, user)\n  return sorted(articles, by=score)[:pageSize]",
      "time_complexity": "O(A * log A) per request where A = articles in DB",
      "space_complexity": "O(A) per request",
      "pros": [
        "Always fresh",
        "Simple implementation",
        "No cache invalidation complexity"
      ],
      "cons": [
        "Way too slow for 100K QPS",
        "DB would melt",
        "Doesn't meet <200ms SLA"
      ],
      "when_to_use": "Never at this scale. Maybe for <100 QPS internal tool."
    },
    {
      "name": "Pure Push: Pre-compute All Feeds",
      "description": "When article is published, fan-out to all relevant users' pre-computed feed caches. Users read directly from cache.",
      "pseudocode": "onNewArticle(article):\n  users = findUsersFollowing(article.publisher)\n  users += findUsersInterestedIn(article.categories)\n  for user in users:\n    insertIntoFeed(user.id, article)\n\ngetUserFeed(userId):\n  return cache.get(feedKey(userId))  // O(1)",
      "time_complexity": "O(1) read, O(U) write where U = affected users",
      "space_complexity": "O(U * F) where F = feed size",
      "pros": [
        "Blazing fast reads",
        "Simple read path"
      ],
      "cons": [
        "Massive write amplification",
        "Storage explosion (10M users \u00d7 100 articles)",
        "Inactive users waste space"
      ],
      "when_to_use": "Twitter's original design for following graph. Works when follow counts are reasonable."
    },
    {
      "name": "Optimal: Hybrid with Multi-tier Caching",
      "description": "Cache at multiple levels: (1) Category/Publisher article lists (shared), (2) User feeds for active users, (3) Generate on-demand for inactive. Use scoring for personalization.",
      "pseudocode": "getUserFeed(userId, page, pageSize):\n  // Tier 1: Check user feed cache\n  cached = cache.get(userFeedKey(userId))\n  if cached: return paginate(cached, page, pageSize)\n  \n  // Tier 2: Build from category caches\n  user = getUser(userId)\n  candidates = []\n  for interest in user.interests:\n    candidates += cache.get(categoryKey(interest))\n  for pub in user.followedPublishers:\n    candidates += cache.get(publisherKey(pub))\n  \n  // Score and rank\n  for article in dedupe(candidates):\n    article.score = score(article, user)\n  \n  feed = sorted(candidates, by=score)\n  cache.set(userFeedKey(userId), feed, ttl=300)\n  return paginate(feed, page, pageSize)",
      "time_complexity": "O(1) cache hit, O(k log k) cache miss where k = candidate pool",
      "space_complexity": "O(C + P + activeUsers) where C = categories, P = publishers",
      "pros": [
        "Fast reads from cache",
        "Shared caches reduce storage",
        "Fresh on cache miss",
        "Scales with active users not total users"
      ],
      "cons": [
        "More complex cache invalidation",
        "Eventual consistency on cache miss"
      ],
      "key_insight": "Shared intermediate caches (categories, publishers) provide 90% of the benefit with 1% of the storage."
    }
  ],
  "optimal_solution": {
    "name": "Hybrid Push-Pull with Multi-tier Caching",
    "explanation_md": "## Approach\n\nThe optimal design uses **three caching tiers** to achieve sub-200ms latency at 100K QPS:\n\n### Tier 1: User Feed Cache\n- Key: `feed:{userId}`\n- Value: Pre-computed list of article IDs with scores\n- TTL: 5 minutes (matches freshness SLA)\n- Population: On first request (lazy) or proactively for active users\n\n### Tier 2: Category Cache (Shared!)\n- Key: `category:{categoryName}:top100`\n- Value: Top 100 articles in that category by popularity+recency\n- TTL: 5 minutes\n- **This is the key optimization**: 1M users interested in 'Tech' share ONE cache entry\n\n### Tier 3: Publisher Cache\n- Key: `publisher:{publisherId}:recent50`\n- Value: 50 most recent articles from publisher\n- TTL: 5 minutes\n\n### Feed Generation Flow\n```\n1. Check user feed cache \u2192 HIT? Return immediately\n2. Fetch user preferences (cached user profile)\n3. Gather candidates from category + publisher caches\n4. Deduplicate candidates\n5. Score each candidate using personalization factors\n6. Sort by score, cache result, return page\n```\n\n### Why This Works\n- **100K QPS**: Most requests hit user feed cache (Tier 1) \u2192 O(1)\n- **Cache misses**: Build from Tier 2/3 caches, still fast (~50ms)\n- **5-minute freshness**: TTLs ensure new articles appear within SLA\n- **Personalization**: Scoring function applies user-specific weights",
    "data_structures": [
      {
        "structure": "HashMap<publisherId, Publisher>",
        "purpose": "O(1) publisher lookup"
      },
      {
        "structure": "HashMap<userId, User>",
        "purpose": "O(1) user profile lookup"
      },
      {
        "structure": "HashMap<articleId, Article>",
        "purpose": "O(1) article lookup"
      },
      {
        "structure": "HashMap<category, List<Article>>",
        "purpose": "Category cache (shared)"
      },
      {
        "structure": "HashMap<publisherId, List<Article>>",
        "purpose": "Publisher's recent articles"
      },
      {
        "structure": "HashMap<userId, List<Article>>",
        "purpose": "User's computed feed cache"
      }
    ],
    "algorithm_steps": [
      "1. **registerPublisher**: Validate RSS, store in publishers HashMap, initialize crawler schedule",
      "2. **fetchArticles**: Poll RSS, deduplicate by URL hash, store article, update category/publisher caches",
      "3. **followPublisher**: Add to user's followed set, invalidate user's feed cache",
      "4. **setUserInterests**: Update user's interests set, invalidate user's feed cache",
      "5. **likeArticle**: Increment publisher affinity counter for user, batch update to DB",
      "6. **getUserFeed**: Check user cache \u2192 build from category/publisher caches \u2192 score \u2192 sort \u2192 cache \u2192 return"
    ],
    "why_decimal": "Not applicable for this problem - we use float for scoring since exact precision isn't required."
  },
  "solution_python_lines": [
    "\"\"\"",
    "News Feed Aggregator System - Interview Implementation",
    "",
    "Design: Hybrid push-pull with multi-tier caching",
    "- Category caches shared across users (key optimization)",
    "- Simple weighted scoring for personalization",
    "- O(1) cache hits, O(k log k) cache misses",
    "\"\"\"",
    "from typing import List, Dict, Set, Optional",
    "from dataclasses import dataclass, field",
    "from collections import defaultdict",
    "import time",
    "",
    "@dataclass",
    "class Article:",
    "    id: str",
    "    publisher_id: str",
    "    title: str",
    "    url: str",
    "    categories: List[str]",
    "    published_at: float",
    "    popularity: float = 0.0",
    "    score: float = 0.0  # Computed per-user",
    "",
    "@dataclass",
    "class Publisher:",
    "    id: str",
    "    name: str",
    "    rss_url: str",
    "    categories: List[str]",
    "",
    "@dataclass",
    "class User:",
    "    id: str",
    "    followed_publishers: Set[str] = field(default_factory=set)",
    "    interests: Set[str] = field(default_factory=set)",
    "    publisher_affinity: Dict[str, int] = field(default_factory=dict)",
    "",
    "class NewsAggregator:",
    "    \"\"\"",
    "    Core news aggregator with personalized feed generation.",
    "    Uses multi-tier caching: user feeds \u2192 category articles \u2192 publisher articles.",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        self.publishers: Dict[str, Publisher] = {}",
    "        self.users: Dict[str, User] = {}",
    "        self.articles: Dict[str, Article] = {}",
    "        ",
    "        # Cache tiers (in production: Redis with TTL)",
    "        self.category_cache: Dict[str, List[Article]] = defaultdict(list)",
    "        self.publisher_cache: Dict[str, List[Article]] = defaultdict(list)",
    "        self.user_feed_cache: Dict[str, List[Article]] = {}",
    "        ",
    "        self.CACHE_SIZE = 100  # Top articles per category",
    "    ",
    "    def register_publisher(self, publisher_id: str, name: str, ",
    "                           rss_url: str, categories: List[str]) -> None:",
    "        \"\"\"Register publisher. In production: validate RSS connectivity.\"\"\"",
    "        self.publishers[publisher_id] = Publisher(",
    "            id=publisher_id, name=name, rss_url=rss_url, categories=categories",
    "        )",
    "    ",
    "    def follow_publisher(self, user_id: str, publisher_id: str) -> None:",
    "        \"\"\"User follows publisher. Invalidates feed cache.\"\"\"",
    "        user = self._get_or_create_user(user_id)",
    "        user.followed_publishers.add(publisher_id)",
    "        self.user_feed_cache.pop(user_id, None)  # Invalidate cache",
    "    ",
    "    def set_user_interests(self, user_id: str, categories: List[str]) -> None:",
    "        \"\"\"Set user's interest categories. Invalidates feed cache.\"\"\"",
    "        user = self._get_or_create_user(user_id)",
    "        user.interests = set(categories)",
    "        self.user_feed_cache.pop(user_id, None)",
    "    ",
    "    def like_article(self, user_id: str, article_id: str) -> None:",
    "        \"\"\"Record engagement for personalization (publisher affinity boost).\"\"\"",
    "        if article_id not in self.articles:",
    "            return",
    "        user = self._get_or_create_user(user_id)",
    "        pub_id = self.articles[article_id].publisher_id",
    "        user.publisher_affinity[pub_id] = user.publisher_affinity.get(pub_id, 0) + 1",
    "    ",
    "    def get_user_feed(self, user_id: str, page: int, page_size: int) -> List[Article]:",
    "        \"\"\"",
    "        Generate personalized feed. O(1) on cache hit, O(k log k) on miss.",
    "        Uses weighted scoring: publisher_match + category_match + recency + affinity.",
    "        \"\"\"",
    "        # Tier 1: Check user feed cache",
    "        if user_id in self.user_feed_cache:",
    "            return self._paginate(self.user_feed_cache[user_id], page, page_size)",
    "        ",
    "        user = self._get_or_create_user(user_id)",
    "        candidates: Dict[str, Article] = {}  # Dedupe by article ID",
    "        ",
    "        # Tier 2: Gather from category caches",
    "        for interest in user.interests:",
    "            for article in self.category_cache.get(interest, []):",
    "                candidates[article.id] = article",
    "        ",
    "        # Tier 3: Gather from publisher caches",
    "        for pub_id in user.followed_publishers:",
    "            for article in self.publisher_cache.get(pub_id, []):",
    "                candidates[article.id] = article",
    "        ",
    "        # Cold start: use trending articles if no preferences",
    "        if not candidates:",
    "            candidates = {a.id: a for a in self._get_trending_articles()}",
    "        ",
    "        # Score each candidate for this user",
    "        scored = []",
    "        for article in candidates.values():",
    "            article.score = self._score_article(article, user)",
    "            scored.append(article)",
    "        ",
    "        # Sort by score descending",
    "        scored.sort(key=lambda a: a.score, reverse=True)",
    "        ",
    "        # Cache the computed feed (TTL in production)",
    "        self.user_feed_cache[user_id] = scored",
    "        ",
    "        return self._paginate(scored, page, page_size)",
    "    ",
    "    def _score_article(self, article: Article, user: User) -> float:",
    "        \"\"\"",
    "        Weighted scoring for personalization.",
    "        score = publisher_match(2.0) + category_match(1.5) + recency(1.0) + affinity(1.0)",
    "        \"\"\"",
    "        score = 0.0",
    "        ",
    "        # Factor 1: Publisher match (highest weight)",
    "        if article.publisher_id in user.followed_publishers:",
    "            score += 2.0",
    "        ",
    "        # Factor 2: Category match",
    "        if set(article.categories) & user.interests:",
    "            score += 1.5",
    "        ",
    "        # Factor 3: Recency (decay over 24 hours)",
    "        age_hours = (time.time() - article.published_at) / 3600",
    "        recency_score = max(0, 1.0 - age_hours / 24)",
    "        score += recency_score",
    "        ",
    "        # Factor 4: Publisher affinity from engagement history",
    "        affinity = user.publisher_affinity.get(article.publisher_id, 0)",
    "        score += min(affinity * 0.3, 1.5)  # Cap affinity boost",
    "        ",
    "        # Factor 5: Global popularity",
    "        score += article.popularity * 0.5",
    "        ",
    "        return score",
    "    ",
    "    def ingest_article(self, article: Article) -> None:",
    "        \"\"\"Ingest article and update caches. Called by crawler workers.\"\"\"",
    "        self.articles[article.id] = article",
    "        ",
    "        # Update publisher cache (most recent 50)",
    "        pub_articles = self.publisher_cache[article.publisher_id]",
    "        pub_articles.insert(0, article)",
    "        self.publisher_cache[article.publisher_id] = pub_articles[:50]",
    "        ",
    "        # Update category caches (top 100 per category)",
    "        for category in article.categories:",
    "            cat_articles = self.category_cache[category]",
    "            cat_articles.append(article)",
    "            cat_articles.sort(key=lambda a: a.published_at, reverse=True)",
    "            self.category_cache[category] = cat_articles[:self.CACHE_SIZE]",
    "    ",
    "    def _get_or_create_user(self, user_id: str) -> User:",
    "        if user_id not in self.users:",
    "            self.users[user_id] = User(id=user_id)",
    "        return self.users[user_id]",
    "    ",
    "    def _get_trending_articles(self) -> List[Article]:",
    "        \"\"\"Fallback for cold start users: return trending articles.\"\"\"",
    "        all_articles = list(self.articles.values())",
    "        all_articles.sort(key=lambda a: (a.popularity, a.published_at), reverse=True)",
    "        return all_articles[:50]",
    "    ",
    "    def _paginate(self, items: List[Article], page: int, page_size: int) -> List[Article]:",
    "        start = page * page_size",
    "        return items[start:start + page_size]",
    "",
    "",
    "if __name__ == '__main__':",
    "    print(\"=\" * 60)",
    "    print(\"NEWS FEED AGGREGATOR DEMO\")",
    "    print(\"=\" * 60)",
    "    ",
    "    aggregator = NewsAggregator()",
    "    now = time.time()",
    "    ",
    "    # Register publishers",
    "    aggregator.register_publisher(\"pub_nyt\", \"NY Times\", \"https://nyt.com/rss\", [\"Politics\", \"World\"])",
    "    aggregator.register_publisher(\"pub_tc\", \"TechCrunch\", \"https://tc.com/rss\", [\"Technology\", \"Startups\"])",
    "    aggregator.register_publisher(\"pub_espn\", \"ESPN\", \"https://espn.com/rss\", [\"Sports\"])",
    "    print(\"\\n[1] Registered 3 publishers\")",
    "    ",
    "    # Ingest articles (simulating crawler)",
    "    articles = [",
    "        Article(\"art_tc_1\", \"pub_tc\", \"AI Startup Raises $100M\", \"url1\", [\"Technology\"], now - 3600, 0.9),",
    "        Article(\"art_tc_2\", \"pub_tc\", \"New iPhone Leaked\", \"url2\", [\"Technology\"], now - 7200, 0.7),",
    "        Article(\"art_nyt_1\", \"pub_nyt\", \"Election Results\", \"url3\", [\"Politics\"], now - 1800, 0.95),",
    "        Article(\"art_nyt_2\", \"pub_nyt\", \"Climate Summit\", \"url4\", [\"World\", \"Politics\"], now - 5400, 0.6),",
    "        Article(\"art_espn_1\", \"pub_espn\", \"Lakers Win Championship\", \"url5\", [\"Sports\"], now - 900, 0.85),",
    "    ]",
    "    for article in articles:",
    "        aggregator.ingest_article(article)",
    "    print(\"[2] Ingested 5 articles\")",
    "    ",
    "    # Setup User 1: Tech enthusiast following TechCrunch",
    "    aggregator.follow_publisher(\"user_1\", \"pub_tc\")",
    "    aggregator.set_user_interests(\"user_1\", [\"Technology\"])",
    "    print(\"\\n[3] User 1: follows TechCrunch, interested in Technology\")",
    "    ",
    "    feed1 = aggregator.get_user_feed(\"user_1\", 0, 10)",
    "    print(f\"    Feed: {[a.title for a in feed1]}\")",
    "    print(f\"    Expected: TechCrunch articles ranked highest\")",
    "    ",
    "    # Setup User 2: Diverse interests",
    "    aggregator.follow_publisher(\"user_2\", \"pub_nyt\")",
    "    aggregator.follow_publisher(\"user_2\", \"pub_espn\")",
    "    aggregator.set_user_interests(\"user_2\", [\"Sports\", \"Politics\"])",
    "    print(\"\\n[4] User 2: follows NYT & ESPN, interested in Sports & Politics\")",
    "    ",
    "    feed2 = aggregator.get_user_feed(\"user_2\", 0, 10)",
    "    print(f\"    Feed: {[a.title for a in feed2]}\")",
    "    ",
    "    # User 3: New user (cold start)",
    "    print(\"\\n[5] User 3: Cold start (no preferences)\")",
    "    feed3 = aggregator.get_user_feed(\"user_3\", 0, 10)",
    "    print(f\"    Feed: {[a.title for a in feed3]}\")",
    "    print(f\"    Falls back to trending articles\")",
    "    ",
    "    # Like-based personalization",
    "    print(\"\\n[6] User 1 likes NYT articles (learning implicit preference)\")",
    "    aggregator.like_article(\"user_1\", \"art_nyt_1\")",
    "    aggregator.like_article(\"user_1\", \"art_nyt_2\")",
    "    aggregator.like_article(\"user_1\", \"art_nyt_1\")  # Like again",
    "    aggregator.user_feed_cache.pop(\"user_1\", None)  # Force recalc",
    "    ",
    "    feed1_updated = aggregator.get_user_feed(\"user_1\", 0, 10)",
    "    print(f\"    Updated feed: {[a.title for a in feed1_updated]}\")",
    "    print(f\"    NYT articles should rank higher due to affinity boost\")",
    "    ",
    "    print(\"\\n\" + \"=\" * 60)",
    "    print(\"DEMO COMPLETE - System handles personalization, cold start,\")",
    "    print(\"caching, and engagement-based ranking\")",
    "    print(\"=\" * 60)"
  ],
  "solution_java_lines": [
    "import java.util.*;",
    "import java.util.stream.Collectors;",
    "",
    "/**",
    " * News Feed Aggregator System - Interview Implementation",
    " * Design: Hybrid push-pull with multi-tier caching",
    " */",
    "public class NewsAggregator {",
    "    ",
    "    static class Article {",
    "        String id, publisherId, title, url;",
    "        List<String> categories;",
    "        long publishedAt;",
    "        double popularity, score;",
    "        ",
    "        Article(String id, String publisherId, String title, String url,",
    "                List<String> categories, long publishedAt, double popularity) {",
    "            this.id = id; this.publisherId = publisherId; this.title = title;",
    "            this.url = url; this.categories = categories;",
    "            this.publishedAt = publishedAt; this.popularity = popularity;",
    "        }",
    "    }",
    "    ",
    "    static class Publisher {",
    "        String id, name, rssUrl;",
    "        List<String> categories;",
    "        Publisher(String id, String name, String rssUrl, List<String> categories) {",
    "            this.id = id; this.name = name; this.rssUrl = rssUrl; this.categories = categories;",
    "        }",
    "    }",
    "    ",
    "    static class User {",
    "        String id;",
    "        Set<String> followedPublishers = new HashSet<>();",
    "        Set<String> interests = new HashSet<>();",
    "        Map<String, Integer> publisherAffinity = new HashMap<>();",
    "        User(String id) { this.id = id; }",
    "    }",
    "    ",
    "    private Map<String, Publisher> publishers = new HashMap<>();",
    "    private Map<String, User> users = new HashMap<>();",
    "    private Map<String, Article> articles = new HashMap<>();",
    "    private Map<String, List<Article>> categoryCache = new HashMap<>();",
    "    private Map<String, List<Article>> publisherCache = new HashMap<>();",
    "    private Map<String, List<Article>> userFeedCache = new HashMap<>();",
    "    private static final int CACHE_SIZE = 100;",
    "    ",
    "    public void registerPublisher(String publisherId, String name, ",
    "                                  String rssUrl, List<String> categories) {",
    "        publishers.put(publisherId, new Publisher(publisherId, name, rssUrl, categories));",
    "    }",
    "    ",
    "    public void followPublisher(String userId, String publisherId) {",
    "        User user = getOrCreateUser(userId);",
    "        user.followedPublishers.add(publisherId);",
    "        userFeedCache.remove(userId);  // Invalidate cache",
    "    }",
    "    ",
    "    public void setUserInterests(String userId, List<String> categories) {",
    "        User user = getOrCreateUser(userId);",
    "        user.interests = new HashSet<>(categories);",
    "        userFeedCache.remove(userId);",
    "    }",
    "    ",
    "    public void likeArticle(String userId, String articleId) {",
    "        if (!articles.containsKey(articleId)) return;",
    "        User user = getOrCreateUser(userId);",
    "        String pubId = articles.get(articleId).publisherId;",
    "        user.publisherAffinity.merge(pubId, 1, Integer::sum);",
    "    }",
    "    ",
    "    public List<Article> getUserFeed(String userId, int page, int pageSize) {",
    "        // Tier 1: Check cache",
    "        if (userFeedCache.containsKey(userId)) {",
    "            return paginate(userFeedCache.get(userId), page, pageSize);",
    "        }",
    "        ",
    "        User user = getOrCreateUser(userId);",
    "        Map<String, Article> candidates = new HashMap<>();",
    "        ",
    "        // Tier 2: Category caches",
    "        for (String interest : user.interests) {",
    "            for (Article a : categoryCache.getOrDefault(interest, List.of())) {",
    "                candidates.put(a.id, a);",
    "            }",
    "        }",
    "        ",
    "        // Tier 3: Publisher caches",
    "        for (String pubId : user.followedPublishers) {",
    "            for (Article a : publisherCache.getOrDefault(pubId, List.of())) {",
    "                candidates.put(a.id, a);",
    "            }",
    "        }",
    "        ",
    "        // Cold start fallback",
    "        if (candidates.isEmpty()) {",
    "            candidates = getTrendingArticles().stream()",
    "                .collect(Collectors.toMap(a -> a.id, a -> a));",
    "        }",
    "        ",
    "        // Score and sort",
    "        List<Article> scored = new ArrayList<>(candidates.values());",
    "        for (Article a : scored) {",
    "            a.score = scoreArticle(a, user);",
    "        }",
    "        scored.sort((a, b) -> Double.compare(b.score, a.score));",
    "        ",
    "        userFeedCache.put(userId, scored);",
    "        return paginate(scored, page, pageSize);",
    "    }",
    "    ",
    "    private double scoreArticle(Article article, User user) {",
    "        double score = 0.0;",
    "        if (user.followedPublishers.contains(article.publisherId)) score += 2.0;",
    "        if (!Collections.disjoint(article.categories, user.interests)) score += 1.5;",
    "        double ageHours = (System.currentTimeMillis() - article.publishedAt) / 3600000.0;",
    "        score += Math.max(0, 1.0 - ageHours / 24);",
    "        int affinity = user.publisherAffinity.getOrDefault(article.publisherId, 0);",
    "        score += Math.min(affinity * 0.3, 1.5);",
    "        score += article.popularity * 0.5;",
    "        return score;",
    "    }",
    "    ",
    "    public void ingestArticle(Article article) {",
    "        articles.put(article.id, article);",
    "        publisherCache.computeIfAbsent(article.publisherId, k -> new ArrayList<>()).add(0, article);",
    "        if (publisherCache.get(article.publisherId).size() > 50) {",
    "            publisherCache.get(article.publisherId).remove(50);",
    "        }",
    "        for (String cat : article.categories) {",
    "            categoryCache.computeIfAbsent(cat, k -> new ArrayList<>()).add(article);",
    "            categoryCache.get(cat).sort((a, b) -> Long.compare(b.publishedAt, a.publishedAt));",
    "            if (categoryCache.get(cat).size() > CACHE_SIZE) {",
    "                categoryCache.put(cat, new ArrayList<>(categoryCache.get(cat).subList(0, CACHE_SIZE)));",
    "            }",
    "        }",
    "    }",
    "    ",
    "    private User getOrCreateUser(String userId) {",
    "        return users.computeIfAbsent(userId, User::new);",
    "    }",
    "    ",
    "    private List<Article> getTrendingArticles() {",
    "        return articles.values().stream()",
    "            .sorted((a, b) -> Double.compare(b.popularity, a.popularity))",
    "            .limit(50).collect(Collectors.toList());",
    "    }",
    "    ",
    "    private List<Article> paginate(List<Article> items, int page, int pageSize) {",
    "        int start = page * pageSize;",
    "        if (start >= items.size()) return List.of();",
    "        return items.subList(start, Math.min(start + pageSize, items.size()));",
    "    }",
    "    ",
    "    public static void main(String[] args) {",
    "        System.out.println(\"=\" .repeat(60));",
    "        System.out.println(\"NEWS FEED AGGREGATOR DEMO\");",
    "        System.out.println(\"=\" .repeat(60));",
    "        ",
    "        NewsAggregator agg = new NewsAggregator();",
    "        long now = System.currentTimeMillis();",
    "        ",
    "        agg.registerPublisher(\"pub_tc\", \"TechCrunch\", \"url\", List.of(\"Technology\"));",
    "        agg.registerPublisher(\"pub_nyt\", \"NY Times\", \"url\", List.of(\"Politics\"));",
    "        System.out.println(\"\\n[1] Registered publishers\");",
    "        ",
    "        agg.ingestArticle(new Article(\"a1\", \"pub_tc\", \"AI News\", \"url\", List.of(\"Technology\"), now - 3600000, 0.9));",
    "        agg.ingestArticle(new Article(\"a2\", \"pub_nyt\", \"Election\", \"url\", List.of(\"Politics\"), now - 1800000, 0.95));",
    "        System.out.println(\"[2] Ingested articles\");",
    "        ",
    "        agg.followPublisher(\"u1\", \"pub_tc\");",
    "        agg.setUserInterests(\"u1\", List.of(\"Technology\"));",
    "        System.out.println(\"\\n[3] User 1 follows TechCrunch\");",
    "        ",
    "        List<Article> feed = agg.getUserFeed(\"u1\", 0, 10);",
    "        System.out.println(\"Feed: \" + feed.stream().map(a -> a.title).collect(Collectors.toList()));",
    "        ",
    "        System.out.println(\"\\n\" + \"=\" .repeat(60));",
    "        System.out.println(\"DEMO COMPLETE\");",
    "    }",
    "}"
  ],
  "code_walkthrough": [
    {
      "lines": "1-15",
      "section": "Data Classes",
      "explanation": "Define **Article**, **Publisher**, and **User** as simple data classes. Article has `score` field computed per-user during ranking. User tracks `publisher_affinity` for engagement-based personalization."
    },
    {
      "lines": "37-51",
      "section": "NewsAggregator Initialization",
      "explanation": "Three cache tiers: `category_cache` (shared across users), `publisher_cache` (recent articles per publisher), `user_feed_cache` (computed feeds). In production, these would be Redis with TTLs."
    },
    {
      "lines": "53-72",
      "section": "User Preference Methods",
      "explanation": "**follow_publisher** and **set_user_interests** modify user profile and **invalidate the user's feed cache**. This ensures next feed request gets fresh personalization."
    },
    {
      "lines": "74-79",
      "section": "Engagement Tracking",
      "explanation": "**like_article** increments publisher affinity counter. This feeds into the scoring function to boost articles from publishers the user engages with."
    },
    {
      "lines": "81-116",
      "section": "getUserFeed - The Core Algorithm",
      "explanation": "**Three-tier feed generation**: (1) Check user cache, (2) Gather candidates from category/publisher caches, (3) Cold start fallback. Then score each candidate using `_score_article`, sort by score, cache result."
    },
    {
      "lines": "118-142",
      "section": "Scoring Function",
      "explanation": "**Weighted scoring for personalization**: publisher_match (2.0) + category_match (1.5) + recency_decay (1.0) + affinity_boost (capped at 1.5) + popularity (0.5). Recency decays linearly over 24 hours."
    },
    {
      "lines": "144-159",
      "section": "Article Ingestion",
      "explanation": "**Updates both cache tiers**: publisher_cache keeps 50 most recent per publisher, category_cache keeps 100 most recent per category. This is called by crawler workers."
    }
  ],
  "debugging_strategy": {
    "how_to_test_incrementally": "1. Test registerPublisher alone - verify storage. 2. Test ingestArticle - verify caches populated. 3. Test getUserFeed for user with no preferences (cold start). 4. Add follow/interests, verify feed changes. 5. Add likes, verify affinity boost.",
    "what_to_print_or_assert": [
      "print(f'Category cache size: {len(self.category_cache[\"Technology\"])}')",
      "print(f'User follows: {user.followed_publishers}')",
      "print(f'Article scores: {[(a.title, a.score) for a in feed]}')",
      "assert len(feed) <= page_size, 'Pagination broken'"
    ],
    "common_failure_modes": [
      "**Cache not invalidated** after follow/interests change",
      "**Empty candidates** due to typo in category name",
      "**Scoring always 0** because time zone issues with published_at",
      "**Same article multiple times** if deduplication missing"
    ],
    "how_to_fix_fast": "Print the candidate set before scoring. If empty, trace back to category/publisher caches. If full but wrong order, print individual score components."
  },
  "complexity_analysis": {
    "time": {
      "registerPublisher": {
        "complexity": "O(1)",
        "explanation": "HashMap insertion"
      },
      "followPublisher": {
        "complexity": "O(1)",
        "explanation": "Set addition + cache invalidation"
      },
      "setUserInterests": {
        "complexity": "O(1)",
        "explanation": "Set replacement + cache invalidation"
      },
      "likeArticle": {
        "complexity": "O(1)",
        "explanation": "HashMap increment"
      },
      "getUserFeed_cache_hit": {
        "complexity": "O(1)",
        "explanation": "Direct cache lookup"
      },
      "getUserFeed_cache_miss": {
        "complexity": "O(k log k)",
        "explanation": "k = candidate pool size (~200), dominated by sort"
      },
      "ingestArticle": {
        "complexity": "O(c log n)",
        "explanation": "c = categories, n = cache size, for sorted insertion"
      },
      "overall": "**O(1) for cache hits (majority of requests), O(k log k) for cache misses**"
    },
    "space": {
      "complexity": "O(P + C*N + A + U)",
      "breakdown": "- P publishers: O(P)\n- Category caches: O(C * CACHE_SIZE) where C = ~20 categories\n- Publisher caches: O(P * 50)\n- Article storage: O(A) where A = active articles\n- User profiles: O(U) where U = active users\n- User feed caches: Only for active users, bounded",
      "note": "Key insight: Category caches are shared. 1M users with 'Tech' interest share ONE 100-article cache."
    },
    "can_we_do_better": "For Part 1, this is optimal. Further optimizations: (1) Pre-compute feeds for top 1% most active users, (2) Use probabilistic data structures for deduplication, (3) Shard by user ID for horizontal scaling."
  },
  "dry_run": {
    "example": "registerPublisher(nyt), registerPublisher(tc), followPublisher(u1, tc), setUserInterests(u1, [Tech]), ingestArticle(tc_1), getUserFeed(u1, 0, 10)",
    "trace_table": "| Step | Operation | State Change | Caches |\n|------|-----------|--------------|--------|\n| 1 | registerPublisher(nyt) | publishers={nyt} | - |\n| 2 | registerPublisher(tc) | publishers={nyt, tc} | - |\n| 3 | followPublisher(u1, tc) | u1.follows={tc} | user_feed_cache invalidated |\n| 4 | setUserInterests(u1, [Tech]) | u1.interests={Tech} | user_feed_cache invalidated |\n| 5 | ingestArticle(tc_1 with [Tech]) | articles={tc_1} | category_cache[Tech]=[tc_1], publisher_cache[tc]=[tc_1] |\n| 6 | getUserFeed(u1, 0, 10) | - | MISS \u2192 gather from caches \u2192 score \u2192 cache |\n\n**Feed Generation Detail (Step 6)**:\n1. Cache MISS for u1\n2. Gather from category_cache['Tech'] \u2192 [tc_1]\n3. Gather from publisher_cache['tc'] \u2192 [tc_1] (already have)\n4. Candidates after dedupe: {tc_1}\n5. Score tc_1: pub_match(2.0) + cat_match(1.5) + recency(~0.9) + affinity(0) + pop(0.5) = **4.9**\n6. Cache and return: [tc_1]",
    "final_answer": "Feed contains [tc_1] with score 4.9"
  },
  "test_cases": [
    {
      "name": "Basic: Single publisher, single user",
      "category": "Happy Path",
      "input": "Register TC, ingest article, user follows TC, get feed",
      "expected": "Feed contains TC article",
      "explanation": "Publisher match gives high score"
    },
    {
      "name": "Cold Start: New user with no preferences",
      "category": "Edge Case",
      "input": "getUserFeed(new_user, 0, 10)",
      "expected": "Returns trending articles",
      "explanation": "Fallback to global trending when no personalization possible"
    },
    {
      "name": "Interest vs Follow priority",
      "category": "Personalization",
      "input": "User follows ESPN (Sports), interested in Tech. TC (Tech) and ESPN articles.",
      "expected": "ESPN articles rank higher due to publisher_match weight",
      "explanation": "publisher_match (2.0) > category_match (1.5)"
    },
    {
      "name": "Engagement boost",
      "category": "Personalization",
      "input": "User likes NYT articles 3 times, then gets feed",
      "expected": "NYT articles boosted by affinity (0.9 bonus)",
      "explanation": "publisher_affinity[nyt]=3 \u2192 3*0.3=0.9 boost"
    },
    {
      "name": "Cache invalidation on follow",
      "category": "Cache Behavior",
      "input": "Get feed (cached), follow new publisher, get feed again",
      "expected": "Second feed includes new publisher's articles",
      "explanation": "followPublisher invalidates user's feed cache"
    },
    {
      "name": "Pagination",
      "category": "Edge Case",
      "input": "20 articles in feed, getUserFeed(u, 1, 10)",
      "expected": "Returns articles 10-19",
      "explanation": "page=1 means skip first page"
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Computing feed from scratch on every request",
      "why_wrong": "At 100K QPS, this would require massive compute. Each request does DB queries, joins, scoring.",
      "correct_approach": "Multi-tier caching. User feed cache handles repeat requests. Category caches are shared.",
      "code_wrong": "def get_feed(user_id):\n    articles = db.query('SELECT * WHERE ...')  # DB hit every time\n    return rank(articles)",
      "code_correct": "if user_id in cache: return cache[user_id]\ncandidates = gather_from_category_caches()  # Fast\nfeed = rank(candidates)\ncache[user_id] = feed"
    },
    {
      "mistake": "Pre-computing feeds for ALL users (pure push)",
      "why_wrong": "10M users \u00d7 100 articles \u00d7 1KB = 1TB cache. 90% of users are inactive. Massive waste.",
      "correct_approach": "Hybrid: Pre-compute for active users (top 10%), on-demand for rest.",
      "code_wrong": "on_new_article(article):\n    for user in all_10_million_users:  # Fan-out disaster\n        update_feed(user, article)",
      "code_correct": "on_new_article(article):\n    update_category_cache(article)  # Shared cache\n    update_publisher_cache(article)"
    },
    {
      "mistake": "Not invalidating cache on preference change",
      "why_wrong": "User follows new publisher but feed still shows old content. Bad UX.",
      "correct_approach": "Invalidate user's feed cache when follows/interests change.",
      "code_wrong": "def follow_publisher(user_id, pub_id):\n    user.follows.add(pub_id)  # Missing cache invalidation",
      "code_correct": "def follow_publisher(user_id, pub_id):\n    user.follows.add(pub_id)\n    self.user_feed_cache.pop(user_id, None)  # Invalidate"
    },
    {
      "mistake": "No cold start handling",
      "why_wrong": "New users with no preferences see empty feed. Terrible onboarding.",
      "correct_approach": "Fall back to trending/popular articles for cold start users.",
      "code_wrong": "if not candidates:\n    return []  # Empty feed for new users",
      "code_correct": "if not candidates:\n    candidates = get_trending_articles()  # Graceful fallback"
    }
  ],
  "interview_tips": {
    "opening": "Thank you for this problem. It's a classic news aggregator design - reminds me of Google News or Apple News. Before I dive in, let me ask a few clarifying questions to understand the scope...",
    "clarifying_questions_to_ask": [
      "What's the read/write ratio? This affects whether I optimize for reads or writes.",
      "How personalized should feeds be? Simple follows/interests, or ML-based recommendations?",
      "What's the freshness requirement? Can we serve slightly stale data from cache?",
      "Single region or global? This affects replication and latency design.",
      "What does 'deduplication' mean? Same URL? Same story from different sources?"
    ],
    "what_to_mention_proactively": [
      "I'll use a hybrid push-pull model - pre-compute for active users, on-demand for others",
      "Key insight: category caches are sharable across millions of users",
      "I'll use a simple weighted scoring function - ML ranking is a follow-up",
      "Cache invalidation on preference changes ensures consistency"
    ],
    "communication_during_coding": [
      "I'm using a HashMap for O(1) publisher lookup",
      "Notice the three cache tiers - this is the core optimization",
      "The scoring function weights publisher_match highest because that's explicit intent",
      "Cold start fallback to trending articles - important for onboarding"
    ],
    "if_stuck": [
      "Step back: What's the constraint? 100K QPS reads \u2192 must cache aggressively",
      "Draw the data flow: Publishers \u2192 Ingestion \u2192 Storage \u2192 Cache \u2192 API \u2192 Users",
      "Ask: What do I cache? At what granularity? With what TTL?"
    ],
    "time_management": "0-8min: Clarify requirements, discuss scale | 8-15min: High-level architecture diagram | 15-25min: Deep dive on feed generation | 25-35min: API design, data model | 35-45min: Scaling, follow-ups"
  },
  "pattern_recognition": {
    "pattern_name": "Fan-out on Read with Multi-tier Caching",
    "indicators": [
      "Read-heavy workload (1000:1 or higher read/write ratio)",
      "Personalization requirement",
      "Shared content across users (categories, publishers)",
      "Strict latency SLA (< 200ms)"
    ],
    "similar_problems": [
      "LC 355 - Design Twitter: Similar feed generation, but follows instead of interests",
      "Design Instagram Feed: Same pattern, add image/video handling",
      "Design Reddit Home Page: Same pattern with subreddit subscriptions",
      "Design Notification System: Similar fan-out concerns"
    ],
    "template": "1. Identify shared caches (category/topic level)\n2. Build candidate pool from shared caches\n3. Apply per-user scoring/filtering\n4. Cache the final result with TTL\n5. Handle cold start gracefully"
  },
  "follow_up_preparation": {
    "part_2_hint": "**Real-Time Notifications & Breaking News**: Add WebSocket connections for push notifications. Use a pub-sub system (Kafka) to detect breaking news (high velocity articles). Push to connected users in real-time bypassing cache.",
    "part_3_hint": "**ML Recommendations**: Replace simple scoring with ML model. Add feature store for user embeddings (click history, read time, etc.). Use two-stage ranking: coarse (rule-based) \u2192 fine (ML model). A/B testing framework for model comparison.",
    "data_structure_evolution": "Part 1: HashMap caches \u2192 Part 2: Add Kafka for real-time, WebSocket for push \u2192 Part 3: Add Redis ML feature store, model serving layer"
  },
  "communication_script": {
    "opening_verbatim": "Thank you for this problem. News aggregation is a fascinating area - it combines content ingestion, personalization, and scale challenges. Let me start by understanding the requirements better...",
    "after_clarification": "Great, so we're looking at 100K QPS reads, <200ms latency, personalization based on follows and interests, and ~5 million articles per day. Let me draw out the high-level architecture first, then we'll deep dive into the feed generation which is the core challenge.",
    "while_coding": [
      "I'm creating three cache tiers - this is the key to hitting our latency target...",
      "Notice I invalidate the user's feed cache when they follow someone...",
      "The scoring function is simple for now - this is where ML would plug in for Part 3..."
    ],
    "after_coding": "Let me trace through a scenario: User 1 follows TechCrunch, interested in Technology. When they request feed...",
    "when_stuck_verbatim": "I'm thinking about how to handle the celebrity publisher problem - if NYT has 10 million followers, pure push would be expensive. Let me think about this...",
    "after_mistake": "Actually, I realize I'm not invalidating cache when user changes interests. Let me fix that - it's important for consistency.",
    "before_moving_on": "So this design handles Part 1 requirements: 100K QPS with <200ms latency using multi-tier caching, personalization via weighted scoring, and graceful degradation for cold start. Ready to discuss Part 2?"
  },
  "interviewer_perspective": {
    "what_they_evaluate": [
      "Can they identify the key constraints (read-heavy, latency) and design accordingly?",
      "Do they understand caching strategies and trade-offs?",
      "Can they design a ranking/scoring system?",
      "Do they consider edge cases (cold start, cache invalidation)?",
      "Can they communicate clearly while designing?"
    ],
    "bonus_points": [
      "Mentioning category caches as shared optimization unprompted",
      "Discussing cache invalidation strategy proactively",
      "Considering cold start problem",
      "Drawing clear architecture diagrams",
      "Mentioning monitoring/observability for production"
    ],
    "red_flags": [
      "Jumping straight to distributed systems without understanding the problem",
      "Not considering caching at all",
      "Over-engineering (adding ML, Kafka, etc. before basics)",
      "Not asking about scale requirements",
      "Unable to explain trade-offs between approaches"
    ],
    "what_differentiates_strong_candidates": "Strong candidates start with requirements, calculate back-of-envelope numbers, identify the core challenge (feed generation at scale), propose a simple solution first, then iterate. They explain WHY they make each decision, not just WHAT."
  },
  "time_milestones": {
    "by_5_min": "Clarified requirements, understood scale (100K QPS, 5M articles/day, <200ms)",
    "by_10_min": "Identified it's read-heavy, proposed caching approach, drawn high-level diagram",
    "by_20_min": "Deep-dived into feed generation algorithm, explained caching tiers",
    "by_30_min": "Covered API design, data models, ranking algorithm",
    "by_40_min": "Discussed scaling, failure modes, monitoring",
    "by_45_min": "Wrapped up Part 1, ready for follow-ups",
    "warning_signs": "If still clarifying at 10 min, or haven't mentioned caching by 15 min, speed up."
  },
  "recovery_strategies": {
    "when_you_make_a_bug": "Say: 'Good catch - I missed invalidating the cache there. Let me fix that.' Shows you can receive feedback.",
    "when_you_dont_know_syntax": "For system design, syntax matters less. Say: 'The idea is to use a sorted set for ranking - the exact API would be ZADD in Redis.'",
    "when_approach_is_wrong": "Say: 'Actually, pure push won't work at this scale due to write amplification. Let me reconsider with a hybrid approach.'",
    "when_completely_stuck": "Say: 'I'm stuck on how to handle the celebrity publisher case. Could you give me a hint about the expected direction?'",
    "when_running_out_of_time": "Say: 'I'm running low on time. Let me summarize the key points and mention what I'd add given more time.'"
  },
  "ai_copilot_tips": {
    "when_using_cursor_or_copilot": "For system design, AI tools are less helpful than for coding. Use them for: drawing diagrams, generating boilerplate data models, syntax lookup.",
    "what_to_do": [
      "Use AI to generate data class definitions quickly",
      "Use AI to look up Redis/Kafka APIs you forgot",
      "Use AI to double-check your complexity analysis"
    ],
    "what_not_to_do": [
      "Don't ask AI 'design a news aggregator' - YOU need to drive the design",
      "Don't copy-paste architecture diagrams without understanding them",
      "Don't let AI suggest over-engineered solutions"
    ],
    "how_to_demonstrate_understanding": "Explain each component you add. If AI suggests Kafka, explain WHY you need it (decoupling, replay, etc.).",
    "expectation_adjustment": "System design is mostly verbal. AI helps less here. Focus on communication."
  },
  "signal_points": {
    "wow_factors": [
      "Drawing a clear architecture diagram unprompted",
      "Calculating back-of-envelope: '100K QPS \u00d7 200ms = need 20K concurrent connections'",
      "Mentioning category caches as shared optimization",
      "Discussing cold start problem before asked",
      "Proposing monitoring: 'I'd track cache hit rate, p99 latency, feed freshness'"
    ],
    "subtle_signals_of_experience": [
      "Asking about read/write ratio before designing",
      "Considering failure modes: 'What if publisher RSS is down?'",
      "Thinking about operational aspects: deployments, rollbacks",
      "Mentioning A/B testing for ranking algorithms"
    ]
  },
  "red_flags_to_avoid": {
    "behavioral": [
      "Not drawing any diagrams",
      "Refusing to make assumptions when asked to",
      "Getting defensive when interviewer challenges design",
      "Not asking any clarifying questions"
    ],
    "technical": [
      "Proposing pure push for 10M users (write amplification)",
      "No caching strategy for read-heavy system",
      "Using relational DB for 100K QPS without caching layer",
      "Not considering personalization at all"
    ],
    "communication": [
      "Jumping to implementation without high-level design",
      "Using buzzwords without explaining (e.g., 'I'll use Kafka' without saying why)",
      "Not explaining trade-offs",
      "Monologuing for 10 minutes without checking in"
    ]
  },
  "final_checklist": {
    "before_saying_done": [
      "Did I cover the functional requirements (register, fetch, feed, follow, interests)?",
      "Did I address the non-functional requirements (100K QPS, <200ms, 99.9% availability)?",
      "Did I explain the caching strategy?",
      "Did I describe the ranking/personalization algorithm?",
      "Did I discuss failure modes and edge cases?",
      "Did I draw a clear architecture diagram?"
    ],
    "quick_code_review": [
      "Data models are clear",
      "API signatures match requirements",
      "Caching logic is correct",
      "Cache invalidation is handled",
      "Cold start is handled"
    ]
  },
  "production_considerations": {
    "what_id_add_in_production": [
      "**Redis Cluster** for cache tier with proper TTLs and eviction policies",
      "**Kafka** for article ingestion pipeline (durability, replay)",
      "**Elasticsearch** for full-text search capability",
      "**CDN** for static content and API caching at edge",
      "**Rate limiting** per user and per publisher",
      "**Circuit breakers** for publisher RSS fetching",
      "**Monitoring**: Cache hit rate, p99 latency, feed freshness distribution",
      "**A/B testing framework** for ranking algorithm experiments"
    ],
    "why_not_in_interview": "Keep focus on core design. Mention these verbally to show production awareness.",
    "how_to_mention": "Say: 'In production, I'd add Redis Cluster for caching, Kafka for ingestion durability, and monitoring for cache hit rates. But for this discussion, let me focus on the core feed generation algorithm.'"
  },
  "generated_at": "2026-01-19T04:06:17.131266",
  "_meta": {
    "problem_id": "news_feed_aggregator",
    "part_number": null,
    "model": "claude-opus-4-5-20251101"
  }
}